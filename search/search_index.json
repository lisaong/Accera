{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Accera Accera is a compiler that enables you to experiment with loop optimizations without hand-writing Assembly code. Accera is available as a Python library and supports cross-compiling to a wide range of processor targets . Writing highly optimized compute-intensive code in a traditional programming language is a difficult and time-consuming process. It requires special engineering skills, such as fluency in Assembly language and a deep understanding of computer architecture. Manually optimizing the simplest numerical algorithms already requires a significant engineering effort. Moreover, highly optimized numerical code is prone to bugs, is often hard to read and maintain, and needs to be reimplemented every time a new target architecture is introduced. Accera aims to solve these problems. Accera has three goals: Performance: generate the fastest implementation of any compute-intensive algorithm. Readability: do so without sacrificing code readability and maintainability. Writability: a user-friendly programming model, designed for agility. Install To install for Linux, macOS, or Windows (requires Python 3.7-3.9): pip install accera See the Install Instructions for more details on installing pre-built Python 3 packages and how to build Accera from source. Quickstart In this example, we will: Implement matrix multiplication with a ReLU activation (matmul + ReLU), commonly used in in machine learning algorithms Generate two implementations: a naive algorithm and one with loop transformations Compare the timings of both implementations Run in your browser No installation is required. This will launch a Jupyter notebook with the quickstart example running in the cloud. Run on your machine Create a Python 3 script called quickstart.py : import accera as acc # define placeholder inputs/output A = acc . Array ( role = acc . Array . Role . INPUT , shape = ( 512 , 512 )) B = acc . Array ( role = acc . Array . Role . INPUT , shape = ( 512 , 512 )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , shape = ( 512 , 512 )) # implement the logic for matmul and relu matmul = acc . Nest ( shape = ( 512 , 512 , 512 )) i1 , j1 , k1 = matmul . get_indices () @matmul . iteration_logic def _ (): C [ i1 , j1 ] += A [ i1 , k1 ] * B [ k1 , j1 ] relu = acc . Nest ( shape = ( 512 , 512 )) i2 , j2 = relu . get_indices () @relu . iteration_logic def _ (): C [ i2 , j2 ] = acc . max ( C [ i2 , j2 ], 0.0 ) package = acc . Package () # fuse the i and j indices of matmul and relu, add to the package schedule = acc . fuse ( matmul . create_schedule (), relu . create_schedule (), partial = 2 ) package . add ( schedule , args = ( A , B , C ), base_name = \"matmul_relu_fusion_naive\" ) # transform the schedule, add to the package f , i , j , k = schedule . get_indices () ii , jj = schedule . tile (( i , j ), ( 16 , 16 )) # loop tiling schedule . reorder ( j , i , f , k , jj , ii ) # loop reordering plan = schedule . create_plan () plan . unroll ( ii ) # loop unrolling package . add ( plan , args = ( A , B , C ), base_name = \"matmul_relu_fusion_transformed\" ) # build a dynamically-linked package (a .dll or .so) that exports both functions print ( package . build ( name = \"hello_accera\" , format = acc . Package . Format . HAT_DYNAMIC )) Ensure that you have a compiler in your PATH: Windows: Install Microsoft Visual Studio and run vcvars64.bat to setup the command prompt Linux/macOS: Install gcc Don't have a compiler handy? We recommend trying Accera in your browser instead Install Accera: pip install accera Generate the library that implements two versions of matmul + ReLU: python quickstart.py To consume and compare the library functions, create a file called benchmark.py in the same location: import hatlib as hat import numpy as np # load the package hat_package = hat . load ( \"hello_accera.hat\" ) # call one of the functions with test inputs A_test = np . random . rand ( 512 , 512 ) . astype ( np . float32 ) B_test = np . random . rand ( 512 , 512 ) . astype ( np . float32 ) C_test = np . zeros (( 512 , 512 )) . astype ( np . float32 ) C_numpy = np . maximum ( C_test + A_test @ B_test , 0.0 ) matmul_relu = hat_package [ \"matmul_relu_fusion_transformed\" ] matmul_relu ( A_test , B_test , C_test ) # check correctness np . testing . assert_allclose ( C_test , C_numpy , atol = 1e-3 ) # benchmark all functions hat . run_benchmark ( \"hello_accera.hat\" , batch_size = 5 , min_time_in_sec = 5 ) Run the benchmark to get the timing results: python benchmark.py Next Steps The Manual is a good place to start for an introduction to the Accera Python programming model. In particular, the schedule transformations describe how you can experiment with different loop transformations with just a few lines of Python. Finally, the .hat format is just a C header file containing metadata. Learn more about the HAT format and benchmarking . How it works In a nutshell, Accera takes the Python code that defines the loop schedule and algorithm and converts it into MLIR intermediate representation (IR). Accera's compiler then takes this IR through a series of MLIR pipelines to perform transformations. The result is a binary library with a C header file. The library implements the algorithms that are defined in Python, and is compatible with the target. To peek into the stages of IR transformation that Accera does, try replacing format=acc.Package.Format.HAT_DYNAMIC with format=acc.Package.Format.MLIR_DYNAMIC in quickstart.py , re-run the script, and search the _tmp subfolder for the intermediate *.mlir files. We plan to document these IR constructs in the future. Documentation Get to know Accera's concepts and Python constructs in the Documentation page. Tutorials More step-by-step examples are available on the Tutorials page. We're working on more examples and tutorials soon. Contributions Accera is a research platform-in-progress. We would love your contributions, feedback, questions, and feature requests! Please file a Github issue or send us a pull request. Please review the Microsoft Code of Conduct to learn more. Credits Accera is built using several open source libraries, including: LLVM , pybind11 , toml++ , tomlkit , vcpkg , pyyaml , and HAT . For testing, we also use numpy and catch2 . License This project is released under the MIT License .","title":"Index"},{"location":"#welcome-to-accera","text":"Accera is a compiler that enables you to experiment with loop optimizations without hand-writing Assembly code. Accera is available as a Python library and supports cross-compiling to a wide range of processor targets . Writing highly optimized compute-intensive code in a traditional programming language is a difficult and time-consuming process. It requires special engineering skills, such as fluency in Assembly language and a deep understanding of computer architecture. Manually optimizing the simplest numerical algorithms already requires a significant engineering effort. Moreover, highly optimized numerical code is prone to bugs, is often hard to read and maintain, and needs to be reimplemented every time a new target architecture is introduced. Accera aims to solve these problems. Accera has three goals: Performance: generate the fastest implementation of any compute-intensive algorithm. Readability: do so without sacrificing code readability and maintainability. Writability: a user-friendly programming model, designed for agility.","title":"Welcome to Accera"},{"location":"#install","text":"To install for Linux, macOS, or Windows (requires Python 3.7-3.9): pip install accera See the Install Instructions for more details on installing pre-built Python 3 packages and how to build Accera from source.","title":"Install"},{"location":"#quickstart","text":"In this example, we will: Implement matrix multiplication with a ReLU activation (matmul + ReLU), commonly used in in machine learning algorithms Generate two implementations: a naive algorithm and one with loop transformations Compare the timings of both implementations","title":"Quickstart"},{"location":"#run-in-your-browser","text":"No installation is required. This will launch a Jupyter notebook with the quickstart example running in the cloud.","title":"Run in your browser"},{"location":"#run-on-your-machine","text":"Create a Python 3 script called quickstart.py : import accera as acc # define placeholder inputs/output A = acc . Array ( role = acc . Array . Role . INPUT , shape = ( 512 , 512 )) B = acc . Array ( role = acc . Array . Role . INPUT , shape = ( 512 , 512 )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , shape = ( 512 , 512 )) # implement the logic for matmul and relu matmul = acc . Nest ( shape = ( 512 , 512 , 512 )) i1 , j1 , k1 = matmul . get_indices () @matmul . iteration_logic def _ (): C [ i1 , j1 ] += A [ i1 , k1 ] * B [ k1 , j1 ] relu = acc . Nest ( shape = ( 512 , 512 )) i2 , j2 = relu . get_indices () @relu . iteration_logic def _ (): C [ i2 , j2 ] = acc . max ( C [ i2 , j2 ], 0.0 ) package = acc . Package () # fuse the i and j indices of matmul and relu, add to the package schedule = acc . fuse ( matmul . create_schedule (), relu . create_schedule (), partial = 2 ) package . add ( schedule , args = ( A , B , C ), base_name = \"matmul_relu_fusion_naive\" ) # transform the schedule, add to the package f , i , j , k = schedule . get_indices () ii , jj = schedule . tile (( i , j ), ( 16 , 16 )) # loop tiling schedule . reorder ( j , i , f , k , jj , ii ) # loop reordering plan = schedule . create_plan () plan . unroll ( ii ) # loop unrolling package . add ( plan , args = ( A , B , C ), base_name = \"matmul_relu_fusion_transformed\" ) # build a dynamically-linked package (a .dll or .so) that exports both functions print ( package . build ( name = \"hello_accera\" , format = acc . Package . Format . HAT_DYNAMIC )) Ensure that you have a compiler in your PATH: Windows: Install Microsoft Visual Studio and run vcvars64.bat to setup the command prompt Linux/macOS: Install gcc Don't have a compiler handy? We recommend trying Accera in your browser instead Install Accera: pip install accera Generate the library that implements two versions of matmul + ReLU: python quickstart.py To consume and compare the library functions, create a file called benchmark.py in the same location: import hatlib as hat import numpy as np # load the package hat_package = hat . load ( \"hello_accera.hat\" ) # call one of the functions with test inputs A_test = np . random . rand ( 512 , 512 ) . astype ( np . float32 ) B_test = np . random . rand ( 512 , 512 ) . astype ( np . float32 ) C_test = np . zeros (( 512 , 512 )) . astype ( np . float32 ) C_numpy = np . maximum ( C_test + A_test @ B_test , 0.0 ) matmul_relu = hat_package [ \"matmul_relu_fusion_transformed\" ] matmul_relu ( A_test , B_test , C_test ) # check correctness np . testing . assert_allclose ( C_test , C_numpy , atol = 1e-3 ) # benchmark all functions hat . run_benchmark ( \"hello_accera.hat\" , batch_size = 5 , min_time_in_sec = 5 ) Run the benchmark to get the timing results: python benchmark.py","title":"Run on your machine"},{"location":"#next-steps","text":"The Manual is a good place to start for an introduction to the Accera Python programming model. In particular, the schedule transformations describe how you can experiment with different loop transformations with just a few lines of Python. Finally, the .hat format is just a C header file containing metadata. Learn more about the HAT format and benchmarking .","title":"Next Steps"},{"location":"#how-it-works","text":"In a nutshell, Accera takes the Python code that defines the loop schedule and algorithm and converts it into MLIR intermediate representation (IR). Accera's compiler then takes this IR through a series of MLIR pipelines to perform transformations. The result is a binary library with a C header file. The library implements the algorithms that are defined in Python, and is compatible with the target. To peek into the stages of IR transformation that Accera does, try replacing format=acc.Package.Format.HAT_DYNAMIC with format=acc.Package.Format.MLIR_DYNAMIC in quickstart.py , re-run the script, and search the _tmp subfolder for the intermediate *.mlir files. We plan to document these IR constructs in the future.","title":"How it works"},{"location":"#documentation","text":"Get to know Accera's concepts and Python constructs in the Documentation page.","title":"Documentation"},{"location":"#tutorials","text":"More step-by-step examples are available on the Tutorials page. We're working on more examples and tutorials soon.","title":"Tutorials"},{"location":"#contributions","text":"Accera is a research platform-in-progress. We would love your contributions, feedback, questions, and feature requests! Please file a Github issue or send us a pull request. Please review the Microsoft Code of Conduct to learn more.","title":"Contributions"},{"location":"#credits","text":"Accera is built using several open source libraries, including: LLVM , pybind11 , toml++ , tomlkit , vcpkg , pyyaml , and HAT . For testing, we also use numpy and catch2 .","title":"Credits"},{"location":"#license","text":"This project is released under the MIT License .","title":"License"},{"location":"Install/","text":"Install from PyPI The quickest way to get up and running is to install the pre-built Python packages: MacOS Ubuntu Windows Build and Install You can also build and install the latest version of Accera by following these instructions: MacOS Ubuntu Windows","title":"Index"},{"location":"Install/#install-from-pypi","text":"The quickest way to get up and running is to install the pre-built Python packages: MacOS Ubuntu Windows","title":"Install from PyPI"},{"location":"Install/#build-and-install","text":"You can also build and install the latest version of Accera by following these instructions: MacOS Ubuntu Windows","title":"Build and Install"},{"location":"Install/Building_on_MacOS/","text":"Building on MacOS Requirements Accera requires the following tools and libraries: A C++ compiler that supports C++ 17, such as clang , which is bundled in XCode CMake 3.14 or newer Python 3.7 or newer Ninja Ccache LLVM OpenMP 5, if using parallelization Homebrew is a package manager that makes it easy to install the prerequesits. Homebrew can be downloaded and installed by: /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" If you already have Homebrew installed, update it to the latest version by typing: brew update Install the dependencies: brew install cmake python@3.7 ninja-build ccache libomp Clang Select the clang compiler from XCode: xcode-select --install Clone Accera A version of git should already be included in XCode. Clone the git repository: git clone --recurse-submodules https://github.com/microsoft/Accera Build and install Accera Run the build.sh script to install dependencies and build the Accera Python package (replace <path_to_accera> with the path to the cloned Accera repository). cd <path_to_accera> sh ./build.sh This typically takes a few hours to build and then install Accera's fork of LLVM. We recommend you reserve at least 20GB of disk space for the LLVM build. Update or install the resulting .whl file from the dist sudirectory. The name depends on your Python version, your OS and your CPU architecture e.g. pip install -U ./dist/accera-0.0.1-cp37-cp37-macosx_10_15_x86_64.whl --find-links = dist Build and install using CMake Accera can also be built using CMake (intended for expert users). Build dependencies cd <path_to_accera> git submodule init git submodule update ./external/vcpkg/bootstrap-vcpkg.sh ./external/vcpkg/vcpkg install catch2 tomlplusplus accera-llvm --overlay-ports = external/llvm The last command typically takes a few hours to build and then install Accera's fork of LLVM. We recommend you reserve at least 20GB of disk space for the LLVM build. Configure CMake cd <path_to_accera> mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE = Release -G Ninja Build and run tests cmake --build . --config Release ctest -C Release Install cmake --build . --config Release --target install","title":"macOS"},{"location":"Install/Building_on_MacOS/#building-on-macos","text":"","title":"Building on MacOS"},{"location":"Install/Building_on_MacOS/#requirements","text":"Accera requires the following tools and libraries: A C++ compiler that supports C++ 17, such as clang , which is bundled in XCode CMake 3.14 or newer Python 3.7 or newer Ninja Ccache LLVM OpenMP 5, if using parallelization Homebrew is a package manager that makes it easy to install the prerequesits. Homebrew can be downloaded and installed by: /usr/bin/ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)\" If you already have Homebrew installed, update it to the latest version by typing: brew update Install the dependencies: brew install cmake python@3.7 ninja-build ccache libomp","title":"Requirements"},{"location":"Install/Building_on_MacOS/#clang","text":"Select the clang compiler from XCode: xcode-select --install","title":"Clang"},{"location":"Install/Building_on_MacOS/#clone-accera","text":"A version of git should already be included in XCode. Clone the git repository: git clone --recurse-submodules https://github.com/microsoft/Accera","title":"Clone Accera"},{"location":"Install/Building_on_MacOS/#build-and-install-accera","text":"Run the build.sh script to install dependencies and build the Accera Python package (replace <path_to_accera> with the path to the cloned Accera repository). cd <path_to_accera> sh ./build.sh This typically takes a few hours to build and then install Accera's fork of LLVM. We recommend you reserve at least 20GB of disk space for the LLVM build. Update or install the resulting .whl file from the dist sudirectory. The name depends on your Python version, your OS and your CPU architecture e.g. pip install -U ./dist/accera-0.0.1-cp37-cp37-macosx_10_15_x86_64.whl --find-links = dist","title":"Build and install Accera"},{"location":"Install/Building_on_MacOS/#build-and-install-using-cmake","text":"Accera can also be built using CMake (intended for expert users).","title":"Build and install using CMake"},{"location":"Install/Building_on_MacOS/#build-dependencies","text":"cd <path_to_accera> git submodule init git submodule update ./external/vcpkg/bootstrap-vcpkg.sh ./external/vcpkg/vcpkg install catch2 tomlplusplus accera-llvm --overlay-ports = external/llvm The last command typically takes a few hours to build and then install Accera's fork of LLVM. We recommend you reserve at least 20GB of disk space for the LLVM build.","title":"Build dependencies"},{"location":"Install/Building_on_MacOS/#configure-cmake","text":"cd <path_to_accera> mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE = Release -G Ninja","title":"Configure CMake"},{"location":"Install/Building_on_MacOS/#build-and-run-tests","text":"cmake --build . --config Release ctest -C Release","title":"Build and run tests"},{"location":"Install/Building_on_MacOS/#install","text":"cmake --build . --config Release --target install","title":"Install"},{"location":"Install/Building_on_Ubuntu/","text":"Building on Ubuntu Requirements Accera requires the following tools and libraries: A C++ compiler that supports C++ 17, such as GCC 8 CMake 3.14 or newer Python 3.7 or newer Ninja Ccache LLVM OpenMP 5, if using parallelization sudo apt update sudo apt-get install gcc-8 g++-8 cmake python3 python3-pip ninja-build ccache libomp-11-dev pkg-config zip Some Ubuntu distributions will install an older version of CMake. Check the version of cmake using cmake --version , and download a newer version if older than 3.14. Clone Accera Install git if you don't already have it: sudo apt-get install git Clone the git repository git clone --recurse-submodules https://github.com/microsoft/Accera Build and install Accera Run the build.sh script to install dependencies and build the Accera Python package (replace <path_to_accera> with the path to the cloned Accera repository). cd <path_to_accera> sh ./build.sh This typically takes a few hours to build and then install Accera's fork of LLVM. We recommend you reserve at least 20GB of disk space for the LLVM build. Update or install the resulting .whl files from the dist sudirectory. The --find-links option tells pip to look at the dist subdirectory for the dependent packages. The name depends on your Python version, your OS and your CPU architecture e.g. pip install -U ./dist/accera-0.0.1-cp37-cp37m-linux_x86_64.whl --find-links = dist Build and install using CMake Accera can also be built using CMake (intended for expert users). Build dependencies cd <path_to_accera> git submodule init git submodule update ./external/vcpkg/bootstrap-vcpkg.sh ./external/vcpkg/vcpkg install catch2 tomlplusplus accera-llvm --overlay-ports = external/llvm The last command typically takes a few hours to build and then install Accera's fork of LLVM. We recommend you reserve at least 20GB of disk space for the LLVM build. Configure CMake cd <path_to_accera> mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE = Release -G Ninja Build and run tests cmake --build . --config Release ctest -C Release Install cmake --build . --config Release --target install","title":"Ubuntu"},{"location":"Install/Building_on_Ubuntu/#building-on-ubuntu","text":"","title":"Building on Ubuntu"},{"location":"Install/Building_on_Ubuntu/#requirements","text":"Accera requires the following tools and libraries: A C++ compiler that supports C++ 17, such as GCC 8 CMake 3.14 or newer Python 3.7 or newer Ninja Ccache LLVM OpenMP 5, if using parallelization sudo apt update sudo apt-get install gcc-8 g++-8 cmake python3 python3-pip ninja-build ccache libomp-11-dev pkg-config zip Some Ubuntu distributions will install an older version of CMake. Check the version of cmake using cmake --version , and download a newer version if older than 3.14.","title":"Requirements"},{"location":"Install/Building_on_Ubuntu/#clone-accera","text":"","title":"Clone Accera"},{"location":"Install/Building_on_Ubuntu/#install-git-if-you-dont-already-have-it","text":"sudo apt-get install git","title":"Install git if you don't already have it:"},{"location":"Install/Building_on_Ubuntu/#clone-the-git-repository","text":"git clone --recurse-submodules https://github.com/microsoft/Accera","title":"Clone the git repository"},{"location":"Install/Building_on_Ubuntu/#build-and-install-accera","text":"Run the build.sh script to install dependencies and build the Accera Python package (replace <path_to_accera> with the path to the cloned Accera repository). cd <path_to_accera> sh ./build.sh This typically takes a few hours to build and then install Accera's fork of LLVM. We recommend you reserve at least 20GB of disk space for the LLVM build. Update or install the resulting .whl files from the dist sudirectory. The --find-links option tells pip to look at the dist subdirectory for the dependent packages. The name depends on your Python version, your OS and your CPU architecture e.g. pip install -U ./dist/accera-0.0.1-cp37-cp37m-linux_x86_64.whl --find-links = dist","title":"Build and install Accera"},{"location":"Install/Building_on_Ubuntu/#build-and-install-using-cmake","text":"Accera can also be built using CMake (intended for expert users).","title":"Build and install using CMake"},{"location":"Install/Building_on_Ubuntu/#build-dependencies","text":"cd <path_to_accera> git submodule init git submodule update ./external/vcpkg/bootstrap-vcpkg.sh ./external/vcpkg/vcpkg install catch2 tomlplusplus accera-llvm --overlay-ports = external/llvm The last command typically takes a few hours to build and then install Accera's fork of LLVM. We recommend you reserve at least 20GB of disk space for the LLVM build.","title":"Build dependencies"},{"location":"Install/Building_on_Ubuntu/#configure-cmake","text":"cd <path_to_accera> mkdir build cd build cmake .. -DCMAKE_BUILD_TYPE = Release -G Ninja","title":"Configure CMake"},{"location":"Install/Building_on_Ubuntu/#build-and-run-tests","text":"cmake --build . --config Release ctest -C Release","title":"Build and run tests"},{"location":"Install/Building_on_Ubuntu/#install","text":"cmake --build . --config Release --target install","title":"Install"},{"location":"Install/Building_on_Windows/","text":"Building on Windows Requirements Visual Studio Accera requires a C++ compiler that supports C++ 17. You can download Visual Studio 2019 Enterprise Edition or Visual Studio 2022 Community Edition . Install Update 10 or later which includes the LLVM OpenMP libraries only for VS 2019. Select Desktop Development with C++ . Accera requires Spectre-mitigated libraries : 1. Go to Indivudual Components 2. Type in \"Spectre\" in the search box 3. Select the latest version of the MSVC libraries, e.g. MSVC v142 - VS 2019 C++ x64/x86 Spectre-mitigated libs (Latest) (your actual version may vary) CMake Accera requires CMake 3.14 or newer. A version of CMake that satisfies this requirement is included with Visual Studio 2019 and Visual Studio 2022. Python Accera's packages require Python 3.7 64-bit or newer, plus a version of pip that supports 64-bit packages ( win_amd64 ). One way to obtain this is to download and install Miniconda . Download \"Miniconda3 Windows 64-bit\". Optional: Create a conda environment After installing Miniconda, you can optionally create an environment to manage different Python versions. From an \"Anaconda Prompt\", create and then activate an environment for Python 3.7 (or a newer version if you prefer). Make sure to activate an environment from other applications as well that you use for development of Accera. conda create -n py37 python = 3 .7 conda activate py37 Clone Accera Visual Studio 2019 and 2022 include a version of git . To use it, launch Visual Studio 2019 or 2022, and select Clone a repository . Repository location: https://github.com/microsoft/Accera Build and install Accera From a command line that has Python in the path, such as an Anaconda Command Prompt, run the build.bat script to install dependencies and build the Accera Python package. Replace <path_to_accera> with the path to the cloned Accera repository. cd <path_to_accera> build.bat This typically takes a few hours to build and then install Accera's fork of LLVM. We recommend you reserve at least 20GB of disk space for the LLVM build. Update or install the resulting .whl file from the dist sudirectory. The --find-links option tells pip to look at the dist subdirectory for the dependent packages. The whl filename depends on your Python version, your OS and your CPU architecture e.g. pip install -U dist \\a ccera-0.0.1-cp37-cp37m-win_amd64.whl --find-links = dist Build and install using CMake Accera can also be built using CMake (intended for expert users). Build dependencies cd <path_to_accera> git submodule init git submodule update external \\v cpkg \\b ootstrap-vcpkg.bat external \\v cpkg \\v cpkg install catch2:x64-windows tomlplusplus:x64-windows accera-llvm:x64-windows --overlay-ports = external \\l lvm The last command typically takes a few hours to build and then install Accera's fork of LLVM. We recommend you reserve at least 20GB of disk space for the LLVM build. Configure CMake cd <path_to_accera> mkdir build cd build # For Visual Studio 2019: cmake .. -DCMAKE_BUILD_TYPE = Release -G \"Visual Studio 16 2019\" -Ax64 # For Visual Studio 2022: cmake .. -DCMAKE_BUILD_TYPE = Release -G \"Visual Studio 17 2022\" -Ax64 Build and run tests cmake --build . --config Release -- /m ctest -C Release Install cmake --build . --config Release --target install -- /m","title":"Windows"},{"location":"Install/Building_on_Windows/#building-on-windows","text":"","title":"Building on Windows"},{"location":"Install/Building_on_Windows/#requirements","text":"","title":"Requirements"},{"location":"Install/Building_on_Windows/#visual-studio","text":"Accera requires a C++ compiler that supports C++ 17. You can download Visual Studio 2019 Enterprise Edition or Visual Studio 2022 Community Edition . Install Update 10 or later which includes the LLVM OpenMP libraries only for VS 2019. Select Desktop Development with C++ . Accera requires Spectre-mitigated libraries : 1. Go to Indivudual Components 2. Type in \"Spectre\" in the search box 3. Select the latest version of the MSVC libraries, e.g. MSVC v142 - VS 2019 C++ x64/x86 Spectre-mitigated libs (Latest) (your actual version may vary)","title":"Visual Studio"},{"location":"Install/Building_on_Windows/#cmake","text":"Accera requires CMake 3.14 or newer. A version of CMake that satisfies this requirement is included with Visual Studio 2019 and Visual Studio 2022.","title":"CMake"},{"location":"Install/Building_on_Windows/#python","text":"Accera's packages require Python 3.7 64-bit or newer, plus a version of pip that supports 64-bit packages ( win_amd64 ). One way to obtain this is to download and install Miniconda . Download \"Miniconda3 Windows 64-bit\".","title":"Python"},{"location":"Install/Building_on_Windows/#optional-create-a-conda-environment","text":"After installing Miniconda, you can optionally create an environment to manage different Python versions. From an \"Anaconda Prompt\", create and then activate an environment for Python 3.7 (or a newer version if you prefer). Make sure to activate an environment from other applications as well that you use for development of Accera. conda create -n py37 python = 3 .7 conda activate py37","title":"Optional: Create a conda environment"},{"location":"Install/Building_on_Windows/#clone-accera","text":"Visual Studio 2019 and 2022 include a version of git . To use it, launch Visual Studio 2019 or 2022, and select Clone a repository . Repository location: https://github.com/microsoft/Accera","title":"Clone Accera"},{"location":"Install/Building_on_Windows/#build-and-install-accera","text":"From a command line that has Python in the path, such as an Anaconda Command Prompt, run the build.bat script to install dependencies and build the Accera Python package. Replace <path_to_accera> with the path to the cloned Accera repository. cd <path_to_accera> build.bat This typically takes a few hours to build and then install Accera's fork of LLVM. We recommend you reserve at least 20GB of disk space for the LLVM build. Update or install the resulting .whl file from the dist sudirectory. The --find-links option tells pip to look at the dist subdirectory for the dependent packages. The whl filename depends on your Python version, your OS and your CPU architecture e.g. pip install -U dist \\a ccera-0.0.1-cp37-cp37m-win_amd64.whl --find-links = dist","title":"Build and install Accera"},{"location":"Install/Building_on_Windows/#build-and-install-using-cmake","text":"Accera can also be built using CMake (intended for expert users).","title":"Build and install using CMake"},{"location":"Install/Building_on_Windows/#build-dependencies","text":"cd <path_to_accera> git submodule init git submodule update external \\v cpkg \\b ootstrap-vcpkg.bat external \\v cpkg \\v cpkg install catch2:x64-windows tomlplusplus:x64-windows accera-llvm:x64-windows --overlay-ports = external \\l lvm The last command typically takes a few hours to build and then install Accera's fork of LLVM. We recommend you reserve at least 20GB of disk space for the LLVM build.","title":"Build dependencies"},{"location":"Install/Building_on_Windows/#configure-cmake","text":"cd <path_to_accera> mkdir build cd build # For Visual Studio 2019: cmake .. -DCMAKE_BUILD_TYPE = Release -G \"Visual Studio 16 2019\" -Ax64 # For Visual Studio 2022: cmake .. -DCMAKE_BUILD_TYPE = Release -G \"Visual Studio 17 2022\" -Ax64","title":"Configure CMake"},{"location":"Install/Building_on_Windows/#build-and-run-tests","text":"cmake --build . --config Release -- /m ctest -C Release","title":"Build and run tests"},{"location":"Install/Building_on_Windows/#install","text":"cmake --build . --config Release --target install -- /m","title":"Install"},{"location":"Install/Installing_Accera_on_MacOS/","text":"Installing on MacOS Install dependencies Accera requires the following tools and libraries for building the generated code: A C++ compiler, such as clang , which is bundled in XCode Python 3.7 or newer OpenMP 5, if using parallelization Homebrew is a package manager that makes it easy to install the prerequesits. Homebrew can be downloaded and installed by: /usr/bin/ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \" If you already have Homebrew installed, update it to the latest version by typing: brew update Install the dependencies: brew install cmake python@3.7 Install the optional dependency if using parallelization: brew install libomp Clang Select the clang compiler from XCode: xcode-select --install Install Accera The accera Python package can be installed from PyPI: pip install accera","title":"macOS"},{"location":"Install/Installing_Accera_on_MacOS/#installing-on-macos","text":"","title":"Installing on MacOS"},{"location":"Install/Installing_Accera_on_MacOS/#install-dependencies","text":"Accera requires the following tools and libraries for building the generated code: A C++ compiler, such as clang , which is bundled in XCode Python 3.7 or newer OpenMP 5, if using parallelization Homebrew is a package manager that makes it easy to install the prerequesits. Homebrew can be downloaded and installed by: /usr/bin/ruby -e \" $( curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install ) \" If you already have Homebrew installed, update it to the latest version by typing: brew update Install the dependencies: brew install cmake python@3.7 Install the optional dependency if using parallelization: brew install libomp","title":"Install dependencies"},{"location":"Install/Installing_Accera_on_MacOS/#clang","text":"Select the clang compiler from XCode: xcode-select --install","title":"Clang"},{"location":"Install/Installing_Accera_on_MacOS/#install-accera","text":"The accera Python package can be installed from PyPI: pip install accera","title":"Install Accera"},{"location":"Install/Installing_Accera_on_Ubuntu/","text":"Installing on Ubuntu Install dependencies Accera requires the following tools and libraries for building the generated code: A C++ compiler, such as GCC 8 Python 3.7 or newer OpenMP 5, if using parallelization Ubuntu 20.04 is recommended. A quick way to start is to use a fresh Docker container for Ubuntu 20.04: docker run -v $PWD :/code -it --entrypoint \"/bin/bash\" ubuntu:focal Install Accera's dependencies: apt update apt-get install gcc-8 g++-8 python3 python3-pip libncurses5 Install the optional dependency if using parallelization: apt-get install libomp-11-dev Install Accera The accera Python package can be installed from PyPI: pip install accera","title":"Ubuntu"},{"location":"Install/Installing_Accera_on_Ubuntu/#installing-on-ubuntu","text":"","title":"Installing on Ubuntu"},{"location":"Install/Installing_Accera_on_Ubuntu/#install-dependencies","text":"Accera requires the following tools and libraries for building the generated code: A C++ compiler, such as GCC 8 Python 3.7 or newer OpenMP 5, if using parallelization Ubuntu 20.04 is recommended. A quick way to start is to use a fresh Docker container for Ubuntu 20.04: docker run -v $PWD :/code -it --entrypoint \"/bin/bash\" ubuntu:focal Install Accera's dependencies: apt update apt-get install gcc-8 g++-8 python3 python3-pip libncurses5 Install the optional dependency if using parallelization: apt-get install libomp-11-dev","title":"Install dependencies"},{"location":"Install/Installing_Accera_on_Ubuntu/#install-accera","text":"The accera Python package can be installed from PyPI: pip install accera","title":"Install Accera"},{"location":"Install/Installing_Accera_on_Windows/","text":"Installing on Windows Install dependencies Visual Studio Accera's generated code requires a C++ compiler. Download Visual Studio 2019 Enterprise Edition or Visual Studio 2022 Community Edition , and select Desktop development with C++ during installation. If you've selected VS 2019 and would like to use parallelization, ensure that Update 10 or later is installed. Both VS 2019 Update 10 or later and VS 2022 will include the LLVM OpenMP libraries. Python Accera's packages require Python 3.7 64-bit or newer, plus a version of pip that supports 64-bit packages ( win_amd64 ). One way to obtain this is to download and install Miniconda . Download \"Miniconda3 Windows 64-bit\". Optional: Create a conda environment After installing Miniconda, you can optionally to create an environment to manage different Python versions. From an \"Anaconda Prompt\", create and then activate an environment for Python 3.7 (or a newer version if you prefer): conda create -n py37 python = 3 .7 conda activate py37 Install Accera The accera Python package can be installed from PyPI: pip install accera","title":"Windows"},{"location":"Install/Installing_Accera_on_Windows/#installing-on-windows","text":"","title":"Installing on Windows"},{"location":"Install/Installing_Accera_on_Windows/#install-dependencies","text":"","title":"Install dependencies"},{"location":"Install/Installing_Accera_on_Windows/#visual-studio","text":"Accera's generated code requires a C++ compiler. Download Visual Studio 2019 Enterprise Edition or Visual Studio 2022 Community Edition , and select Desktop development with C++ during installation. If you've selected VS 2019 and would like to use parallelization, ensure that Update 10 or later is installed. Both VS 2019 Update 10 or later and VS 2022 will include the LLVM OpenMP libraries.","title":"Visual Studio"},{"location":"Install/Installing_Accera_on_Windows/#python","text":"Accera's packages require Python 3.7 64-bit or newer, plus a version of pip that supports 64-bit packages ( win_amd64 ). One way to obtain this is to download and install Miniconda . Download \"Miniconda3 Windows 64-bit\".","title":"Python"},{"location":"Install/Installing_Accera_on_Windows/#optional-create-a-conda-environment","text":"After installing Miniconda, you can optionally to create an environment to manage different Python versions. From an \"Anaconda Prompt\", create and then activate an environment for Python 3.7 (or a newer version if you prefer): conda create -n py37 python = 3 .7 conda activate py37","title":"Optional: Create a conda environment"},{"location":"Install/Installing_Accera_on_Windows/#install-accera","text":"The accera Python package can be installed from PyPI: pip install accera","title":"Install Accera"},{"location":"Manual/","text":"Accera v1.2.1 Manual Introduction Arrays Simple Affine Loop Nests Schedules Fusing Targets Plans - Caching Plans - Vectorization and Parallelization Deferred layout of constant arrays Parameters Packages","title":"Index"},{"location":"Manual/#accera-v121-manual","text":"Introduction Arrays Simple Affine Loop Nests Schedules Fusing Targets Plans - Caching Plans - Vectorization and Parallelization Deferred layout of constant arrays Parameters Packages","title":"Accera v1.2.1 Manual"},{"location":"Manual/00%20Introduction/","text":"Introduction Accera is a programming model, a domain-specific programming language embedded in Python (eDSL), and an optimizing cross-compiler for compute-intensive code. Accera currently supports CPU and GPU targets and focuses on optimization of nested for-loops. Writing highly optimized compute-intensive code in a traditional programming language is a difficult and time-consuming process. It requires special engineering skills, such as fluency in Assembly language and a deep understanding of computer architecture. Manually optimizing the simplest numerical algorithms already requires a significant engineering effort. Moreover, highly optimized numerical code is prone to bugs, is often hard to read and maintain, and needs to be reimplemented every time a new target architecture is introduced. Accera aims to solve these problems. Accera has three goals: Performance : generate the fastest implementation of any compute-intensive algorithm. Readability : do so without sacrificing code readability and maintainability. Writability : a user-friendly programming model, designed for agility. The Accera language was designed with the following guiding principles in mind: 1: Strict separation of logic from implementation Traditional programming languages tend to tightly couple the code logic ( what the program does) with its implementation ( how the program is implemented). For example, consider the simple example of multiplying a 16\u00d711 matrix A by a 11\u00d710 matrix B . The logic of the algorithm is to calculate, for each value of i and j , the sum over k of A[i,k]\u00b7B[k,j] . In Python, this logic can be expressed as # C += A @ B for i in range ( 16 ): for j in range ( 10 ): for k in range ( 11 ): C [ i , j ] += A [ i , k ] * B [ k , j ] However, the code above expresses more than just the logic of matrix multiplication, it also specifies a concrete plan for executing this logic: first perform all the work required to calculate C(0,0) in ascending order of k ; then proceed to C(0,1) ; etc. In principle, the iterations of this loop could be performed in any order and the logic would remain intact, but the code above insists on one specific order. Moreover, this code doesn't take advantage of important optimization techniques, such as double-buffered caching or vectorization. In contrast, the Accera programming model draws a strict distinction between the logic and its implementation. Namely, the programmer first writes the logic using a pseudocode-like syntax, independent of the target platform and without any consideration for performance. After the abstract logic is specified, the programmer moves on to define the concrete implementation details. 2: Mindfully trade-off safety versus expressivity The Accera programming model starts with a default implementation of the specified logic and allows the programmer to transform and manipulate that implementation in different ways. When used correctly, these transformations should be safe , which means that they do not influence the underlying logic. Using safe transformations allows the programmer to focus on performance, without having to worry about correctness. Moreover, safe transformations allow automatic search algorithms to search the space of transformations more aggressively, converge faster, and find better optima. However, safety is usually achieved by restricting and constraining a programming language. Excessive restrictions can limit the expressivity and the power of a language and prevent its users from creating highly-sophisticated and highly-optimized implementations. The Accera programming model navigates the trade-off between safety and expressivity by being very explicit about the safety guarantees provided by each transformation under different circumstances. Some situations are safer than others, but in all cases, the programmer knows exactly what safety guarantees are given. 3: The programmer is in control The Accera language gives the programmer maximal control over the generated code and avoids under-the-hood magic that cannot be overridden or controlled. Convenience methods and carefully chosen default values prevent verbosity, but the programmer can always override these and fine-tune the implementation as they see fit.","title":"Introduction"},{"location":"Manual/00%20Introduction/#introduction","text":"Accera is a programming model, a domain-specific programming language embedded in Python (eDSL), and an optimizing cross-compiler for compute-intensive code. Accera currently supports CPU and GPU targets and focuses on optimization of nested for-loops. Writing highly optimized compute-intensive code in a traditional programming language is a difficult and time-consuming process. It requires special engineering skills, such as fluency in Assembly language and a deep understanding of computer architecture. Manually optimizing the simplest numerical algorithms already requires a significant engineering effort. Moreover, highly optimized numerical code is prone to bugs, is often hard to read and maintain, and needs to be reimplemented every time a new target architecture is introduced. Accera aims to solve these problems. Accera has three goals: Performance : generate the fastest implementation of any compute-intensive algorithm. Readability : do so without sacrificing code readability and maintainability. Writability : a user-friendly programming model, designed for agility. The Accera language was designed with the following guiding principles in mind:","title":"Introduction"},{"location":"Manual/00%20Introduction/#1-strict-separation-of-logic-from-implementation","text":"Traditional programming languages tend to tightly couple the code logic ( what the program does) with its implementation ( how the program is implemented). For example, consider the simple example of multiplying a 16\u00d711 matrix A by a 11\u00d710 matrix B . The logic of the algorithm is to calculate, for each value of i and j , the sum over k of A[i,k]\u00b7B[k,j] . In Python, this logic can be expressed as # C += A @ B for i in range ( 16 ): for j in range ( 10 ): for k in range ( 11 ): C [ i , j ] += A [ i , k ] * B [ k , j ] However, the code above expresses more than just the logic of matrix multiplication, it also specifies a concrete plan for executing this logic: first perform all the work required to calculate C(0,0) in ascending order of k ; then proceed to C(0,1) ; etc. In principle, the iterations of this loop could be performed in any order and the logic would remain intact, but the code above insists on one specific order. Moreover, this code doesn't take advantage of important optimization techniques, such as double-buffered caching or vectorization. In contrast, the Accera programming model draws a strict distinction between the logic and its implementation. Namely, the programmer first writes the logic using a pseudocode-like syntax, independent of the target platform and without any consideration for performance. After the abstract logic is specified, the programmer moves on to define the concrete implementation details.","title":"1: Strict separation of logic from implementation"},{"location":"Manual/00%20Introduction/#2-mindfully-trade-off-safety-versus-expressivity","text":"The Accera programming model starts with a default implementation of the specified logic and allows the programmer to transform and manipulate that implementation in different ways. When used correctly, these transformations should be safe , which means that they do not influence the underlying logic. Using safe transformations allows the programmer to focus on performance, without having to worry about correctness. Moreover, safe transformations allow automatic search algorithms to search the space of transformations more aggressively, converge faster, and find better optima. However, safety is usually achieved by restricting and constraining a programming language. Excessive restrictions can limit the expressivity and the power of a language and prevent its users from creating highly-sophisticated and highly-optimized implementations. The Accera programming model navigates the trade-off between safety and expressivity by being very explicit about the safety guarantees provided by each transformation under different circumstances. Some situations are safer than others, but in all cases, the programmer knows exactly what safety guarantees are given.","title":"2: Mindfully trade-off safety versus expressivity"},{"location":"Manual/00%20Introduction/#3-the-programmer-is-in-control","text":"The Accera language gives the programmer maximal control over the generated code and avoids under-the-hood magic that cannot be overridden or controlled. Convenience methods and carefully chosen default values prevent verbosity, but the programmer can always override these and fine-tune the implementation as they see fit.","title":"3: The programmer is in control"},{"location":"Manual/01%20Arrays/","text":"Section 1: Arrays Accera stores data in multidimensional arrays of scalar elements. All of the elements of an array share the same basic type (e.g., float32, int8). An array has a fixed dimension, denoted by d , which is known at compile-time (e.g., a matrix is a 2-dimensional array). Each dimension has a positive size and the sequence of d sizes is called the shape of the array. An element of an array is referred to by a d -coordinate zero-based index vector . Affine memory layout Arrays are multidimensional, while computer memories have a linear (one-dimensional) address space. There are many ways to lay out a multidimensional array in the one-dimensional computer memory. Accera arrays are required to have an affine memory layout. This means that each array has an affine memory map , which is d -dimensional vector denoted by a , and a memory offset value denoted by o : the array element that corresponds to the index vector i is stored at memory address i\u00b7a+o (where i\u00b7a denotes a vector dot product). Affine memory maps are rich enough to represent many standard array layouts. For example, for 2-dimensional arrays (matrices), affine maps can represent row-major , column-major , triangular , banded , and Toeplitz matrices. On the other hand, affine maps cannot represent z-ordering or other striped or blocked layouts. Array shape Each dimension corresponds to an element in the affine memory map, and the dimension whose element is the largest in absolute value is called the major dimension . The shape of an array is defined at compile-time, with the possible exception of the major dimension size, which can remain undefined. The major dimension is the only one whose size is not necessarily defined at compile time. If the major dimension size is not defined, Accera assumes that the size is arbitrary (or infinite). In other words, the sizes of the loops determine how much of the array is visited along this dimension. For example, a row-major matrix must have a compile-time-constant number of columns, but the number of rows can be left undefined and the sizes of the loops control how many rows are processed. Default and inferred memory layout The memory map can be explicitly specified by the user, but Accera also offers some shortcuts. The user can set the layout to be FIRST_MAJOR (e.g., for two dimensional arrays, first-major is equivalent to row-major) or LAST_MAJOR . In both cases, the affine map is inferred from the array shape. Specifically, if the layout is LAST_MAJOR and the shape is denoted by the vector s , then the map a is set to [1, s0, s0\u00d7s1, s0\u00d7s1\u00d7s2, ...] . If the layout is FIRST_MAJOR and the dimension equals 4, then a is set to [s0\u00d7s1\u00d7s2, s1\u00d7s2, s2, 1] . Note that, in both cases, the size of the major dimension is not used in the definition of a , which hints as to why the major dimension size does not need to be defined at compile time. If no layout is specified, the default layout is FIRST_MAJOR . Array properties Accera arrays are either defined with internal scope or external scope. An internal array only exists inside a specific Accera function and is not available outside of that function. An external array is define outside of an Accera function and passed in as an argument. The memory layout of an external array is specified as part of the Accera function signature. External arrays are assumed to be disjoint, namely, they do not share any memory with each other. Accera arrays are either mutable or immutable. The elements of a mutable array can be set by an Accera function, while an immutable array is read-only. Array properties are not explicitly set by the programmer, but are implied by the role of the array (see below). Array roles Accera supports four different array roles, which are input , input/output , constant , and temporary . Each role is treated differently by the Accera compiler. Input arrays Input arrays are immutable external arrays. Their element type, shape, and affine layout are known at compile time, but their contents are only available at runtime. If the Accera function is emitted as a function in C, each input array is passed in as a const pointer argument. For example, we can construct a 10\u00d720 input array of 32-bit floating-point numbers by writing import accera as acc A = acc . Array ( shape = ( 10 , 20 ), role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 ) The layout of this array would be the default layout, which is acc.Array.Layout.FIRST_MAJOR . Input/output arrays Input/output arrays are mutable external arrays. They are similar to input arrays, but their values can be changed. Input/output arrays are used to output the results of the loop-nest computation. If the Accera function is emitted as a function in C, each input array is passed in as a non-const pointer argument. Constant arrays Constant arrays are immutable internal arrays. They are the only type of array whose contents are known at compile-time. Since they are internally scoped, their memory layout can be chosen automatically without any external constraints. For example, a constant array can be automatically laid out according to the loop-nest's memory access pattern. The layout of a constant array could even depend on its contents (e.g., its sparsity pattern). We must provide the constant array data (the element values) when we construct it. This data can be any Python buffer, or a numpy array: import accera as acc import numpy as np matrix = np . random . rand ( 16 , 16 ) B = acc . Array ( role = acc . Array . Role . CONST , data = matrix ) Temporary arrays Temporary arrays are mutable internal arrays, and are used when two Accera schedules are fused into one (more on fusing in Section 4 ). The elements of a temporary array are initialized to zeros and used to store intermediate values. Like constant arrays, temporary arrays can be laid out arbitrarily and, in fact, the Accera compiler can choose not to store them in physical memory at all (more on this later).","title":"Arrays"},{"location":"Manual/01%20Arrays/#section-1-arrays","text":"Accera stores data in multidimensional arrays of scalar elements. All of the elements of an array share the same basic type (e.g., float32, int8). An array has a fixed dimension, denoted by d , which is known at compile-time (e.g., a matrix is a 2-dimensional array). Each dimension has a positive size and the sequence of d sizes is called the shape of the array. An element of an array is referred to by a d -coordinate zero-based index vector .","title":"Section 1: Arrays"},{"location":"Manual/01%20Arrays/#affine-memory-layout","text":"Arrays are multidimensional, while computer memories have a linear (one-dimensional) address space. There are many ways to lay out a multidimensional array in the one-dimensional computer memory. Accera arrays are required to have an affine memory layout. This means that each array has an affine memory map , which is d -dimensional vector denoted by a , and a memory offset value denoted by o : the array element that corresponds to the index vector i is stored at memory address i\u00b7a+o (where i\u00b7a denotes a vector dot product). Affine memory maps are rich enough to represent many standard array layouts. For example, for 2-dimensional arrays (matrices), affine maps can represent row-major , column-major , triangular , banded , and Toeplitz matrices. On the other hand, affine maps cannot represent z-ordering or other striped or blocked layouts.","title":"Affine memory layout"},{"location":"Manual/01%20Arrays/#array-shape","text":"Each dimension corresponds to an element in the affine memory map, and the dimension whose element is the largest in absolute value is called the major dimension . The shape of an array is defined at compile-time, with the possible exception of the major dimension size, which can remain undefined. The major dimension is the only one whose size is not necessarily defined at compile time. If the major dimension size is not defined, Accera assumes that the size is arbitrary (or infinite). In other words, the sizes of the loops determine how much of the array is visited along this dimension. For example, a row-major matrix must have a compile-time-constant number of columns, but the number of rows can be left undefined and the sizes of the loops control how many rows are processed.","title":"Array shape"},{"location":"Manual/01%20Arrays/#default-and-inferred-memory-layout","text":"The memory map can be explicitly specified by the user, but Accera also offers some shortcuts. The user can set the layout to be FIRST_MAJOR (e.g., for two dimensional arrays, first-major is equivalent to row-major) or LAST_MAJOR . In both cases, the affine map is inferred from the array shape. Specifically, if the layout is LAST_MAJOR and the shape is denoted by the vector s , then the map a is set to [1, s0, s0\u00d7s1, s0\u00d7s1\u00d7s2, ...] . If the layout is FIRST_MAJOR and the dimension equals 4, then a is set to [s0\u00d7s1\u00d7s2, s1\u00d7s2, s2, 1] . Note that, in both cases, the size of the major dimension is not used in the definition of a , which hints as to why the major dimension size does not need to be defined at compile time. If no layout is specified, the default layout is FIRST_MAJOR .","title":"Default and inferred memory layout"},{"location":"Manual/01%20Arrays/#array-properties","text":"Accera arrays are either defined with internal scope or external scope. An internal array only exists inside a specific Accera function and is not available outside of that function. An external array is define outside of an Accera function and passed in as an argument. The memory layout of an external array is specified as part of the Accera function signature. External arrays are assumed to be disjoint, namely, they do not share any memory with each other. Accera arrays are either mutable or immutable. The elements of a mutable array can be set by an Accera function, while an immutable array is read-only. Array properties are not explicitly set by the programmer, but are implied by the role of the array (see below).","title":"Array properties"},{"location":"Manual/01%20Arrays/#array-roles","text":"Accera supports four different array roles, which are input , input/output , constant , and temporary . Each role is treated differently by the Accera compiler.","title":"Array roles"},{"location":"Manual/01%20Arrays/#input-arrays","text":"Input arrays are immutable external arrays. Their element type, shape, and affine layout are known at compile time, but their contents are only available at runtime. If the Accera function is emitted as a function in C, each input array is passed in as a const pointer argument. For example, we can construct a 10\u00d720 input array of 32-bit floating-point numbers by writing import accera as acc A = acc . Array ( shape = ( 10 , 20 ), role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 ) The layout of this array would be the default layout, which is acc.Array.Layout.FIRST_MAJOR .","title":"Input arrays"},{"location":"Manual/01%20Arrays/#inputoutput-arrays","text":"Input/output arrays are mutable external arrays. They are similar to input arrays, but their values can be changed. Input/output arrays are used to output the results of the loop-nest computation. If the Accera function is emitted as a function in C, each input array is passed in as a non-const pointer argument.","title":"Input/output arrays"},{"location":"Manual/01%20Arrays/#constant-arrays","text":"Constant arrays are immutable internal arrays. They are the only type of array whose contents are known at compile-time. Since they are internally scoped, their memory layout can be chosen automatically without any external constraints. For example, a constant array can be automatically laid out according to the loop-nest's memory access pattern. The layout of a constant array could even depend on its contents (e.g., its sparsity pattern). We must provide the constant array data (the element values) when we construct it. This data can be any Python buffer, or a numpy array: import accera as acc import numpy as np matrix = np . random . rand ( 16 , 16 ) B = acc . Array ( role = acc . Array . Role . CONST , data = matrix )","title":"Constant arrays"},{"location":"Manual/01%20Arrays/#temporary-arrays","text":"Temporary arrays are mutable internal arrays, and are used when two Accera schedules are fused into one (more on fusing in Section 4 ). The elements of a temporary array are initialized to zeros and used to store intermediate values. Like constant arrays, temporary arrays can be laid out arbitrarily and, in fact, the Accera compiler can choose not to store them in physical memory at all (more on this later).","title":"Temporary arrays"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/","text":"Section 2: Simple affine loop nests In this section we introduce loop nests and define the types of loops nests that appear in the Accera programming model. Affine loop nests Many important compute-intensive workloads can be expressed using nested for-loops. An algorithm that can be defined using nested for-loops is called a loop nest . Accera is restricted to the class of affine loop nests . A loop nest is affine if the indices of the elements accessed on each iteration are an affine function of the loop iterator variables. For example, the following loop nest is affine: for i in range ( M ): for j in range ( N ): C [ 2 * i + 2 , j + 2 ] += A [ 3 * i , j ] + B [ j , i ] because 2*i+2 , j+2 , 3*i , j and i are all affine functions of the iterator variables i and j . On the other hand, the following loop nest is not affine: for i in range ( M ): for j in range ( N ): C [ i * i , j ] += A [ i * i , j ] + B [ i * j , i ] because i*i and i*j are quadratic (non-affine) functions of i and j . Simple affine loops nests, a.k.a. simple nests An important subclass of affine loop nests is the class of simple affine loop nests , or just simple nests for short. An affine loop nest is simple if it satisfies the following properties: 1. The loops are perfectly nested : all the computation is entirely contained within the deepest loop. 2. All the loops are normalized : each loop starts at 0, increments by 1, and ends at a compile-time constant size. 3. The loop iterations are order invariant : the logic doesn't change if the loop iterations are executed in a different sequential order. 4. No conditional exit : the loop doesn't contain break or continue commands The matrix-matrix multiplication example given in the introduction is an example of a simple nest. Another example is 2-dimensional convolution , which is the fundamental operation in convolutional neural networks, and can be written in Python as: # Convolve M x N data matrix A with S x T filter matrix B and add output to matrix C for i in range ( M ): for j in range ( N ): for k in range ( S ): for l in range ( T ): C [ i , j ] += A [ i + k , j + l ] * B [ k , l ] While Accera supports arbitrary affine loop nests, the programmer defines the logic of their algorithm using simple nests. More complex nests are obtained by applying schedule transformations (see Section 3 ) or by fusing multiple schedules (see Section 4 ). Defining the loop nest logic The programmer's goal is to create a highly optimized target-specific implementation of an affine loop nest. The first step towards this goal is to define the logic of one or more simple nests. The logic is a target-independent pseudo-code of a simple nest, written without considering performance. For example, the following code defines the logic of the matrix-matrix multiplication loop nest: # Import accera import accera as acc # Define matrix sizes M = 16 N = 10 S = 11 A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( M , S )) B = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( S , N )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , element_type = acc . ScalarType . float32 , shape = ( M , N )) # Define a simple affine loop nest and name its loops i, j, k nest = acc . Nest ( shape = ( M , N , S )) i , j , k = nest . get_indices () # Define the logic of each iteration in the nest @nest . iteration_logic def _ (): C [ i , j ] += A [ i , k ] * B [ k , j ] We start by defining the arrays that participate in the computation: A and B are input arrays and C is an input/output array. Next, we initialize nest to be an empty skeleton of a loop nest, with nested loops of sizes M , N , S . These loops are logical -- think of them as pseudo-code loops -- they do not define the execution order of the iterations. The index variables that correspond to the three loops are named i, j, k respectively. The last part of the example sets the iteration logic to C[i, j] += A[i, k] * B[k, j] . Note that this iteration logic follows an affine memory access pattern. The syntax in the example makes use of Python decorators and is shorthand for the more explicit syntax: def logic_fn (): C [ i , j ] += A [ i , k ] * B [ k , j ] nest . iteration_logic ( logic_fn ) Supported operations The iteration logic can include the following operations (assuming accera was imported as rp ): Assignment operators Operation Types (Operands must be of same type) Description a = b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Assigns the value of scalar b to scalar a Not yet implemented: unsigned types ( acc.ScalarType.uint8/16/32/64 ) Arithmetic operators Operation Types (Operands must be of same type) Description a + b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the sum of scalars a and b a - b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the difference between scalars a and b a * b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the product of scalars a and b a / b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the quotient of scalars a and b . If the operands are integers, an integer division result is returned a ** b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the b 'th power of scalar a a // b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the floor of the quotient of scalars a and b a % b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the signed remainder after dividing scalar a by scalar b -a acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the additive inverse of scalar a Not yet implemented: unsigned types ( acc.ScalarType.uint8/16/32/64 ) Comment: Accera also supports the corresponding compound-assignment operators, such as a += b , a -= b , etc. Relational operators Operation Types (Operands must be of same type) Description a == b acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if scalar a equals scalar b , else False a != b acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if scalar a is not equal to scalar b , else False a < b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if scalar a is strictly smaller than scalar b , else False a <= b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if scalar a is smaller than or equal to scalar b , else False a > b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if scalar a is strictly greater than scalar b , else False a >= b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if scalar a is greater than or equal to scalar b , else False Not yet implemented: unsigned types ( acc.ScalarType.uint8/16/32/64 ) Logical operators Operation Types (Operands must be of same type) Description acc.logical_and(a, b) acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if scalars a and b are non-zero, else False acc.logical_or(a, b) acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if either scalar a or scalar b are non-zero, else False acc.logical_not(a) acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if a is zero, else False Not yet implemented: unsigned types ( acc.ScalarType.uint8/16/32/64 ) Bitwise operators Operation Types (Operands must be of same type) Description a & b acc.ScalarType.int8/16/32/64 Returns the bitwise AND of the bits in scalars a and b a \\| b acc.ScalarType.int8/16/32/64 Returns the bitwise OR of the bits in scalars a and b a ^ b acc.ScalarType.int8/16/32/64 Returns the bitwise XOR of the bits in scalars a and b ~a acc.ScalarType.int8/16/32/64 Returns the bitwise inverse of the bits in scalar a a << b acc.ScalarType.int8/16/32/64 Returns scalar a whose bitwise representation is shifted left by b bits a >> b acc.ScalarType.int8/16/32/64 Returns scalar a whose bitwise representation is shifted right by b bits Comment: Accera also supports the corresponding compound-assignment operators, such as a &= b , a |= b , etc. Not yet implemented: unsigned types ( acc.ScalarType.uint8/16/32/64 ) Intrinsics Operation Types (Operands must be of same type) Description acc.abs(a) acc.ScalarType.float32/64 Returns the absolute value of scalar a acc.max(a, b) acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the larger of the two scalars a and b acc.min(a, b) acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the smaller of the two scalars a and b acc.ceil(a) acc.ScalarType.float32/64 Returns the value of scalar a rounded up to the nearest integer as an int64 type acc.floor(a) acc.ScalarType.float32/64 Returns the value of scalar a rounded down to the nearest integer as an int64 type acc.sqrt(a) acc.ScalarType.float32/64 Returns the square root of scalar a acc.exp(a) acc.ScalarType.float32/64 Returns the exponential e raised to the scalar a acc.log(a) acc.ScalarType.float32/64 Returns the natural logarithm (base e ) of scalar a acc.log10(a) acc.ScalarType.float32/64 Returns the common logarithm (base 10) of scalar a acc.log2(a) acc.ScalarType.float32/64 Returns the binary logarithm (base 2) of scalar a acc.sin(a) acc.ScalarType.float32/64 Returns the sine of scalar a , where a is in radians acc.cos(a) acc.ScalarType.float32/64 Returns the cosine of scalar a , where a is in radians acc.tan(a) acc.ScalarType.float32/64 Returns the tangent of scalar a , where a is in radians acc.sinh(a) acc.ScalarType.float32/64 Returns the hyperbolic sine of scalar a , where a is in radians acc.cosh(a) acc.ScalarType.float32/64 Returns the hyperbolic cosine of scalar a , where a is in radians acc.tanh(a) acc.ScalarType.float32/64 Returns the hyperbolic tangent of scalar a , where a is in radians Not yet implemented: unsigned types ( acc.ScalarType.uint8/16/32/64 ) Accera program stages We take a step back to describe the stages of an Accera program: Nest : A nest captures the logic of a simple nest, without any optimizations or implementation details. Schedule : A Nest is used to create a schedule. The schedule controls the order in which the nest iterations are visited. Multiple schedules can be fused into a single schedule, which may no longer represent a simple nest. Plan : A Schedule is used to create a plan. A plan controls the implementation details that are specific to a specific target platform (e.g., data caching strategy, vectorization, assignment of arrays and caches to different types of memory). Package : A Plan is used to create a function in a function package. The package is then compiled and emitted. Once a package is emitted, the Accera functions contained in it can be called from external client code. This external code is typically not written using Accera. Accera currently supports the following package formats: HAT , which is a schematized version of a standard C library. The external client code can be written in C or C++ and linked with the HAT package. MLIR , which uses standard MLIR dialects. The external code must also be in MLIR. Overall, to build and emit nest (defined above), we would write: # create a default schedule from the nest schedule = nest . create_schedule () # create a default plan from the schedule plan = schedule . create_plan () # create a HAT package. Create a function in the package based on the plan package = acc . Package () package . add ( plan , args = ( A , B , C ), base_name = \"simple_matmul\" ) # build the HAT package package . build ( format = acc . Package . Format . HAT_DYNAMIC , name = \"linear_algebra\" ) It may not be immediately clear why so many stages are needed just to compile a simple nest. The importance of each step will hopefully become clear once we describe the stages in detail. In the example above, The call to package.add takes three arguments: the first is the plan that defines the function's implementation; the second is the order of the input and input/output arrays in the function signature; and the third is a base name for the function. The full name of the function is the base name followed by an automatically-generated unique identifier. For example, the function in the example could appear in the package as simple_matmul_8f24bef5 . The automatically-generated suffix ensures that each function in the package has a unique name. More details on function packages can be found in Section 10 . Convenience syntax For convenience, Accera also provides shortcuts to avoid unneeded verbosity. Specifically, we can create a function in a package directly from a nest, as follows: package . add ( nest , args = ( A , B , C ), base_name = \"simple_matmul\" ) The abbreviated syntax makes it seem like a callable function is generated directly from nest , but what actually happens behind the scenes is that nest creates a default schedule, which creates a default plan, which is added as a function in the package. Accera has a similar convenience syntax to create a function from a schedule: package . add ( schedule , args = ( A , B , C ), base_name = \"simple_matmul\" ) and to create a plan directly from a nest: plan = nest . create_plan ()","title":"Simple Affine Loop Nests"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#section-2-simple-affine-loop-nests","text":"In this section we introduce loop nests and define the types of loops nests that appear in the Accera programming model.","title":"Section 2: Simple affine loop nests"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#affine-loop-nests","text":"Many important compute-intensive workloads can be expressed using nested for-loops. An algorithm that can be defined using nested for-loops is called a loop nest . Accera is restricted to the class of affine loop nests . A loop nest is affine if the indices of the elements accessed on each iteration are an affine function of the loop iterator variables. For example, the following loop nest is affine: for i in range ( M ): for j in range ( N ): C [ 2 * i + 2 , j + 2 ] += A [ 3 * i , j ] + B [ j , i ] because 2*i+2 , j+2 , 3*i , j and i are all affine functions of the iterator variables i and j . On the other hand, the following loop nest is not affine: for i in range ( M ): for j in range ( N ): C [ i * i , j ] += A [ i * i , j ] + B [ i * j , i ] because i*i and i*j are quadratic (non-affine) functions of i and j .","title":"Affine loop nests"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#simple-affine-loops-nests-aka-simple-nests","text":"An important subclass of affine loop nests is the class of simple affine loop nests , or just simple nests for short. An affine loop nest is simple if it satisfies the following properties: 1. The loops are perfectly nested : all the computation is entirely contained within the deepest loop. 2. All the loops are normalized : each loop starts at 0, increments by 1, and ends at a compile-time constant size. 3. The loop iterations are order invariant : the logic doesn't change if the loop iterations are executed in a different sequential order. 4. No conditional exit : the loop doesn't contain break or continue commands The matrix-matrix multiplication example given in the introduction is an example of a simple nest. Another example is 2-dimensional convolution , which is the fundamental operation in convolutional neural networks, and can be written in Python as: # Convolve M x N data matrix A with S x T filter matrix B and add output to matrix C for i in range ( M ): for j in range ( N ): for k in range ( S ): for l in range ( T ): C [ i , j ] += A [ i + k , j + l ] * B [ k , l ] While Accera supports arbitrary affine loop nests, the programmer defines the logic of their algorithm using simple nests. More complex nests are obtained by applying schedule transformations (see Section 3 ) or by fusing multiple schedules (see Section 4 ).","title":"Simple affine loops nests, a.k.a. simple nests"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#defining-the-loop-nest-logic","text":"The programmer's goal is to create a highly optimized target-specific implementation of an affine loop nest. The first step towards this goal is to define the logic of one or more simple nests. The logic is a target-independent pseudo-code of a simple nest, written without considering performance. For example, the following code defines the logic of the matrix-matrix multiplication loop nest: # Import accera import accera as acc # Define matrix sizes M = 16 N = 10 S = 11 A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( M , S )) B = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( S , N )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , element_type = acc . ScalarType . float32 , shape = ( M , N )) # Define a simple affine loop nest and name its loops i, j, k nest = acc . Nest ( shape = ( M , N , S )) i , j , k = nest . get_indices () # Define the logic of each iteration in the nest @nest . iteration_logic def _ (): C [ i , j ] += A [ i , k ] * B [ k , j ] We start by defining the arrays that participate in the computation: A and B are input arrays and C is an input/output array. Next, we initialize nest to be an empty skeleton of a loop nest, with nested loops of sizes M , N , S . These loops are logical -- think of them as pseudo-code loops -- they do not define the execution order of the iterations. The index variables that correspond to the three loops are named i, j, k respectively. The last part of the example sets the iteration logic to C[i, j] += A[i, k] * B[k, j] . Note that this iteration logic follows an affine memory access pattern. The syntax in the example makes use of Python decorators and is shorthand for the more explicit syntax: def logic_fn (): C [ i , j ] += A [ i , k ] * B [ k , j ] nest . iteration_logic ( logic_fn )","title":"Defining the loop nest logic"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#supported-operations","text":"The iteration logic can include the following operations (assuming accera was imported as rp ):","title":"Supported operations"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#assignment-operators","text":"Operation Types (Operands must be of same type) Description a = b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Assigns the value of scalar b to scalar a Not yet implemented: unsigned types ( acc.ScalarType.uint8/16/32/64 )","title":"Assignment operators"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#arithmetic-operators","text":"Operation Types (Operands must be of same type) Description a + b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the sum of scalars a and b a - b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the difference between scalars a and b a * b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the product of scalars a and b a / b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the quotient of scalars a and b . If the operands are integers, an integer division result is returned a ** b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the b 'th power of scalar a a // b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the floor of the quotient of scalars a and b a % b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the signed remainder after dividing scalar a by scalar b -a acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the additive inverse of scalar a Not yet implemented: unsigned types ( acc.ScalarType.uint8/16/32/64 ) Comment: Accera also supports the corresponding compound-assignment operators, such as a += b , a -= b , etc.","title":"Arithmetic operators"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#relational-operators","text":"Operation Types (Operands must be of same type) Description a == b acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if scalar a equals scalar b , else False a != b acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if scalar a is not equal to scalar b , else False a < b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if scalar a is strictly smaller than scalar b , else False a <= b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if scalar a is smaller than or equal to scalar b , else False a > b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if scalar a is strictly greater than scalar b , else False a >= b acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if scalar a is greater than or equal to scalar b , else False Not yet implemented: unsigned types ( acc.ScalarType.uint8/16/32/64 )","title":"Relational operators"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#logical-operators","text":"Operation Types (Operands must be of same type) Description acc.logical_and(a, b) acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if scalars a and b are non-zero, else False acc.logical_or(a, b) acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if either scalar a or scalar b are non-zero, else False acc.logical_not(a) acc.ScalarType.bool, acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns True if a is zero, else False Not yet implemented: unsigned types ( acc.ScalarType.uint8/16/32/64 )","title":"Logical operators"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#bitwise-operators","text":"Operation Types (Operands must be of same type) Description a & b acc.ScalarType.int8/16/32/64 Returns the bitwise AND of the bits in scalars a and b a \\| b acc.ScalarType.int8/16/32/64 Returns the bitwise OR of the bits in scalars a and b a ^ b acc.ScalarType.int8/16/32/64 Returns the bitwise XOR of the bits in scalars a and b ~a acc.ScalarType.int8/16/32/64 Returns the bitwise inverse of the bits in scalar a a << b acc.ScalarType.int8/16/32/64 Returns scalar a whose bitwise representation is shifted left by b bits a >> b acc.ScalarType.int8/16/32/64 Returns scalar a whose bitwise representation is shifted right by b bits Comment: Accera also supports the corresponding compound-assignment operators, such as a &= b , a |= b , etc. Not yet implemented: unsigned types ( acc.ScalarType.uint8/16/32/64 )","title":"Bitwise operators"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#intrinsics","text":"Operation Types (Operands must be of same type) Description acc.abs(a) acc.ScalarType.float32/64 Returns the absolute value of scalar a acc.max(a, b) acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the larger of the two scalars a and b acc.min(a, b) acc.ScalarType.int8/16/32/64, acc.ScalarType.float32/64 Returns the smaller of the two scalars a and b acc.ceil(a) acc.ScalarType.float32/64 Returns the value of scalar a rounded up to the nearest integer as an int64 type acc.floor(a) acc.ScalarType.float32/64 Returns the value of scalar a rounded down to the nearest integer as an int64 type acc.sqrt(a) acc.ScalarType.float32/64 Returns the square root of scalar a acc.exp(a) acc.ScalarType.float32/64 Returns the exponential e raised to the scalar a acc.log(a) acc.ScalarType.float32/64 Returns the natural logarithm (base e ) of scalar a acc.log10(a) acc.ScalarType.float32/64 Returns the common logarithm (base 10) of scalar a acc.log2(a) acc.ScalarType.float32/64 Returns the binary logarithm (base 2) of scalar a acc.sin(a) acc.ScalarType.float32/64 Returns the sine of scalar a , where a is in radians acc.cos(a) acc.ScalarType.float32/64 Returns the cosine of scalar a , where a is in radians acc.tan(a) acc.ScalarType.float32/64 Returns the tangent of scalar a , where a is in radians acc.sinh(a) acc.ScalarType.float32/64 Returns the hyperbolic sine of scalar a , where a is in radians acc.cosh(a) acc.ScalarType.float32/64 Returns the hyperbolic cosine of scalar a , where a is in radians acc.tanh(a) acc.ScalarType.float32/64 Returns the hyperbolic tangent of scalar a , where a is in radians Not yet implemented: unsigned types ( acc.ScalarType.uint8/16/32/64 )","title":"Intrinsics"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#accera-program-stages","text":"We take a step back to describe the stages of an Accera program: Nest : A nest captures the logic of a simple nest, without any optimizations or implementation details. Schedule : A Nest is used to create a schedule. The schedule controls the order in which the nest iterations are visited. Multiple schedules can be fused into a single schedule, which may no longer represent a simple nest. Plan : A Schedule is used to create a plan. A plan controls the implementation details that are specific to a specific target platform (e.g., data caching strategy, vectorization, assignment of arrays and caches to different types of memory). Package : A Plan is used to create a function in a function package. The package is then compiled and emitted. Once a package is emitted, the Accera functions contained in it can be called from external client code. This external code is typically not written using Accera. Accera currently supports the following package formats: HAT , which is a schematized version of a standard C library. The external client code can be written in C or C++ and linked with the HAT package. MLIR , which uses standard MLIR dialects. The external code must also be in MLIR. Overall, to build and emit nest (defined above), we would write: # create a default schedule from the nest schedule = nest . create_schedule () # create a default plan from the schedule plan = schedule . create_plan () # create a HAT package. Create a function in the package based on the plan package = acc . Package () package . add ( plan , args = ( A , B , C ), base_name = \"simple_matmul\" ) # build the HAT package package . build ( format = acc . Package . Format . HAT_DYNAMIC , name = \"linear_algebra\" ) It may not be immediately clear why so many stages are needed just to compile a simple nest. The importance of each step will hopefully become clear once we describe the stages in detail. In the example above, The call to package.add takes three arguments: the first is the plan that defines the function's implementation; the second is the order of the input and input/output arrays in the function signature; and the third is a base name for the function. The full name of the function is the base name followed by an automatically-generated unique identifier. For example, the function in the example could appear in the package as simple_matmul_8f24bef5 . The automatically-generated suffix ensures that each function in the package has a unique name. More details on function packages can be found in Section 10 .","title":"Accera program stages"},{"location":"Manual/02%20Simple%20Affine%20Loop%20Nests/#convenience-syntax","text":"For convenience, Accera also provides shortcuts to avoid unneeded verbosity. Specifically, we can create a function in a package directly from a nest, as follows: package . add ( nest , args = ( A , B , C ), base_name = \"simple_matmul\" ) The abbreviated syntax makes it seem like a callable function is generated directly from nest , but what actually happens behind the scenes is that nest creates a default schedule, which creates a default plan, which is added as a function in the package. Accera has a similar convenience syntax to create a function from a schedule: package . add ( schedule , args = ( A , B , C ), base_name = \"simple_matmul\" ) and to create a plan directly from a nest: plan = nest . create_plan ()","title":"Convenience syntax"},{"location":"Manual/03%20Schedules/","text":"Section 3: Schedules We begin this section with nest from Section 2 , which captures the logic of matrix-matrix multiplication. We use nest to create a Schedule , which controls the execution order of the nest iterations. Schedules are target-independent, in the sense that the same schedule can be used to emit code for multiple target platforms. We create a default schedule as follows: schedule = nest . create_schedule () The default schedule is equivalent to the following straightforward for-loop version of the loop nest: for i in range ( 16 ): for j in range ( 10 ): for k in range ( 11 ): C [ i , j ] += A [ i , k ] * B [ k , j ] In other words, each of the logical pseudo-code loops in nest becomes an actual for-loop in the default schedule. We can now transform this schedule in various ways. These transformations merely change the order of the loop iterations, and therefore preserve the logic defined in nest . Note that transforming schedule does not modify nest , and in fact, we could generate additional independent schedules by calling nest.create_schedule() multiple times. Iteration spaces: a geometric representation of schedules The Accera programming model embraces a geometric interpretation of schedules. Specifically, the schedule is imagined to be a multidimensional discrete hypercube called the iteration space of the nest. The elements of the iteration space represent the individual iterations of the loop nest. Initially, the dimensions of the iteration space corresponds to the logical loops defined in nest . For example, the default iteration space for the matrix-matrix multiplication nest forms a three dimensional discrete hypercube, whose shape is (16, 10, 11). How does an iteration space imply an order over the iterations? The dimensions of the iteration space are ordered. By default, their order corresponds to the original order of the logical loops in nest . The order over the dimensions induces a lexicographic order over the individual elements of the iteration space. Adopting this geometric interpretation helps us visualize how different transformations modify the schedule. Some transformations rearrange the elements of the iteration space, some increase its dimension, and some even pad the space with empty (no-op) elements. The transformed iteration space defines a new lexicographic order over the individual iterations. Comment: It is important not to confuse arrays, like A , B , C , with iteration spaces, like schedule . A possible source of confusion could be that both arrays and iteration spaces have a multidimensional rectilinear structure (i.e., they both look like hypercubes). However, arrays and iteration spaces are fundamentally different. Arrays are data structures whose elements are scalars. Iteration spaces are abstract geometric representations of schedules and their elements represent individual iterations of a loop nest. Transformations apply to iteration spaces, not to arrays. Comment: Accera's geometric interpretation of schedules resembles the iteration domain polyhedron , which is the cornerstone of the polyhedral model of compiler optimization. However, unlike polyhedrons, Accera iteration spaces are not embedded in a continuous space and cannot be manipulated by algebraic transformations. Accera iteration spaces always remain rectilinear and are inherently discrete objects. Iteration space slices Iteration space slices are an abstract concept that affects many different aspects of the Accera programming model. Since the iteration space dimensions are ordered, each element of the iteration space can be identified by a vector of coordinates. For example, the vector (5, 6, 7) identifies the iteration at position 5 along the first dimension, 6 along the second dimension, and 7 along the third dimension. If one or more of the coordinates is replaced with the wildcard symbol *, we get an iteration space slice , which is a set of iterations obtained by replacing the wildcard with all possible values. For example, (*, *, 5) represents a slice that contains all the elements whose last coordinate is 5. The dimension of a slice equals the number of wildcards in its definition. Loops, indices, and dimensions When we defined nest , we used variables such as i , j , k to name the loops in the loop-nest. When we described the default schedule using equivalent for-loops, i , j , and k became the index variables of those loops. When we represent a schedule as an iteration space, these variables are used as the names of the corresponding iteration space dimensions. From here on, we move seamlessly between these different representations and use the terms loop , index , and dimension interchangeably. Schedule transformations Iteration space transformations change the shape of the iteration space, possibly adding dimensions and padding the space with empty elements. The iteration space always retains its rectilinear shape (i.e., the shape of a hypercube). In some cases, Accera transformations must pad the iteration space with empty elements to avoid reaching a jagged iteration space structure. reorder # Reorder the indices. schedule . reorder ( k , i , j ) The reorder transformation sets the order of the indices in the schedule. From the iteration space point-of-view, reorder performs a pivot rotation of the iteration space, which orients its dimensions in the specified order. Since the iteration space elements are executed in lexicographic order, pivoting the iteration space is equivalent to reordering the loops. For example, we can write: schedule . reorder ( k , i , j ) After this transformation, schedule becomes equivalent to the Python code: for k in range ( 11 ): for i in range ( 16 ): for j in range ( 10 ): C [ i , j ] += A [ i , k ] * B [ k , j ] Some orders are not allowed. Describing the restrictions in full requires concepts that have not yet been introduced, so we merely mention the restrictions here and explain them in detail later on. The restrictions are: 1. The inner dimension created by a split transformation (see below) always comes after its corresponding outer dimension . 2. The fusing dimension created by a fuse operation (see Section 4 ) must always precede any unfused dimensions . Also note that reorder can also have the following overloaded form: schedule . reorder ( order = ( k , i , j )) This form is better suited for use with parameters (see Section 9 ). split # Splits dimension i into equally-sized parts, orients those parts along a new dimension ii, and stacks those parts along dimension i ii = schedule . split ( i , size ) From the iteration space point-of-view, the split transformation takes a dimension i and a size , modifies i , and creates a new dimension ii . Assume that the original size of dimension i was n : The split transformation splits dimension i into ceil(n/size) parts of size size , orients each of those parts along dimension ii , and stacks the ceil(n/size) parts along dimension i . If the split size does not divide the dimension size, empty elements are added such that the split size does divide the dimension size. As a result of the split, the size of i becomes ceil(n/size) , the size of the new dimension ii equals size , and the iteration space remains rectilinear. In loop terms, ii = split(i, size) splits loop i into two loops: an inner loop ii and an outer loop, which inherits the original name i . Note that the outer loop always precedes the corresponding inner loop in the loop ordering. For example, starting from nest defined in Section 2 , we could write: schedule = nest . create_schedule () jj = schedule . split ( j , 5 ) The resulting iteration space has a shape of (16,2,5,11) and corresponds to the following python code: for i in range ( 16 ): for j in range ( 0 , 10 , 5 ): for jj in range ( 5 ): for k in range ( 11 ): C [ i , j + jj ] += A [ i , k ] * B [ k , j + jj ] Note that loop j is no longer normalized (it has a stride of 5 rather than 1), which means that the nest is no longer a simple nest. As mentioned in the previous section, Nest objects always represent simple nests, but Schedule objects can represent more complex affine loop nests. After performing a split, both the outer index and the inner index can be split again. For example, schedule = nest . create_schedule () ii = schedule . split ( i , 4 ) iii = schedule . split ( i , 2 ) iiii = schedule . split ( ii , 2 ) After the first split, the iteration space has a shape of (4, 4, 10, 11). After the second split, the shape becomes (2, 2, 4, 10, 11). Finally, the shape becomes (2, 2, 2, 2, 10, 11). The transformed schedule corresponds to the following python code: for i in range ( 0 , 16 , 8 ): for iii in range ( 0 , 8 , 4 ): for ii in range ( 0 , 4 , 2 ): for iiii in range ( 2 ): for j in range ( 10 ): for k in range ( 11 ): C [ i + ii + iii + iiii , j ] += A [ i + ii + iii + iiii , k ] * B [ k , j ] The split does not necessarily need to divide the dimension size. For example, consider the following code: schedule = nest . create_schedule () kk = schedule . split ( k , 4 ) # original size of dimension k was 11 From the iteration space point-of-view, this code splits dimension k into three parts of size 4, where the last part is padded with empty (no-op) elements. Before the transformation, the iteration space shape is (16, 10, 11), and after the transformation, the shape is (16, 10, 3, 4) (so, 160 empty elements were added). In loop form, the transformed iteration space corresponds to the following Python code: for i in range ( 16 ): for j in range ( 10 ): for k in range ( 0 , 11 , 4 ): for kk in range ( 4 ): if k + kk < 11 : C [ i , j ] += A [ i , k + kk ] * B [ k + kk , j ] Note that Accera optimizes away costly if statements by unswitching the loops, which results in code that looks more like this: for i in range ( 16 ): for j in range ( 10 ): for k in range ( 0 , 8 , 4 ): for kk in range ( 4 ): C [ i , j ] += A [ i , k + kk ] * B [ k + kk , j ] # loop unswitching: handle the last iteration of the k loop separately for kk in range ( 3 ): C [ i , j ] += A [ i , 8 + kk ] * B [ 8 + kk , j ] Meaningless splits We describe Accera's behavior in a few degenerate cases. If the split size equals the dimension size, the transformation simply renames the split dimension. For example, schedule = nest . create_schedule () kk = schedule . split ( k , 11 ) # original size of dimension k was 11 After the split, the size of k becomes 1 and the size of kk is 11 . The new shape of the iteration space is (16, 10, 1, 11). The dimension k becomes meaningless and therefore the schedule is basically unchanged. If the split size is greater than the dimension size, this is just a special case of the situation where the split size doesn't divide the dimension size. As mentioned above, Accera solves this by adding empty elements. For example, schedule = nest . create_schedule () kk = schedule . split ( k , 13 ) # original size of dimension k was 11 After the split, the size of k becomes 1 and the size of kk is 13 . The new shape of the iteration space is (16, 10, 1, 13), which means that 320 empty elements were added. These empty elements are removed during code generation, which means that the schedule is basically unchanged. Finally, note that kk = schedule.split(k, 1) simply adds a meaningless new dimension kk of size 1, and again, the schedule is unchanged. Convenience syntax: tile The tile transformation is a convenience syntax and does not provide any unique functionality. Consider the following code schedule = nest . create_schedule () ii , jj , kk = schedule . tile (( i , j , k ), ( 8 , 2 , 3 )) The tile transformation above is shorthand for the following sequence of transformations: ii = schedule . split ( i , 8 ) jj = schedule . split ( j , 2 ) kk = schedule . split ( k , 3 ) It will result in a sequence of indices that are ordered as: (i, ii, j, jj, k, kk) In words, the tile transformation takes a tuple of indices and a tuple of sizes, and splits each index by the corresponding size. Then, the indices involved in the split are reordered such that each of the outer indices (parent index) precede its inner indices (child index). Indices that did not participate in the transformation remain in their relative positions. skew # Skew dimension i with respect to dimension j. schedule . skew ( i , j ) The skew transformation is easiest to explain for a two-dimensional iteration space of shape (N, M) . Skewing dimension i (the row dimension) with respect to j (the column dimension) modifies the iteration space column-by-column: column j gets j empty elements added to its beginning and M-j-1 empty elements to its end. As a result, each column grows from size N to size N+M-1 . Geometrically, the original iteration space elements take the form of a 45-degree parallelogram, embedded within a bounding rectangle of shape (N+M-1, M) . The element that used to be at coordinate (i, j) moves to coordinate (i+j, j) . Similarly, skewing j with respect to i adds empty elements at the beginning and end of each row, and results in a iteration space of shape (N, N+M-1) . In higher dimensions, we simply apply the two-dimensional skew transformation independently to each two-dimensional slice along the two specified dimensions. To demonstrate the importance of this transformation, consider convolving a 10-element vector with a 3-element filter. The loop logic for this operation is defined as follows: import accera as acc N = 10 # input size K = 3 # filter size M = N - K + 1 # output size = 8 A = acc . Array ( role = acc . Array . Role . INPUT , shape = ( N ,)) B = acc . Array ( role = acc . Array . Role . INPUT , shape = ( K ,)) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , shape = ( M ,)) nest = acc . Nest ( shape = ( M , K )) i , j = nest . get_indices () @nest . iteration_logic def _ (): C [ i ] += A [ i + j ] * B [ j ] schedule = nest . create_schedule () schedule corresponds to an iteration space of shape (8,3), where the first dimension corresponds to the 8 elements of the output vector. This schedule calculates the outputs one by one: first C[0] , then C[1] , etc. Here is the equivalent Python code: for i in range ( 8 ): for j in range ( 3 ): C [ i ] += A [ i + j ] * B [ j ] Now, say that we apply the skew transformation as follows: schedule . skew ( i , j ) This transformation results in an iteration shape of shape (10, 3), where the first dimension now corresponds to the 10 elements of the input. This transformed schedule processes the input elements one-by-one: it extracts all the information from A[0] ( A[0] is only used in the calculation of C[0] ), then it moves on to A[1] (which contributes to both C[0] and C[1] ), and so on. In this example, the default schedule achieves memory locality with respect to array C whereas the skewed schedule achieves memory locality with respect to array A . In loop form, the transformed iteration space corresponds to the following Python code: for i in range ( 10 ): for j in range ( 3 ): if ( i - j ) >= 0 and ( i - j ) < 8 : C [ i - j ] += A [ i ] * B [ j ] Behind the scenes, unswitching the loops results in code that looks more like this: # triangle of height 2, width 3 for j in range ( 1 ): C [ 0 - j ] += A [ 0 ] * B [ j ] for j in range ( 2 ): C [ 1 - j ] += A [ 1 ] * B [ j ] # rectangle of shape (6, 3) for i in range ( 2 , 8 ): for j in range ( 3 ): C [ i - j ] += A [ i ] * B [ j ] # upside-down triangle of height 2, width 3 for j in range ( 2 ): C [ 6 + j ] += A [ 8 ] * B [ 2 - j ] for j in range ( 1 ): C [ 7 + j ] += A [ 9 ] * B [ 2 - j ] Finally, note that some loops have small sizes that can be replaced by unrolls. To enable the unrolling of these small loops, we can use this optional parameter: schedule . skew ( i , j , unroll_loops_smaller_than = 3 ) This will unroll all loops that are smaller than 3, which include the range(2) and range(1) loops in the example above. pad # Adds empty elements to the beginning of dimension i. schedule . pad ( i , size ) The pad transformation pads the beginning of dimension i with empty elements. This operation is meaningless by itself, but can be useful when used with splitting or fusing. Order-invariant schedules and safety We say that a schedule is order-invariant if its underlying logic doesn't depend on the execution order of its iterations. For example, schedules created from a single Nest (via a call to create_schedule() ) are order-invariant. All of the schedules discussed so far have been order-invariant. We say that a schedule is safe if its underlying logic is guaranteed not to change, regardless of how we transform it. Not all schedules are safe, but order-invariant schedules are. This is because all of the transformations introduced in this section merely change the order in which iterations are executed, without adding or removing any work. In Section 4 , we introduce fused schedules, which are not order-invariant, but may still be safe.","title":"Schedules"},{"location":"Manual/03%20Schedules/#section-3-schedules","text":"We begin this section with nest from Section 2 , which captures the logic of matrix-matrix multiplication. We use nest to create a Schedule , which controls the execution order of the nest iterations. Schedules are target-independent, in the sense that the same schedule can be used to emit code for multiple target platforms. We create a default schedule as follows: schedule = nest . create_schedule () The default schedule is equivalent to the following straightforward for-loop version of the loop nest: for i in range ( 16 ): for j in range ( 10 ): for k in range ( 11 ): C [ i , j ] += A [ i , k ] * B [ k , j ] In other words, each of the logical pseudo-code loops in nest becomes an actual for-loop in the default schedule. We can now transform this schedule in various ways. These transformations merely change the order of the loop iterations, and therefore preserve the logic defined in nest . Note that transforming schedule does not modify nest , and in fact, we could generate additional independent schedules by calling nest.create_schedule() multiple times.","title":"Section 3: Schedules"},{"location":"Manual/03%20Schedules/#iteration-spaces-a-geometric-representation-of-schedules","text":"The Accera programming model embraces a geometric interpretation of schedules. Specifically, the schedule is imagined to be a multidimensional discrete hypercube called the iteration space of the nest. The elements of the iteration space represent the individual iterations of the loop nest. Initially, the dimensions of the iteration space corresponds to the logical loops defined in nest . For example, the default iteration space for the matrix-matrix multiplication nest forms a three dimensional discrete hypercube, whose shape is (16, 10, 11). How does an iteration space imply an order over the iterations? The dimensions of the iteration space are ordered. By default, their order corresponds to the original order of the logical loops in nest . The order over the dimensions induces a lexicographic order over the individual elements of the iteration space. Adopting this geometric interpretation helps us visualize how different transformations modify the schedule. Some transformations rearrange the elements of the iteration space, some increase its dimension, and some even pad the space with empty (no-op) elements. The transformed iteration space defines a new lexicographic order over the individual iterations. Comment: It is important not to confuse arrays, like A , B , C , with iteration spaces, like schedule . A possible source of confusion could be that both arrays and iteration spaces have a multidimensional rectilinear structure (i.e., they both look like hypercubes). However, arrays and iteration spaces are fundamentally different. Arrays are data structures whose elements are scalars. Iteration spaces are abstract geometric representations of schedules and their elements represent individual iterations of a loop nest. Transformations apply to iteration spaces, not to arrays. Comment: Accera's geometric interpretation of schedules resembles the iteration domain polyhedron , which is the cornerstone of the polyhedral model of compiler optimization. However, unlike polyhedrons, Accera iteration spaces are not embedded in a continuous space and cannot be manipulated by algebraic transformations. Accera iteration spaces always remain rectilinear and are inherently discrete objects.","title":"Iteration spaces: a geometric representation of schedules"},{"location":"Manual/03%20Schedules/#iteration-space-slices","text":"Iteration space slices are an abstract concept that affects many different aspects of the Accera programming model. Since the iteration space dimensions are ordered, each element of the iteration space can be identified by a vector of coordinates. For example, the vector (5, 6, 7) identifies the iteration at position 5 along the first dimension, 6 along the second dimension, and 7 along the third dimension. If one or more of the coordinates is replaced with the wildcard symbol *, we get an iteration space slice , which is a set of iterations obtained by replacing the wildcard with all possible values. For example, (*, *, 5) represents a slice that contains all the elements whose last coordinate is 5. The dimension of a slice equals the number of wildcards in its definition.","title":"Iteration space slices"},{"location":"Manual/03%20Schedules/#loops-indices-and-dimensions","text":"When we defined nest , we used variables such as i , j , k to name the loops in the loop-nest. When we described the default schedule using equivalent for-loops, i , j , and k became the index variables of those loops. When we represent a schedule as an iteration space, these variables are used as the names of the corresponding iteration space dimensions. From here on, we move seamlessly between these different representations and use the terms loop , index , and dimension interchangeably.","title":"Loops, indices, and dimensions"},{"location":"Manual/03%20Schedules/#schedule-transformations","text":"Iteration space transformations change the shape of the iteration space, possibly adding dimensions and padding the space with empty elements. The iteration space always retains its rectilinear shape (i.e., the shape of a hypercube). In some cases, Accera transformations must pad the iteration space with empty elements to avoid reaching a jagged iteration space structure.","title":"Schedule transformations"},{"location":"Manual/03%20Schedules/#reorder","text":"# Reorder the indices. schedule . reorder ( k , i , j ) The reorder transformation sets the order of the indices in the schedule. From the iteration space point-of-view, reorder performs a pivot rotation of the iteration space, which orients its dimensions in the specified order. Since the iteration space elements are executed in lexicographic order, pivoting the iteration space is equivalent to reordering the loops. For example, we can write: schedule . reorder ( k , i , j ) After this transformation, schedule becomes equivalent to the Python code: for k in range ( 11 ): for i in range ( 16 ): for j in range ( 10 ): C [ i , j ] += A [ i , k ] * B [ k , j ] Some orders are not allowed. Describing the restrictions in full requires concepts that have not yet been introduced, so we merely mention the restrictions here and explain them in detail later on. The restrictions are: 1. The inner dimension created by a split transformation (see below) always comes after its corresponding outer dimension . 2. The fusing dimension created by a fuse operation (see Section 4 ) must always precede any unfused dimensions . Also note that reorder can also have the following overloaded form: schedule . reorder ( order = ( k , i , j )) This form is better suited for use with parameters (see Section 9 ).","title":"reorder"},{"location":"Manual/03%20Schedules/#split","text":"# Splits dimension i into equally-sized parts, orients those parts along a new dimension ii, and stacks those parts along dimension i ii = schedule . split ( i , size ) From the iteration space point-of-view, the split transformation takes a dimension i and a size , modifies i , and creates a new dimension ii . Assume that the original size of dimension i was n : The split transformation splits dimension i into ceil(n/size) parts of size size , orients each of those parts along dimension ii , and stacks the ceil(n/size) parts along dimension i . If the split size does not divide the dimension size, empty elements are added such that the split size does divide the dimension size. As a result of the split, the size of i becomes ceil(n/size) , the size of the new dimension ii equals size , and the iteration space remains rectilinear. In loop terms, ii = split(i, size) splits loop i into two loops: an inner loop ii and an outer loop, which inherits the original name i . Note that the outer loop always precedes the corresponding inner loop in the loop ordering. For example, starting from nest defined in Section 2 , we could write: schedule = nest . create_schedule () jj = schedule . split ( j , 5 ) The resulting iteration space has a shape of (16,2,5,11) and corresponds to the following python code: for i in range ( 16 ): for j in range ( 0 , 10 , 5 ): for jj in range ( 5 ): for k in range ( 11 ): C [ i , j + jj ] += A [ i , k ] * B [ k , j + jj ] Note that loop j is no longer normalized (it has a stride of 5 rather than 1), which means that the nest is no longer a simple nest. As mentioned in the previous section, Nest objects always represent simple nests, but Schedule objects can represent more complex affine loop nests. After performing a split, both the outer index and the inner index can be split again. For example, schedule = nest . create_schedule () ii = schedule . split ( i , 4 ) iii = schedule . split ( i , 2 ) iiii = schedule . split ( ii , 2 ) After the first split, the iteration space has a shape of (4, 4, 10, 11). After the second split, the shape becomes (2, 2, 4, 10, 11). Finally, the shape becomes (2, 2, 2, 2, 10, 11). The transformed schedule corresponds to the following python code: for i in range ( 0 , 16 , 8 ): for iii in range ( 0 , 8 , 4 ): for ii in range ( 0 , 4 , 2 ): for iiii in range ( 2 ): for j in range ( 10 ): for k in range ( 11 ): C [ i + ii + iii + iiii , j ] += A [ i + ii + iii + iiii , k ] * B [ k , j ] The split does not necessarily need to divide the dimension size. For example, consider the following code: schedule = nest . create_schedule () kk = schedule . split ( k , 4 ) # original size of dimension k was 11 From the iteration space point-of-view, this code splits dimension k into three parts of size 4, where the last part is padded with empty (no-op) elements. Before the transformation, the iteration space shape is (16, 10, 11), and after the transformation, the shape is (16, 10, 3, 4) (so, 160 empty elements were added). In loop form, the transformed iteration space corresponds to the following Python code: for i in range ( 16 ): for j in range ( 10 ): for k in range ( 0 , 11 , 4 ): for kk in range ( 4 ): if k + kk < 11 : C [ i , j ] += A [ i , k + kk ] * B [ k + kk , j ] Note that Accera optimizes away costly if statements by unswitching the loops, which results in code that looks more like this: for i in range ( 16 ): for j in range ( 10 ): for k in range ( 0 , 8 , 4 ): for kk in range ( 4 ): C [ i , j ] += A [ i , k + kk ] * B [ k + kk , j ] # loop unswitching: handle the last iteration of the k loop separately for kk in range ( 3 ): C [ i , j ] += A [ i , 8 + kk ] * B [ 8 + kk , j ]","title":"split"},{"location":"Manual/03%20Schedules/#meaningless-splits","text":"We describe Accera's behavior in a few degenerate cases. If the split size equals the dimension size, the transformation simply renames the split dimension. For example, schedule = nest . create_schedule () kk = schedule . split ( k , 11 ) # original size of dimension k was 11 After the split, the size of k becomes 1 and the size of kk is 11 . The new shape of the iteration space is (16, 10, 1, 11). The dimension k becomes meaningless and therefore the schedule is basically unchanged. If the split size is greater than the dimension size, this is just a special case of the situation where the split size doesn't divide the dimension size. As mentioned above, Accera solves this by adding empty elements. For example, schedule = nest . create_schedule () kk = schedule . split ( k , 13 ) # original size of dimension k was 11 After the split, the size of k becomes 1 and the size of kk is 13 . The new shape of the iteration space is (16, 10, 1, 13), which means that 320 empty elements were added. These empty elements are removed during code generation, which means that the schedule is basically unchanged. Finally, note that kk = schedule.split(k, 1) simply adds a meaningless new dimension kk of size 1, and again, the schedule is unchanged.","title":"Meaningless splits"},{"location":"Manual/03%20Schedules/#convenience-syntax-tile","text":"The tile transformation is a convenience syntax and does not provide any unique functionality. Consider the following code schedule = nest . create_schedule () ii , jj , kk = schedule . tile (( i , j , k ), ( 8 , 2 , 3 )) The tile transformation above is shorthand for the following sequence of transformations: ii = schedule . split ( i , 8 ) jj = schedule . split ( j , 2 ) kk = schedule . split ( k , 3 ) It will result in a sequence of indices that are ordered as: (i, ii, j, jj, k, kk) In words, the tile transformation takes a tuple of indices and a tuple of sizes, and splits each index by the corresponding size. Then, the indices involved in the split are reordered such that each of the outer indices (parent index) precede its inner indices (child index). Indices that did not participate in the transformation remain in their relative positions.","title":"Convenience syntax: tile"},{"location":"Manual/03%20Schedules/#skew","text":"# Skew dimension i with respect to dimension j. schedule . skew ( i , j ) The skew transformation is easiest to explain for a two-dimensional iteration space of shape (N, M) . Skewing dimension i (the row dimension) with respect to j (the column dimension) modifies the iteration space column-by-column: column j gets j empty elements added to its beginning and M-j-1 empty elements to its end. As a result, each column grows from size N to size N+M-1 . Geometrically, the original iteration space elements take the form of a 45-degree parallelogram, embedded within a bounding rectangle of shape (N+M-1, M) . The element that used to be at coordinate (i, j) moves to coordinate (i+j, j) . Similarly, skewing j with respect to i adds empty elements at the beginning and end of each row, and results in a iteration space of shape (N, N+M-1) . In higher dimensions, we simply apply the two-dimensional skew transformation independently to each two-dimensional slice along the two specified dimensions. To demonstrate the importance of this transformation, consider convolving a 10-element vector with a 3-element filter. The loop logic for this operation is defined as follows: import accera as acc N = 10 # input size K = 3 # filter size M = N - K + 1 # output size = 8 A = acc . Array ( role = acc . Array . Role . INPUT , shape = ( N ,)) B = acc . Array ( role = acc . Array . Role . INPUT , shape = ( K ,)) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , shape = ( M ,)) nest = acc . Nest ( shape = ( M , K )) i , j = nest . get_indices () @nest . iteration_logic def _ (): C [ i ] += A [ i + j ] * B [ j ] schedule = nest . create_schedule () schedule corresponds to an iteration space of shape (8,3), where the first dimension corresponds to the 8 elements of the output vector. This schedule calculates the outputs one by one: first C[0] , then C[1] , etc. Here is the equivalent Python code: for i in range ( 8 ): for j in range ( 3 ): C [ i ] += A [ i + j ] * B [ j ] Now, say that we apply the skew transformation as follows: schedule . skew ( i , j ) This transformation results in an iteration shape of shape (10, 3), where the first dimension now corresponds to the 10 elements of the input. This transformed schedule processes the input elements one-by-one: it extracts all the information from A[0] ( A[0] is only used in the calculation of C[0] ), then it moves on to A[1] (which contributes to both C[0] and C[1] ), and so on. In this example, the default schedule achieves memory locality with respect to array C whereas the skewed schedule achieves memory locality with respect to array A . In loop form, the transformed iteration space corresponds to the following Python code: for i in range ( 10 ): for j in range ( 3 ): if ( i - j ) >= 0 and ( i - j ) < 8 : C [ i - j ] += A [ i ] * B [ j ] Behind the scenes, unswitching the loops results in code that looks more like this: # triangle of height 2, width 3 for j in range ( 1 ): C [ 0 - j ] += A [ 0 ] * B [ j ] for j in range ( 2 ): C [ 1 - j ] += A [ 1 ] * B [ j ] # rectangle of shape (6, 3) for i in range ( 2 , 8 ): for j in range ( 3 ): C [ i - j ] += A [ i ] * B [ j ] # upside-down triangle of height 2, width 3 for j in range ( 2 ): C [ 6 + j ] += A [ 8 ] * B [ 2 - j ] for j in range ( 1 ): C [ 7 + j ] += A [ 9 ] * B [ 2 - j ] Finally, note that some loops have small sizes that can be replaced by unrolls. To enable the unrolling of these small loops, we can use this optional parameter: schedule . skew ( i , j , unroll_loops_smaller_than = 3 ) This will unroll all loops that are smaller than 3, which include the range(2) and range(1) loops in the example above.","title":"skew"},{"location":"Manual/03%20Schedules/#pad","text":"# Adds empty elements to the beginning of dimension i. schedule . pad ( i , size ) The pad transformation pads the beginning of dimension i with empty elements. This operation is meaningless by itself, but can be useful when used with splitting or fusing.","title":"pad"},{"location":"Manual/03%20Schedules/#order-invariant-schedules-and-safety","text":"We say that a schedule is order-invariant if its underlying logic doesn't depend on the execution order of its iterations. For example, schedules created from a single Nest (via a call to create_schedule() ) are order-invariant. All of the schedules discussed so far have been order-invariant. We say that a schedule is safe if its underlying logic is guaranteed not to change, regardless of how we transform it. Not all schedules are safe, but order-invariant schedules are. This is because all of the transformations introduced in this section merely change the order in which iterations are executed, without adding or removing any work. In Section 4 , we introduce fused schedules, which are not order-invariant, but may still be safe.","title":"Order-invariant schedules and safety"},{"location":"Manual/04%20Fusing/","text":"Section 4: Fusing Multiple schedules can be combined into a single schedule using the fuse operation. The fused schedule represents the union of the work in the original schedules. The fused schedule can be transformed using any of the transformations presented in Section 3 . Full fusing import accera as acc # Fuse three schedules to create a fused schedule schedule = acc . fuse ( schedule0 , schedule1 , ... ) Full fusing is the most straightforward form of fusing, where each dimension is fused with the corresponding dimension from the other schedules. Full fusing of same-shaped iteration spaces First, consider the simplest case, where we fuse schedules whose iteration spaces have identical shapes. The fused schedule schedule gets a new dimension, called the fusing dimension , which did not exist in the original schedules. By default, the fusing dimension is the first dimension in the fused schedule and its size equals the number of schedules that were fused. The first slice along the fusing dimension contains a copy of schedule0 , the second slice contains a copy schedule1 , and so on. Since the fusing dimension is the first dimension, the fused schedule is logically equivalent to fully executing schedule0 , followed by schedule1 , and so on. To interleave the original schedules, we apply additional transformations to the fused schedule. As a concrete example, imagine that we want to shift and then scale each of the elements of a matrix, or in other words, perform the equivalent of the Python code: C = ( C + A ) * B where all three matrices are 16 by 16. One way to do this without fusing is to simply write: A = acc . Array ( role = acc . Array . Role . INPUT , shape = ( 16 , 16 )) B = acc . Array ( role = acc . Array . Role . INPUT , shape = ( 16 , 16 )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , shape = ( 16 , 16 )) # Create nest_simple and schedule_simple nest_simple = acc . Nest ( shape = ( 16 , 16 )) i , j = nest_simple . get_indices () @nest_simple . iteration_logic def _ (): C [ i , j ] = ( C [ i , j ] + A [ i , j ]) * B [ i , j ] schedule_simple = nest_simple . create_schedule () Note that each iteration in schedule_simple operates on all three arrays at once. Imagine that there is some computational advantage to operating on only two arrays at a time. For example, say that operating on all three arrays simultaneously creates excessive pressure on the computer's memory caches, which could hurt performance. Therefore, we may want to first compute C += A and only then compute C *= B . Better yet, we may want to compute C in 4\u00d74 blocks: first computing C[0:4, 0:4] += A[0:4, 0:4] ; next computing C[0:4, 0:4] *= B[0:4, 0:4] ; then moving on to the next block and computing C[4:8, 0:4] += A[4:8, 0:4] , and so on. Fusing gives us the flexibility to explore all of these possibilities, and more. First, we define two separate nests, one for the logic C += A and one for the logic C *= B , and obtain their corresponding default schedules: # Create nest0 and schedule0 nest0 = acc . Nest ( shape = ( 16 , 16 )) i0 , j0 = nest0 . get_indices () @nest0 . iteration_logic def _ (): C [ i0 , j0 ] += A [ i0 , j0 ] schedule0 = nest0 . create_schedule () # Create nest1 and schedule1 nest1 = acc . Nest ( shape = ( 16 , 16 )) i1 , j1 = nest1 . get_indices () @nest1 . iteration_logic def _ (): C [ i1 , j1 ] *= B [ i1 , j1 ] schedule1 = nest1 . create_schedule () Before fusing, both schedule0 and schedule1 have a shape of (16, 16). Next we fuse them: # Create a fused schedule schedule = acc . fuse ( schedule0 , schedule1 ) f , i , j = schedule . get_indices () Fusing does not change schedule0 or schedule1 but rather creates a new fused schedule named schedule , whose shape is (2, 16, 16). The first dimension in schedule is the so-called fusing dimension f , its slice (0, *, *) contains a copy of schedule0 , and its slice (1, *, *) contains a copy of schedule1 . In loop form, schedule is now equivalent to the following Python code: # f = 0 for i in range ( 16 ): for j in range ( 16 ): C [ i , j ] += A [ i , j ] # f = 1 for i in range ( 16 ): for j in range ( 16 ): C [ i , j ] *= B [ i , j ] Not much has happened yet: executing schedule as-is is equivalent to executing schedule0 and then executing schedule1 . However, this can be changed by transforming the fused schedule. For example, we can recover schedule_simple by reordering the indices as follows: schedule . reorder ( i , j , f ) The fusing dimension moves from the first position to the last position. Now, schedule is equivalent to the following Python code: for i in range ( 16 ): for j in range ( 16 ): # f = 0 C [ i , j ] += A [ i , j ] # f = 1 C [ i , j ] *= B [ i , j ] We also discussed computing the output block-by-block: first computing C[0:4, 0:4] += A[0:4, 0:4] , then computing C[0:4, 0:4] *= B[0:4, 0:4] , and so on. This can be accomplished with the following sequence of transformations ii , jj = schedule . tile (( i , j ), ( 4 , 4 )) schedule . reorder ( i , j , f , ii , jj ) The resulting schedule is equivalent to the Python code: for i in range ( 0 , 16 , 4 ): for j in range ( 0 , 16 , 4 ): # f = 0 for ii in range ( 4 ): for jj in range ( 4 ): C [ i + ii , j + jj ] += A [ i + ii , j + jj ] # f = 1 for ii in range ( 4 ): for jj in range ( 4 ): C [ i + ii , j + jj ] *= B [ i + ii , j + jj ] Constraint 1: the fusing dimension is executed sequentially The fusing dimension has a special constraint, which does not apply to other dimensions. Specifically, the fusing dimension cannot be parallelized or vectorized (parallelization and vectorization are presented in Section 7 ) and it must be executed sequentially. This constraint enables the safety guarantee discussed below. Safety The fused schedule (before applying any subsequent transformations) is always logically equivalent to executing the original schedules one-by-one. However, is it safe? Recall that a schedule is considered safe if its underlying logic is guaranteed not to change, regardless of how we transform it. The safety of a fully fused schedule depends on the circumstances: Accera guarantees that the order of the fused schedules is preserved for each value of the fused dimensions , regardless of how the fused schedule is transformed. For example, in the example above, the fused dimensions are i and j . Therefore, for any concrete value of i and j , the corresponding operation from schedule0 is guaranteed to execute before the corresponding operation from schedule1 , regardless of how the fused schedule is transformed. More specifically, for each i and j , the operation C[i, j] += A[i, j] is guaranteed to execute before the operation C[i, j] *= B[i, j] , no matter how we transform the fused schedule. Since those are the only operations that touch C[i,j] , the Accera guarantee is sufficient and we conclude that fused schedule is safe. With this guarantee, the programmer can apply any sequence of transformations without worrying about the resulting implementation's correctness. However, note that not every fusing operation creates a safe schedule. For example, imagine that we had fused schedule0 and schedule1 differently: # Reorder schedule1 before fusing schedule1 . reorder ( j1 , i1 ) # Fuse schedule0 with the reordered schedule1 schedule_t = acc . fuse ( schedule0 , schedule1 ) f , a , b = schedule_t . get_indices () In this unnatural example, i0 and j1 are fused and named a , and i1 and j0 are fused and named b . As before, Accera guarantees that for each value of a and b the operation C[a, b] += A[a, b] is executed before C[b, a] *= B[b, a] . As noted above, the fusing operation itself preserves logical equivalence, but if we proceed to transform the fused schedule as follows, schedule_t . reorder ( a , b , f ) the logic actually changes. To see this, note that the resulting schedule is equivalent to the following Python code: for a in range ( 16 ): for b in range ( 16 ): C [ a , b ] += A [ a , b ] C [ b , a ] *= B [ b , a ] In particular, this code sets C[1,0] to C[1,0] * B[1,0] + A[1,0] , whereas the original fused logic set C[1,0] to (C[1,0] + A[1,0]) * B[1,0] . We conclude that schedule_t is certainly not safe. If the programmer creates an unsafe schedule, they take upon themselves the responsibility of maintaining logical equivalence. Fusing iteration spaces with different shapes If the iterations spaces have different shapes, Accera matches their shapes by padding them appropriately with empty cells. Partial fusing In many cases, instead of fusing all of the dimensions, we only need to fuse some of the dimensions, leaving the rest unfused. To fuse the first s dimensions, we use the syntax # Fuse the first s dimensions of three schedules schedule = acc . fuse (( schedule0 , schedule1 , ... ), partial = s ) The order of the dimensions in the fused schedule is as follows: first the fusing dimension f , then the s fused dimensions, then the unfused dimensions of schedule0 , schedule1 , etc. We can easily calculate the number of dimensions in the fused schedule. For example, if we fuse the first s dimensions of a d0 -dimensional space schedule0 and a d1 -dimensional space schedule1 , the fused iteration space will have s fused dimensions, d0 + d1 - 2s unfused dimensions, and the special fusing dimension f , for a total of d0 + d1 - s + 1 dimensions. As before, the fuse operation uses padding to ensure that the fused iteration space is not jagged in any direction. For example, say that schedule0 is 4-dimensional, schedule1 is 3-dimensional, and we partially fuse their first 2 dimensions: schedule = acc . fuse (( schedule0 , schedule1 ), partial = 2 ) f , i , j , k , l , m = schedule . get_indices () The first dimension is the fusing dimensions f , whose size is 2. Next come the fused dimensions i and j . Then, k and l are the two unfused dimensions from schedule0 and m is the unfused dimension from schedule1 . The slice (0, *, *, *, *, 0) contains a copy of schedule0 , the slice (1, *, *, 0, 0, *) contains a copy of schedule1 , and the rest of schedule is padded with empty elements. Note that full fusing is a special case of partial fusing, where s is the larger of the dimensions of schedule0 and schedule1 . Constraint 2: the fusing dimension always precedes unfused dimensions Partial fusing introduces a second constraint on the fusing dimension. Namely, the fusing dimension must precede all of the unfused dimensions in the dimension order. This constraint also applies to dimensions that are derived from the fusing dimension and from the unfused dimensions via splitting. Safety The safety guarantees for partial-fusing are a natural extension of the guarantees for full fusing. Accera guarantees that the order of the fused schedules is preserved for each value of the fused dimensions , regardless of how the fused schedule is transformed. In other words, for each concrete value of the fused dimensions, all the corresponding work in schedule0 (across all of its unfused dimensions) is performed before any of the corresponding work in schedule1 (across all of its unfused dimensions), and this holds no matter how we transform the fused schedule. When fusing, the programmer needs to consider whether this property implies safety - we will show how this can be done in each of the examples below. Partial fusing example: fully-connected neural layer with activation Consider applying an element-wise operation, such as the ReLU function from the field of AI, to the result of a matrix-matrix multiplication. In the language of neural networks, this is called a fully connected layer with a ReLU activation. The function relu(x) is simply max(x,0) . Specifically, imagine that we have an element-wise operator relu and we want to implement the equivalent of the Python code: C = relu ( C + A @ B ) where A has a shape of (16, 11), B has a shape of (11, 10), and C has a shape of (16, 10). We define two nests, one for C += A @ B and the other for C = relu(C) , and obtain their corresponding default schedules: # Create nest0 and schedule0 nest0 = acc . Nest ( shape = ( 16 , 10 , 11 )) i0 , j0 , k0 = nest0 . get_indices () @nest0 . iteration_logic def _ (): C [ i0 , j0 ] += A [ i0 , k0 ] * B [ k0 , j0 ] schedule0 = nest0 . create_schedule () # Create nest1 and schedule1 nest1 = acc . Nest ( shape = ( 16 , 10 )) i1 , j1 = nest1 . get_indices () @nest1 . iteration_logic def _ (): C [ i1 , j1 ] = acc . max ( C [ i1 , j1 ], 0 ) schedule1 = nest1 . create_schedule () In both schedule0 and schedule1 , the first dimension corresponds to the rows of C and the second dimension corresponds to the columns of C . In addition, schedule0 has a third dimension that schedule1 does not have. Therefore, we fuse the first two dimensions of the iteration spaces and leave the third dimension of schedule0 unfused. schedule = acc . fuse (( schedule0 , schedule1 ), partial = 2 ) f , i , j , k0 = schedule . get_indices () The fused iteration space schedule has a shape of (2, 16, 10, 11), its slice (0, *, *, *) contains a copy of schedule0 , its slice (1, *, *, 0) contains a copy of schedule1 , and the rest of its elements are filled with padding. Note that the code above overwrites the index k0 : originally, k0 was an index of schedule0 , but now it is the corresponding unfused index in schedule . This is a stylistic choice and we could have chosen a different name. Is schedule safe? Recall that Accera guarantees that for each value of i and j , the corresponding work in schedule0 (which is C[i,j] += A[i,k0] * B[k0,j] for all values of k0 ) is executed before the corresponding work in schedule1 (which is C[i,j] = max(C[i,j], 0) ), and this holds regardless of how the fused schedule is transformed. Since these are the only operations that touch C[i,j] and the ReLU operation is always executed last, this confirms that schedule is safe, and from this point forward we can focus all of our attention on optimizing performance without worrying about correctness. Executing schedule as-is is equivalent to executing schedule0 in its entirety and then executing schedule1 . If we want to interleave the two schedules and perform relu immediately after calculating each element of the matrix product, we reorder the dimensions such that i and j preceded f : schedule . reorder ( i , j , f , k0 ) The resulting schedule is now equivalent to the following Python code: for i in range ( 16 ): for j in range ( 10 ): # f = 0 for k0 in range ( 11 ): C [ i , j ] += A [ i , k0 ] * B [ k0 , j ] # f = 1 C [ i , j ] = max ( C [ i , j ], 0 ) Partial fusing example: multiplying three matrices Consider fusing two matrix-matrix multiplications, to get matrix-matrix-matrix multiplication. Specifically, say that our goal is to calculate the equivalent of the Python code: E += A @ B @ D where A is of shape (16, 11), B is of shape (11, 10), D is of shape (10, 7), and E is of shape (16, 7). We start by defining the arrays. In addition to A , B , D , and E , we define a temporary array C to store the intermediate result of A@B . A = acc . Array ( role = acc . Array . Role . INPUT , shape = ( 16 , 11 )) B = acc . Array ( role = acc . Array . Role . INPUT , shape = ( 11 , 10 )) C = acc . Array ( role = acc . Array . Role . TEMP , shape = ( 16 , 10 )) D = acc . Array ( role = acc . Array . Role . INPUT , shape = ( 10 , 7 )) E = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , shape = ( 16 , 7 )) Note that C is declared with a role of TEMP . Recall that temporary arrays are mutable and initialized with zeros. Moreover, temporary arrays are logical objects, which may not actually exist in memory during the entire computation. Next, define a simple nest to compute C += A @ B and another simple nest to compute E += C @ D . # Create nest0 and schedule0 for C = A @ B nest0 = acc . Nest ( shape = ( 16 , 10 , 11 )) i0 , j0 , k0 = nest0 . get_indices () @nest0 . iteration_logic def _ (): C [ i0 , j0 ] += A [ i0 , k0 ] * B [ k0 , j0 ] schedule0 = nest0 . create_schedule () # Create nest1 and schedule1 E += C @ D nest1 = acc . Nest ( shape = ( 16 , 7 , 10 )) i1 , j1 , k1 = nest1 . get_indices () @nest1 . iteration_logic def _ (): E [ i1 , j1 ] += C [ i1 , k1 ] * D [ k1 , j1 ] schedule1 = nest1 . create_schedule () The temporary array C is used to store the output of schedule0 , and is then used again as one of the inputs of schedule1 . Dimensions i0 and j0 correspond to the rows and columns of C in schedule0 . Dimensions i1 and k1 correspond to the rows and columns of C in schedule1 . Therefore, we fuse i0 with i1 and j0 with k1 . To do this, we need to correctly line-up the dimensions of the two iteration spaces and perform partial fusing. schedule1 . reorder ( i1 , k1 , j1 ) schedule = acc . fuse (( schedule0 , schedule1 ), partial = 2 ) f , i , j , k0 , j1 = schedule . get_indices () The fused iteration space has a shape of (2, 16, 10, 11, 7): f is the fusing dimension, i is the result of fusing i0 and i1 , j is the result of fusing j0 and k1 , k0 is the unfused dimension from schedule0 , and j1 is the unfused dimension from schedule1 . The slice (0, *, *, *, 0) contains a copy of schedule0 and the slice (1, *, *, 0, *) contains a copy of schedule1 . The rest of the iteration space is padded with empty elements. Is schedule safe? Again, recall that Accera guarantees that for each value of i and j , all of the corresponding work in schedule0 (which is C[i, j] += A[i, k0] * B[k0, j] for all values of k0 ) is executed before any of the corresponding work from schedule1 (which is E[i, j1] += C[i, j] * D[j, j1] for all values of j1 ). In other words, each element of C is fully computed before it is used. This confirms that schedule is safe. Initially, the fused schedule is equivalent to the following Python code: # f = 0 for i in range ( 0 , 16 ): for j in range ( 0 , 10 ): for k0 in range ( 11 ): C [ i , j ] += A [ i , k0 ] * B [ k0 , j ] # f = 1 for i in range ( 0 , 16 ): for j in range ( 0 , 10 ): for j1 in range ( 7 ): E [ i , j1 ] += C [ i , j ] * D [ j , j1 ] We can now manipulate the fused schedule in various ways. For example, we can do all the work to create one element of C and then immediately do all the work that uses that element, before moving on to the next element. schedule . reorder ( i , j , f , k0 , j1 ) This schedule is equivalent to the following Python code: for i in range ( 0 , 16 ): for j in range ( 0 , 10 ): # f = 0, create C[i, j] for k0 in range ( 11 ): C [ i , j ] += A [ i , k0 ] * B [ k0 , j ] # f = 1, use C[i, j] for j1 in range ( 7 ): E [ i , j1 ] += C [ i , j ] * D [ j , j1 ] The advantage of this schedule is that only one element of C is active at any time in the computation. Accera can reuse the same memory location to store the active element of C , instead of storing all of C in physical memory, Similarly, we can compute a 4\u00d72 block of C , do all the work that uses that block, and then move on to the next block: ii , jj = schedule . tile (( i , j ), ( 4 , 2 )) schedule . reorder ( i , j , f , ii , jj , k0 , j1 ) This schedule is equivalent to the following: for i in range ( 0 , 16 , 4 ): for j in range ( 0 , 10 , 2 ): # f = 0 for ii in range ( 4 ): for jj in range ( 2 ): for k0 in range ( 11 ): C [ i + ii , j + jj ] += A [ i + ii , k0 ] * B [ k0 , j + jj ] # f = 1 for ii in range ( 4 ): for jj in range ( 2 ): for j1 in range ( 7 ): E [ i + ii , j1 ] += C [ i + ii , j + jj ] * D [ j + jj , j1 ]","title":"Fusing"},{"location":"Manual/04%20Fusing/#section-4-fusing","text":"Multiple schedules can be combined into a single schedule using the fuse operation. The fused schedule represents the union of the work in the original schedules. The fused schedule can be transformed using any of the transformations presented in Section 3 .","title":"Section 4: Fusing"},{"location":"Manual/04%20Fusing/#full-fusing","text":"import accera as acc # Fuse three schedules to create a fused schedule schedule = acc . fuse ( schedule0 , schedule1 , ... ) Full fusing is the most straightforward form of fusing, where each dimension is fused with the corresponding dimension from the other schedules.","title":"Full fusing"},{"location":"Manual/04%20Fusing/#full-fusing-of-same-shaped-iteration-spaces","text":"First, consider the simplest case, where we fuse schedules whose iteration spaces have identical shapes. The fused schedule schedule gets a new dimension, called the fusing dimension , which did not exist in the original schedules. By default, the fusing dimension is the first dimension in the fused schedule and its size equals the number of schedules that were fused. The first slice along the fusing dimension contains a copy of schedule0 , the second slice contains a copy schedule1 , and so on. Since the fusing dimension is the first dimension, the fused schedule is logically equivalent to fully executing schedule0 , followed by schedule1 , and so on. To interleave the original schedules, we apply additional transformations to the fused schedule. As a concrete example, imagine that we want to shift and then scale each of the elements of a matrix, or in other words, perform the equivalent of the Python code: C = ( C + A ) * B where all three matrices are 16 by 16. One way to do this without fusing is to simply write: A = acc . Array ( role = acc . Array . Role . INPUT , shape = ( 16 , 16 )) B = acc . Array ( role = acc . Array . Role . INPUT , shape = ( 16 , 16 )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , shape = ( 16 , 16 )) # Create nest_simple and schedule_simple nest_simple = acc . Nest ( shape = ( 16 , 16 )) i , j = nest_simple . get_indices () @nest_simple . iteration_logic def _ (): C [ i , j ] = ( C [ i , j ] + A [ i , j ]) * B [ i , j ] schedule_simple = nest_simple . create_schedule () Note that each iteration in schedule_simple operates on all three arrays at once. Imagine that there is some computational advantage to operating on only two arrays at a time. For example, say that operating on all three arrays simultaneously creates excessive pressure on the computer's memory caches, which could hurt performance. Therefore, we may want to first compute C += A and only then compute C *= B . Better yet, we may want to compute C in 4\u00d74 blocks: first computing C[0:4, 0:4] += A[0:4, 0:4] ; next computing C[0:4, 0:4] *= B[0:4, 0:4] ; then moving on to the next block and computing C[4:8, 0:4] += A[4:8, 0:4] , and so on. Fusing gives us the flexibility to explore all of these possibilities, and more. First, we define two separate nests, one for the logic C += A and one for the logic C *= B , and obtain their corresponding default schedules: # Create nest0 and schedule0 nest0 = acc . Nest ( shape = ( 16 , 16 )) i0 , j0 = nest0 . get_indices () @nest0 . iteration_logic def _ (): C [ i0 , j0 ] += A [ i0 , j0 ] schedule0 = nest0 . create_schedule () # Create nest1 and schedule1 nest1 = acc . Nest ( shape = ( 16 , 16 )) i1 , j1 = nest1 . get_indices () @nest1 . iteration_logic def _ (): C [ i1 , j1 ] *= B [ i1 , j1 ] schedule1 = nest1 . create_schedule () Before fusing, both schedule0 and schedule1 have a shape of (16, 16). Next we fuse them: # Create a fused schedule schedule = acc . fuse ( schedule0 , schedule1 ) f , i , j = schedule . get_indices () Fusing does not change schedule0 or schedule1 but rather creates a new fused schedule named schedule , whose shape is (2, 16, 16). The first dimension in schedule is the so-called fusing dimension f , its slice (0, *, *) contains a copy of schedule0 , and its slice (1, *, *) contains a copy of schedule1 . In loop form, schedule is now equivalent to the following Python code: # f = 0 for i in range ( 16 ): for j in range ( 16 ): C [ i , j ] += A [ i , j ] # f = 1 for i in range ( 16 ): for j in range ( 16 ): C [ i , j ] *= B [ i , j ] Not much has happened yet: executing schedule as-is is equivalent to executing schedule0 and then executing schedule1 . However, this can be changed by transforming the fused schedule. For example, we can recover schedule_simple by reordering the indices as follows: schedule . reorder ( i , j , f ) The fusing dimension moves from the first position to the last position. Now, schedule is equivalent to the following Python code: for i in range ( 16 ): for j in range ( 16 ): # f = 0 C [ i , j ] += A [ i , j ] # f = 1 C [ i , j ] *= B [ i , j ] We also discussed computing the output block-by-block: first computing C[0:4, 0:4] += A[0:4, 0:4] , then computing C[0:4, 0:4] *= B[0:4, 0:4] , and so on. This can be accomplished with the following sequence of transformations ii , jj = schedule . tile (( i , j ), ( 4 , 4 )) schedule . reorder ( i , j , f , ii , jj ) The resulting schedule is equivalent to the Python code: for i in range ( 0 , 16 , 4 ): for j in range ( 0 , 16 , 4 ): # f = 0 for ii in range ( 4 ): for jj in range ( 4 ): C [ i + ii , j + jj ] += A [ i + ii , j + jj ] # f = 1 for ii in range ( 4 ): for jj in range ( 4 ): C [ i + ii , j + jj ] *= B [ i + ii , j + jj ]","title":"Full fusing of same-shaped iteration spaces"},{"location":"Manual/04%20Fusing/#constraint-1-the-fusing-dimension-is-executed-sequentially","text":"The fusing dimension has a special constraint, which does not apply to other dimensions. Specifically, the fusing dimension cannot be parallelized or vectorized (parallelization and vectorization are presented in Section 7 ) and it must be executed sequentially. This constraint enables the safety guarantee discussed below.","title":"Constraint 1: the fusing dimension is executed sequentially"},{"location":"Manual/04%20Fusing/#safety","text":"The fused schedule (before applying any subsequent transformations) is always logically equivalent to executing the original schedules one-by-one. However, is it safe? Recall that a schedule is considered safe if its underlying logic is guaranteed not to change, regardless of how we transform it. The safety of a fully fused schedule depends on the circumstances: Accera guarantees that the order of the fused schedules is preserved for each value of the fused dimensions , regardless of how the fused schedule is transformed. For example, in the example above, the fused dimensions are i and j . Therefore, for any concrete value of i and j , the corresponding operation from schedule0 is guaranteed to execute before the corresponding operation from schedule1 , regardless of how the fused schedule is transformed. More specifically, for each i and j , the operation C[i, j] += A[i, j] is guaranteed to execute before the operation C[i, j] *= B[i, j] , no matter how we transform the fused schedule. Since those are the only operations that touch C[i,j] , the Accera guarantee is sufficient and we conclude that fused schedule is safe. With this guarantee, the programmer can apply any sequence of transformations without worrying about the resulting implementation's correctness. However, note that not every fusing operation creates a safe schedule. For example, imagine that we had fused schedule0 and schedule1 differently: # Reorder schedule1 before fusing schedule1 . reorder ( j1 , i1 ) # Fuse schedule0 with the reordered schedule1 schedule_t = acc . fuse ( schedule0 , schedule1 ) f , a , b = schedule_t . get_indices () In this unnatural example, i0 and j1 are fused and named a , and i1 and j0 are fused and named b . As before, Accera guarantees that for each value of a and b the operation C[a, b] += A[a, b] is executed before C[b, a] *= B[b, a] . As noted above, the fusing operation itself preserves logical equivalence, but if we proceed to transform the fused schedule as follows, schedule_t . reorder ( a , b , f ) the logic actually changes. To see this, note that the resulting schedule is equivalent to the following Python code: for a in range ( 16 ): for b in range ( 16 ): C [ a , b ] += A [ a , b ] C [ b , a ] *= B [ b , a ] In particular, this code sets C[1,0] to C[1,0] * B[1,0] + A[1,0] , whereas the original fused logic set C[1,0] to (C[1,0] + A[1,0]) * B[1,0] . We conclude that schedule_t is certainly not safe. If the programmer creates an unsafe schedule, they take upon themselves the responsibility of maintaining logical equivalence.","title":"Safety"},{"location":"Manual/04%20Fusing/#fusing-iteration-spaces-with-different-shapes","text":"If the iterations spaces have different shapes, Accera matches their shapes by padding them appropriately with empty cells.","title":"Fusing iteration spaces with different shapes"},{"location":"Manual/04%20Fusing/#partial-fusing","text":"In many cases, instead of fusing all of the dimensions, we only need to fuse some of the dimensions, leaving the rest unfused. To fuse the first s dimensions, we use the syntax # Fuse the first s dimensions of three schedules schedule = acc . fuse (( schedule0 , schedule1 , ... ), partial = s ) The order of the dimensions in the fused schedule is as follows: first the fusing dimension f , then the s fused dimensions, then the unfused dimensions of schedule0 , schedule1 , etc. We can easily calculate the number of dimensions in the fused schedule. For example, if we fuse the first s dimensions of a d0 -dimensional space schedule0 and a d1 -dimensional space schedule1 , the fused iteration space will have s fused dimensions, d0 + d1 - 2s unfused dimensions, and the special fusing dimension f , for a total of d0 + d1 - s + 1 dimensions. As before, the fuse operation uses padding to ensure that the fused iteration space is not jagged in any direction. For example, say that schedule0 is 4-dimensional, schedule1 is 3-dimensional, and we partially fuse their first 2 dimensions: schedule = acc . fuse (( schedule0 , schedule1 ), partial = 2 ) f , i , j , k , l , m = schedule . get_indices () The first dimension is the fusing dimensions f , whose size is 2. Next come the fused dimensions i and j . Then, k and l are the two unfused dimensions from schedule0 and m is the unfused dimension from schedule1 . The slice (0, *, *, *, *, 0) contains a copy of schedule0 , the slice (1, *, *, 0, 0, *) contains a copy of schedule1 , and the rest of schedule is padded with empty elements. Note that full fusing is a special case of partial fusing, where s is the larger of the dimensions of schedule0 and schedule1 .","title":"Partial fusing"},{"location":"Manual/04%20Fusing/#constraint-2-the-fusing-dimension-always-precedes-unfused-dimensions","text":"Partial fusing introduces a second constraint on the fusing dimension. Namely, the fusing dimension must precede all of the unfused dimensions in the dimension order. This constraint also applies to dimensions that are derived from the fusing dimension and from the unfused dimensions via splitting.","title":"Constraint 2: the fusing dimension always precedes unfused dimensions"},{"location":"Manual/04%20Fusing/#safety_1","text":"The safety guarantees for partial-fusing are a natural extension of the guarantees for full fusing. Accera guarantees that the order of the fused schedules is preserved for each value of the fused dimensions , regardless of how the fused schedule is transformed. In other words, for each concrete value of the fused dimensions, all the corresponding work in schedule0 (across all of its unfused dimensions) is performed before any of the corresponding work in schedule1 (across all of its unfused dimensions), and this holds no matter how we transform the fused schedule. When fusing, the programmer needs to consider whether this property implies safety - we will show how this can be done in each of the examples below.","title":"Safety"},{"location":"Manual/04%20Fusing/#partial-fusing-example-fully-connected-neural-layer-with-activation","text":"Consider applying an element-wise operation, such as the ReLU function from the field of AI, to the result of a matrix-matrix multiplication. In the language of neural networks, this is called a fully connected layer with a ReLU activation. The function relu(x) is simply max(x,0) . Specifically, imagine that we have an element-wise operator relu and we want to implement the equivalent of the Python code: C = relu ( C + A @ B ) where A has a shape of (16, 11), B has a shape of (11, 10), and C has a shape of (16, 10). We define two nests, one for C += A @ B and the other for C = relu(C) , and obtain their corresponding default schedules: # Create nest0 and schedule0 nest0 = acc . Nest ( shape = ( 16 , 10 , 11 )) i0 , j0 , k0 = nest0 . get_indices () @nest0 . iteration_logic def _ (): C [ i0 , j0 ] += A [ i0 , k0 ] * B [ k0 , j0 ] schedule0 = nest0 . create_schedule () # Create nest1 and schedule1 nest1 = acc . Nest ( shape = ( 16 , 10 )) i1 , j1 = nest1 . get_indices () @nest1 . iteration_logic def _ (): C [ i1 , j1 ] = acc . max ( C [ i1 , j1 ], 0 ) schedule1 = nest1 . create_schedule () In both schedule0 and schedule1 , the first dimension corresponds to the rows of C and the second dimension corresponds to the columns of C . In addition, schedule0 has a third dimension that schedule1 does not have. Therefore, we fuse the first two dimensions of the iteration spaces and leave the third dimension of schedule0 unfused. schedule = acc . fuse (( schedule0 , schedule1 ), partial = 2 ) f , i , j , k0 = schedule . get_indices () The fused iteration space schedule has a shape of (2, 16, 10, 11), its slice (0, *, *, *) contains a copy of schedule0 , its slice (1, *, *, 0) contains a copy of schedule1 , and the rest of its elements are filled with padding. Note that the code above overwrites the index k0 : originally, k0 was an index of schedule0 , but now it is the corresponding unfused index in schedule . This is a stylistic choice and we could have chosen a different name. Is schedule safe? Recall that Accera guarantees that for each value of i and j , the corresponding work in schedule0 (which is C[i,j] += A[i,k0] * B[k0,j] for all values of k0 ) is executed before the corresponding work in schedule1 (which is C[i,j] = max(C[i,j], 0) ), and this holds regardless of how the fused schedule is transformed. Since these are the only operations that touch C[i,j] and the ReLU operation is always executed last, this confirms that schedule is safe, and from this point forward we can focus all of our attention on optimizing performance without worrying about correctness. Executing schedule as-is is equivalent to executing schedule0 in its entirety and then executing schedule1 . If we want to interleave the two schedules and perform relu immediately after calculating each element of the matrix product, we reorder the dimensions such that i and j preceded f : schedule . reorder ( i , j , f , k0 ) The resulting schedule is now equivalent to the following Python code: for i in range ( 16 ): for j in range ( 10 ): # f = 0 for k0 in range ( 11 ): C [ i , j ] += A [ i , k0 ] * B [ k0 , j ] # f = 1 C [ i , j ] = max ( C [ i , j ], 0 )","title":"Partial fusing example: fully-connected neural layer with activation"},{"location":"Manual/04%20Fusing/#partial-fusing-example-multiplying-three-matrices","text":"Consider fusing two matrix-matrix multiplications, to get matrix-matrix-matrix multiplication. Specifically, say that our goal is to calculate the equivalent of the Python code: E += A @ B @ D where A is of shape (16, 11), B is of shape (11, 10), D is of shape (10, 7), and E is of shape (16, 7). We start by defining the arrays. In addition to A , B , D , and E , we define a temporary array C to store the intermediate result of A@B . A = acc . Array ( role = acc . Array . Role . INPUT , shape = ( 16 , 11 )) B = acc . Array ( role = acc . Array . Role . INPUT , shape = ( 11 , 10 )) C = acc . Array ( role = acc . Array . Role . TEMP , shape = ( 16 , 10 )) D = acc . Array ( role = acc . Array . Role . INPUT , shape = ( 10 , 7 )) E = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , shape = ( 16 , 7 )) Note that C is declared with a role of TEMP . Recall that temporary arrays are mutable and initialized with zeros. Moreover, temporary arrays are logical objects, which may not actually exist in memory during the entire computation. Next, define a simple nest to compute C += A @ B and another simple nest to compute E += C @ D . # Create nest0 and schedule0 for C = A @ B nest0 = acc . Nest ( shape = ( 16 , 10 , 11 )) i0 , j0 , k0 = nest0 . get_indices () @nest0 . iteration_logic def _ (): C [ i0 , j0 ] += A [ i0 , k0 ] * B [ k0 , j0 ] schedule0 = nest0 . create_schedule () # Create nest1 and schedule1 E += C @ D nest1 = acc . Nest ( shape = ( 16 , 7 , 10 )) i1 , j1 , k1 = nest1 . get_indices () @nest1 . iteration_logic def _ (): E [ i1 , j1 ] += C [ i1 , k1 ] * D [ k1 , j1 ] schedule1 = nest1 . create_schedule () The temporary array C is used to store the output of schedule0 , and is then used again as one of the inputs of schedule1 . Dimensions i0 and j0 correspond to the rows and columns of C in schedule0 . Dimensions i1 and k1 correspond to the rows and columns of C in schedule1 . Therefore, we fuse i0 with i1 and j0 with k1 . To do this, we need to correctly line-up the dimensions of the two iteration spaces and perform partial fusing. schedule1 . reorder ( i1 , k1 , j1 ) schedule = acc . fuse (( schedule0 , schedule1 ), partial = 2 ) f , i , j , k0 , j1 = schedule . get_indices () The fused iteration space has a shape of (2, 16, 10, 11, 7): f is the fusing dimension, i is the result of fusing i0 and i1 , j is the result of fusing j0 and k1 , k0 is the unfused dimension from schedule0 , and j1 is the unfused dimension from schedule1 . The slice (0, *, *, *, 0) contains a copy of schedule0 and the slice (1, *, *, 0, *) contains a copy of schedule1 . The rest of the iteration space is padded with empty elements. Is schedule safe? Again, recall that Accera guarantees that for each value of i and j , all of the corresponding work in schedule0 (which is C[i, j] += A[i, k0] * B[k0, j] for all values of k0 ) is executed before any of the corresponding work from schedule1 (which is E[i, j1] += C[i, j] * D[j, j1] for all values of j1 ). In other words, each element of C is fully computed before it is used. This confirms that schedule is safe. Initially, the fused schedule is equivalent to the following Python code: # f = 0 for i in range ( 0 , 16 ): for j in range ( 0 , 10 ): for k0 in range ( 11 ): C [ i , j ] += A [ i , k0 ] * B [ k0 , j ] # f = 1 for i in range ( 0 , 16 ): for j in range ( 0 , 10 ): for j1 in range ( 7 ): E [ i , j1 ] += C [ i , j ] * D [ j , j1 ] We can now manipulate the fused schedule in various ways. For example, we can do all the work to create one element of C and then immediately do all the work that uses that element, before moving on to the next element. schedule . reorder ( i , j , f , k0 , j1 ) This schedule is equivalent to the following Python code: for i in range ( 0 , 16 ): for j in range ( 0 , 10 ): # f = 0, create C[i, j] for k0 in range ( 11 ): C [ i , j ] += A [ i , k0 ] * B [ k0 , j ] # f = 1, use C[i, j] for j1 in range ( 7 ): E [ i , j1 ] += C [ i , j ] * D [ j , j1 ] The advantage of this schedule is that only one element of C is active at any time in the computation. Accera can reuse the same memory location to store the active element of C , instead of storing all of C in physical memory, Similarly, we can compute a 4\u00d72 block of C , do all the work that uses that block, and then move on to the next block: ii , jj = schedule . tile (( i , j ), ( 4 , 2 )) schedule . reorder ( i , j , f , ii , jj , k0 , j1 ) This schedule is equivalent to the following: for i in range ( 0 , 16 , 4 ): for j in range ( 0 , 10 , 2 ): # f = 0 for ii in range ( 4 ): for jj in range ( 2 ): for k0 in range ( 11 ): C [ i + ii , j + jj ] += A [ i + ii , k0 ] * B [ k0 , j + jj ] # f = 1 for ii in range ( 4 ): for jj in range ( 2 ): for j1 in range ( 7 ): E [ i + ii , j1 ] += C [ i + ii , j + jj ] * D [ j + jj , j1 ]","title":"Partial fusing example: multiplying three matrices"},{"location":"Manual/05%20Targets/","text":"Section 5: Targets Accera is a cross compiler, which means that it can generate code for different target platforms. A target is described using the Target class. Accera already knows about many different targets, for example: import accera as acc corei9 = acc . Target ( Target . Model . INTEL_7960X , num_threads = 44 ) or v100 = acc . Target ( Target . Model . NVIDIA_V100 ) We can also define custom targets: my_target = acc . Target ( name = \"Custom processor\" , category = acc . Target . Category . CPU , architecture = acc . Target . Architecture . X86_64 , family = \"Broadwell\" , extensions = [ \"MMX\" , \"SSE\" , \"SSE2\" , \"SSE3\" , \"SSSE3\" , \"SSE4\" , \"SSE4.1\" , \"SSE4.2\" , \"AVX\" , \"AVX2\" , \"FMA3\" ], num_cores = 22 , num_threads = 44 , frequency_GHz = 3.2 , turbo_frequency_GHz = 3.8 , cache_sizes = [ 32 , 256 , 56320 ], cache_lines = [ 64 , 64 , 64 ]) One benefit of targets is that they provide a standard way of accessing useful constants. For example, we may want to split an iteration space dimension by the number of elements that fit in a vector register. schedule . split ( i , size = corei9 . vector_bytes / 4 ) For GPU targets, we may tile the iteration space based on input shapes and available resources like shared memory. If you don't know what to use, try starting with the default: # find block_x and block_y in powers of two, such that block_x*block_y=v100.default_block_size. block_x = pow ( 2 , math . log2 ( v100 . default_block_size ) // 2 ) block_y = v100 . default_block_size // block_x ii , jj = schedule . tile (( i , j ), shape = ( block_x , block_y ))","title":"Targets"},{"location":"Manual/05%20Targets/#section-5-targets","text":"Accera is a cross compiler, which means that it can generate code for different target platforms. A target is described using the Target class. Accera already knows about many different targets, for example: import accera as acc corei9 = acc . Target ( Target . Model . INTEL_7960X , num_threads = 44 ) or v100 = acc . Target ( Target . Model . NVIDIA_V100 ) We can also define custom targets: my_target = acc . Target ( name = \"Custom processor\" , category = acc . Target . Category . CPU , architecture = acc . Target . Architecture . X86_64 , family = \"Broadwell\" , extensions = [ \"MMX\" , \"SSE\" , \"SSE2\" , \"SSE3\" , \"SSSE3\" , \"SSE4\" , \"SSE4.1\" , \"SSE4.2\" , \"AVX\" , \"AVX2\" , \"FMA3\" ], num_cores = 22 , num_threads = 44 , frequency_GHz = 3.2 , turbo_frequency_GHz = 3.8 , cache_sizes = [ 32 , 256 , 56320 ], cache_lines = [ 64 , 64 , 64 ]) One benefit of targets is that they provide a standard way of accessing useful constants. For example, we may want to split an iteration space dimension by the number of elements that fit in a vector register. schedule . split ( i , size = corei9 . vector_bytes / 4 ) For GPU targets, we may tile the iteration space based on input shapes and available resources like shared memory. If you don't know what to use, try starting with the default: # find block_x and block_y in powers of two, such that block_x*block_y=v100.default_block_size. block_x = pow ( 2 , math . log2 ( v100 . default_block_size ) // 2 ) block_y = v100 . default_block_size // block_x ii , jj = schedule . tile (( i , j ), shape = ( block_x , block_y ))","title":"Section 5: Targets"},{"location":"Manual/06%20Plans%20-%20Caching/","text":"Section 6: Plans - Caching In the previous sections, we defined the logic and then scheduled its iterations. The next step is to complete the implementation with target-specific options. First, we create a plan from the schedule: plan = schedule . create_plan () We can create multiple plans from a schedule and modify each one without changing the schedule. To manually specify the target platform, the call to create_plan can take a target argument. The default value of this argument is acc.Target.HOST , which represents the current host computer. In this section, we discuss how to add data caching strategies to a plan. Key slices Recall that a slice is a set of iteration space elements that match a coordinate template with wildcards, such as (1, *, 3) . A key-slice is a slice whose wildcards are right-aligned, such as (1, 2, *) and (3, *, *) . The level of a key-slice is the number of wildcards in its definition, so for example, (1, 2, *) is a level 1 key-slice and (3, *, *) is a level 2 key slice. Note that reordering the dimensions of an iteration space changes which slices are key-slices. However, it is always true that the entire d -dimensional iteration space is a level d key-slice and each individual element is a level zero key-slice. Each iteration belongs to one key-slice from each level, from zero to d , for a total of d+1 different key-slices. When the schedule is executed, the key-slices that contain the current iteration are called the current key-slices . The significance of key-slices is that they partition the iteration space into sets of consecutive iterations. Therefore, they can be used to describe phases of the computation, at different levels of granularity. The term key-slice suggests that we will use them to key different actions. Specifically, each time the current level- l key slice changes, we use this event to trigger a cache update. As mentioned above, we can identify a specific current key-slice by noting its level. Another way to specify a current key-slice is to take advantage of the fact that the iteration space dimensions are named and ordered, and to specify the first dimension that is replaced by a wildcard in the key-slice definition. For example, if the names of the iteration space dimensions are (i, j, k) , then the current key-slice that corresponds to the dimension j is one of (0, *, *) , (1, *, *) , etc. Both ways of specifying a current key-slice are useful and Accera uses them interchangeably. Active elements and active blocks A loop nest operates on data stored in arrays. Each key-slice touches a subset of the array elements, which we call the active elements that correspond to that key-slice. Since the current iteration belongs to key-slices at different levels, we also define corresponding sets of active elements at different levels. More precisely, the elements of an array A that are touched (read from or written to) by the iterations of the current level l key-slice are called the level l active elements of A . This set of elements does not necessarily take the shape of a block. Therefore, we define the level l active block of A as the smallest block of elements that contains all of the level l active elements in A . Accera uses active blocks to define caching strategies. Just like we can specify a current key-slice using a dimension, we can also refer to the active block that corresponds to a dimension. For example, if the names of the iteration space dimensions are (i, j, k) and the current iteration is one of the iterations for which i=3 then the active block in A that corresponds to dimension j is the block that includes all the elements touched by the key-slice (3, *, *) . Caches An Accera cache is a local copy of an active block. A cache is contiguous in memory and its memory layout may be different from the layout of the original array. The loop nest iterations operate on the elements of the cache instead of the original array elements. The contents of the active block are copied into the cache at the beginning of the corresponding key-slice. If the array is mutable (namely, if it is an input/output array or a temporary array), the contents of the cache are also copied back into the original array at the end of the key-slice. Caching by level To define a cache for a given array, all we need to do is specify the desired level. For example: AA = plan . cache ( A , level = 2 ) The return value AA is a handle that can be used to refer to the cache in subsequent operations. We can choose the cache layout, just as we did when we defined the original array. AA = plan . cache ( A , level = 2 , layout = acc . Array . Layout . FIRST_MAJOR ) Caching by dimension As mentioned above, we can also specify an active block using a dimension. We use this to define a cache as follows: AA = plan . cache ( A , index = j ) Caching by element budget Note that the current active blocks of an array are nested, and their size is monotonic (nondecreasing) in their level. Therefore, we can also choose the largest active block that does not exceed a certain budget of elements. AA = plan . cache ( A , max_elements = 1024 ) Not yet implemented: Thrifty caching By default, Accera caching strategies are thrifty , which means that data is physically copied into an allocated cache only if the cached data somehow differs from the original active block. In other words, if the original active block happens to already be contiguous in memory and in the correct memory layout, then Accera skips the caching step and instead just uses the original array. Note that on a GPU, if the cache is supposed to be allocated in a different type of memory than the original array (e.g., the array is in global memory but the cache is supposed to be in shared memory) then a physical copy is created. For example, say that A is a two-dimensional array and its active block at the chosen level is always one of its rows. If A is row-major, its rows are already stored contiguously and the data in the active block is identical to the data that would be copied into the cache: both are contiguous and both share the same layout. Since the two are identical, there is no benefit to using the cache over using the original array. The thrifty caching strategy skips the caching step and instead uses the data in the original array. On the other hand, if A is column-major, its rows are not stored contiguously. Copying the active row into a contiguous temporary location could be computationally advantageous. In this case, the thrifty caching strategy would create the cache and populate it with data. Thrifty caching can be turned off using the optional argument thrifty=False . When thrifty caching is turned off, a physical copy is always created. Hierarchical caching Caches can be composed hierarchically. Namely, a high-level key-slice can trigger a copy from the original array into a big cache, and a lower level key-slice can be used to trigger a copy from the big cache into a smaller cache. For example, AA = plan . cache ( A , level = 4 ) AAA = plan . cache ( AA , level = 2 ) Multicaching Caches are defined with a key-slice level , and a higher-level key slice trigger_level can be specified as the trigger key-slice for copying multiple successive active blocks of elements to a local copy. These copied active blocks each have their layouts defined as usual, only the trigger level for copying them has been changed. Note that since active blocks are not mutually exclusive, this can result in the same element being copied into multiple locations in the local copy, however they will be in separate caches. Because of this, a trigger_level may only be specified on an INPUT or CONST array as Accera does not perform multicache write coherence. For example, AA = plan . cache ( A , level = 2 , trigger_level = 4 ) Not yet implemented: Mapping caches to specific types of memory Some target platforms have different types of memory that can hold Accera caches. For example, on a GPU target, caches can be located in global memory or shared memory . To explicitly choose the location of the cache, we write AA = plan . cache ( A , level = 4 , location = v100 . MemoryType . SHARED )","title":"Plans - Caching"},{"location":"Manual/06%20Plans%20-%20Caching/#section-6-plans-caching","text":"In the previous sections, we defined the logic and then scheduled its iterations. The next step is to complete the implementation with target-specific options. First, we create a plan from the schedule: plan = schedule . create_plan () We can create multiple plans from a schedule and modify each one without changing the schedule. To manually specify the target platform, the call to create_plan can take a target argument. The default value of this argument is acc.Target.HOST , which represents the current host computer. In this section, we discuss how to add data caching strategies to a plan.","title":"Section 6: Plans - Caching"},{"location":"Manual/06%20Plans%20-%20Caching/#key-slices","text":"Recall that a slice is a set of iteration space elements that match a coordinate template with wildcards, such as (1, *, 3) . A key-slice is a slice whose wildcards are right-aligned, such as (1, 2, *) and (3, *, *) . The level of a key-slice is the number of wildcards in its definition, so for example, (1, 2, *) is a level 1 key-slice and (3, *, *) is a level 2 key slice. Note that reordering the dimensions of an iteration space changes which slices are key-slices. However, it is always true that the entire d -dimensional iteration space is a level d key-slice and each individual element is a level zero key-slice. Each iteration belongs to one key-slice from each level, from zero to d , for a total of d+1 different key-slices. When the schedule is executed, the key-slices that contain the current iteration are called the current key-slices . The significance of key-slices is that they partition the iteration space into sets of consecutive iterations. Therefore, they can be used to describe phases of the computation, at different levels of granularity. The term key-slice suggests that we will use them to key different actions. Specifically, each time the current level- l key slice changes, we use this event to trigger a cache update. As mentioned above, we can identify a specific current key-slice by noting its level. Another way to specify a current key-slice is to take advantage of the fact that the iteration space dimensions are named and ordered, and to specify the first dimension that is replaced by a wildcard in the key-slice definition. For example, if the names of the iteration space dimensions are (i, j, k) , then the current key-slice that corresponds to the dimension j is one of (0, *, *) , (1, *, *) , etc. Both ways of specifying a current key-slice are useful and Accera uses them interchangeably.","title":"Key slices"},{"location":"Manual/06%20Plans%20-%20Caching/#active-elements-and-active-blocks","text":"A loop nest operates on data stored in arrays. Each key-slice touches a subset of the array elements, which we call the active elements that correspond to that key-slice. Since the current iteration belongs to key-slices at different levels, we also define corresponding sets of active elements at different levels. More precisely, the elements of an array A that are touched (read from or written to) by the iterations of the current level l key-slice are called the level l active elements of A . This set of elements does not necessarily take the shape of a block. Therefore, we define the level l active block of A as the smallest block of elements that contains all of the level l active elements in A . Accera uses active blocks to define caching strategies. Just like we can specify a current key-slice using a dimension, we can also refer to the active block that corresponds to a dimension. For example, if the names of the iteration space dimensions are (i, j, k) and the current iteration is one of the iterations for which i=3 then the active block in A that corresponds to dimension j is the block that includes all the elements touched by the key-slice (3, *, *) .","title":"Active elements and active blocks"},{"location":"Manual/06%20Plans%20-%20Caching/#caches","text":"An Accera cache is a local copy of an active block. A cache is contiguous in memory and its memory layout may be different from the layout of the original array. The loop nest iterations operate on the elements of the cache instead of the original array elements. The contents of the active block are copied into the cache at the beginning of the corresponding key-slice. If the array is mutable (namely, if it is an input/output array or a temporary array), the contents of the cache are also copied back into the original array at the end of the key-slice.","title":"Caches"},{"location":"Manual/06%20Plans%20-%20Caching/#caching-by-level","text":"To define a cache for a given array, all we need to do is specify the desired level. For example: AA = plan . cache ( A , level = 2 ) The return value AA is a handle that can be used to refer to the cache in subsequent operations. We can choose the cache layout, just as we did when we defined the original array. AA = plan . cache ( A , level = 2 , layout = acc . Array . Layout . FIRST_MAJOR )","title":"Caching by level"},{"location":"Manual/06%20Plans%20-%20Caching/#caching-by-dimension","text":"As mentioned above, we can also specify an active block using a dimension. We use this to define a cache as follows: AA = plan . cache ( A , index = j )","title":"Caching by dimension"},{"location":"Manual/06%20Plans%20-%20Caching/#caching-by-element-budget","text":"Note that the current active blocks of an array are nested, and their size is monotonic (nondecreasing) in their level. Therefore, we can also choose the largest active block that does not exceed a certain budget of elements. AA = plan . cache ( A , max_elements = 1024 )","title":"Caching by element budget"},{"location":"Manual/06%20Plans%20-%20Caching/#not-yet-implemented-thrifty-caching","text":"By default, Accera caching strategies are thrifty , which means that data is physically copied into an allocated cache only if the cached data somehow differs from the original active block. In other words, if the original active block happens to already be contiguous in memory and in the correct memory layout, then Accera skips the caching step and instead just uses the original array. Note that on a GPU, if the cache is supposed to be allocated in a different type of memory than the original array (e.g., the array is in global memory but the cache is supposed to be in shared memory) then a physical copy is created. For example, say that A is a two-dimensional array and its active block at the chosen level is always one of its rows. If A is row-major, its rows are already stored contiguously and the data in the active block is identical to the data that would be copied into the cache: both are contiguous and both share the same layout. Since the two are identical, there is no benefit to using the cache over using the original array. The thrifty caching strategy skips the caching step and instead uses the data in the original array. On the other hand, if A is column-major, its rows are not stored contiguously. Copying the active row into a contiguous temporary location could be computationally advantageous. In this case, the thrifty caching strategy would create the cache and populate it with data. Thrifty caching can be turned off using the optional argument thrifty=False . When thrifty caching is turned off, a physical copy is always created.","title":"Not yet implemented: Thrifty caching"},{"location":"Manual/06%20Plans%20-%20Caching/#hierarchical-caching","text":"Caches can be composed hierarchically. Namely, a high-level key-slice can trigger a copy from the original array into a big cache, and a lower level key-slice can be used to trigger a copy from the big cache into a smaller cache. For example, AA = plan . cache ( A , level = 4 ) AAA = plan . cache ( AA , level = 2 )","title":"Hierarchical caching"},{"location":"Manual/06%20Plans%20-%20Caching/#multicaching","text":"Caches are defined with a key-slice level , and a higher-level key slice trigger_level can be specified as the trigger key-slice for copying multiple successive active blocks of elements to a local copy. These copied active blocks each have their layouts defined as usual, only the trigger level for copying them has been changed. Note that since active blocks are not mutually exclusive, this can result in the same element being copied into multiple locations in the local copy, however they will be in separate caches. Because of this, a trigger_level may only be specified on an INPUT or CONST array as Accera does not perform multicache write coherence. For example, AA = plan . cache ( A , level = 2 , trigger_level = 4 )","title":"Multicaching"},{"location":"Manual/06%20Plans%20-%20Caching/#not-yet-implemented-mapping-caches-to-specific-types-of-memory","text":"Some target platforms have different types of memory that can hold Accera caches. For example, on a GPU target, caches can be located in global memory or shared memory . To explicitly choose the location of the cache, we write AA = plan . cache ( A , level = 4 , location = v100 . MemoryType . SHARED )","title":"Not yet implemented: Mapping caches to specific types of memory"},{"location":"Manual/07%20Plans%20-%20Vectorization%20and%20Parallelization/","text":"Section 7: Plans - Vectorization and Parallelization The plan includes operations and optimizations that control instruction pipelining, vectorized SIMD instructions, and parallelization. unroll By default, each dimension of the iteration space is implemented as a for-loop. The unroll instruction marks a dimension for unrolling rather than looping. For example, imagine that we have the following nest, which multiplies the entires of an array by a constant: import accera as acc my_target = acc . Target ( type = acc . Target . Category . CPU ) nest = acc . Nest ( shape = ( 3 , 5 )) i , j = nest . get_indices () @nest . iteration_logic def _ (): A [ i , j ] *= 2.0 plan = nest . create_plan ( my_target ) If we build plan as is, the resulting implementation would be equivalent to the following Python code: for i in range ( 3 ): for j in range ( 5 ): A [ i , j ] *= 2.0 If we add the instruction plan.unroll(index=j) , the resulting implementation becomes equivalent to: for i in range ( 3 ): A [ i , 0 ] *= 2.0 A [ i , 1 ] *= 2.0 A [ i , 2 ] *= 2.0 A [ i , 3 ] *= 2.0 A [ i , 4 ] *= 2.0 If, instead of unrolling j , we add the instruction plan.unroll(index=i) , the resulting implementation becomes equivalent to: for j in range ( 5 ): A [ 0 , j ] *= 2.0 for j in range ( 5 ): A [ 1 , j ] *= 2.0 for j in range ( 5 ): A [ 2 , j ] *= 2.0 And, of course, we could also unroll both dimensions, removing for-loops completely. vectorize Many modern target platforms support SIMD vector instructions. SIMD instructions perform the same operation on an entire vector of elements, all at once. By default, each dimension of an iteration space becomes a for-loop, but the vectorize instruction labels a dimension for vectorized execution, rather than for-looping. For example, assume that the host supports 256-bit vector instructions, which means that its vector instructions operate on 8 floating-point elements at once. Imagine that we already have arrays A , B , and C , and that we write the following code: nest = acc . Nest ( shape = ( 64 ,)) i = nest . get_indices () @nest . iteration_logic def _ (): C [ i ] = A [ i ] * B [ i ] schedule = nest . create_schedule () ii = schedule . split ( i , 8 ) plan = nest . create_plan () plan . vectorize ( index = ii ) The dimension marked for vectorization is of size 8, which is a supported vector size on the specific target platform. Therefore, the resulting binary will contain something like: 00000001400010B0: C5 FC 10 0C 11 vmovups ymm1,ymmword ptr [rcx+rdx] 00000001400010B5: C5 F4 59 0A vmulps ymm1,ymm1,ymmword ptr [rdx] 00000001400010B9: C4 C1 7C 11 0C 10 vmovups ymmword ptr [r8+rdx],ymm1 00000001400010BF: 48 8D 52 20 lea rdx,[rdx+20h] 00000001400010C3: 48 83 E8 01 sub rax,1 00000001400010C7: 75 E7 jne 00000001400010B0 Note how the multiplication instruction vmulps and the memory move instruction vmovups deal with 8 32-bit floating point values at a time. Different targets support different vector instructions, with different vector sizes. The following table includes iteration logic that vectorizes correctly on most targets with vectorization support, such as Intel Haswell, Broadwell or newer, and ARM v7/A32. Other examples of iteration logic may or may not vectorize correctly. Variables prefixed with v are vector types, and those prefixed with s are scalar types. Vector pseudocode Equivalent to Supported types v1 += s0 * v0 for i in range(vector_size): \u2003 v1[i] += s0 * v0[i] float32 v1 += v0 * s0 for i in range(vector_size): \u2003 v1[i] += v0[i] * s0 float32 v1 += v0 / s0 for i in range(vector_size): \u2003 v1[i] += v0[i] / s0 float32 v1 -= s0 * v0 for i in range(vector_size): \u2003 v1[i] -= s0 * v0[i] float32 v1 -= v0 * s0 for i in range(vector_size): \u2003 v1[i] -= v0[i] * s0 float32 v1 -= v0 / s0 for i in range(vector_size): \u2003 v1[i] -= v0[i] / s0 float32 v2 += v0 * v1 for i in range(vector_size): \u2003 v2[i] += v0[i] * v1[i] float32 vector inner (dot) product: s0 += dot(v0, v1) for i in range(vector_size): \u2003 s0 += v0[i] * v1[i] float32 v2 = v0 + v1 for i in range(vector_size): \u2003 v2[i] = v0[i] + v1[i] int8/16/32/64, float32 v2 = v0 - v1 for i in range(vector_size): \u2003 v2[i] = v0[i] - v1[i] int8/16/32/64, float32 v2 = v0 * v1 for i in range(vector_size): \u2003 v2[i] = v0[i] * v1[i] int8/16/32/64, float32 v2 = v0 / v1 for i in range(vector_size): \u2003 v2[i] = v0[i] / v1[i] float32 v1 = abs(v[0]) for i in range(vector_size): \u2003 v1[i] = abs(v0[i]) int8/16/32/64, float32 v2 = (v0 == v1) for i in range(vector_size): \u2003 v2[i] = 0XF..F if v0[i] == v1[i] else 0 int8/16/32/64, float32 v2 = (v0 > v1) for i in range(vector_size): \u2003 v2[i] = 0XF..F if v0[i] > v1[i] else 0 int8/16/32/64, float32 v2 = (v0 >= v1) for i in range(vector_size): \u2003 v2[i] = 0XF..F if v0[i] >= v1[i] else 0 int8/16/32/64, float32 v2 = (v0 < v1) for i in range(vector_size): \u2003 v2[i] = 0XF..F if v0[i] < v1[i] else 0 int8/16/32/64, float32 v2 = (v0 <= v1) for i in range(vector_size): \u2003 v2[i] = 0XF..F if v0[i] <= v1[i] else 0 int8/16/32/64, float32 v1 = v0 << s0 for i in range(vector_size): \u2003 v1[i] = v0[i] << s0 int16/32/64, float32 v1 = v0 >> s0 for i in range(vector_size): \u2003 v1[i] = v0[i] >> s0 int16/32/64, float32 s0 = sum(v0) for i in range(vector_size): \u2003 s0 += v0[i] int8/16/32/64, float32 s0 = max(v0 + v1) for i in range(vector_size): \u2003 s0 = max(v0[i] + v1[i], s0) int/8int16/32/64, float32 s0 = max(v0 - v1) for i in range(vector_size): \u2003 s0 = max(v0[i] - v1[i], s0) int/8int16/32/64, float32 In addition, Accera can perform vectorized load and store operations to/from vector registers and memory if the memory locations are contiguous. To vectorize dimension i , the number of active elements that corresponds to dimension i must exactly match the vector instruction width of the target processor. For example, if the target processor has vector instructions that operate on either 4 or 8 floating point elements at once, then the number of active elements can be either 4 or 8. Additionally, those active elements must occupy adjacent memory locations (they cannot be spread out). Convenience syntax: kernelize The kernelize instruction is a convenience syntax and does not provide any unique functionality. Specifically, kernelize is equivalent to a sequence of unroll instructions, followed by an optional vectorize instruction. A typical Accera design pattern is to break a loop-nest into tiles and then apply an optimized kernel to each tile. For example, imagine that the loop nest multiplies two 256\u00d7256 matrices and the kernel is a highly optimized procedure for multiplying 4\u00d74 matrices. In the future, Accera will introduce different ways to write highly optimized kernels, but currently, it only supports automatic kernelization using the kernelize instruction. As mentioned above, kernelize is shorthand for unrolling and vectorizing. These instructions structure the code in a way that makes it easy for downstream compiler heuristics to automatically generate kernels. Consider, once again, the matrix multiplication example we saw previously in Section 2 . Imagine we declare the schedule and reorder as follows: schedule = nest . create_schedule () schedule . reorder ( i , k , j ) Notice that i, k, j are the last three dimensions in the iteration space and the resulting implementation becomes equivalent to: for i in range ( M ): for k in range ( S ): for j in range ( N ): C [ i , j ] += A [ i , k ] * B [ k , j ] The instruction: plan . kernelize ( unroll_indices = ( i , k ), vectorize_indices = j ) is just shorthand for plan . unroll ( i ) plan . unroll ( k ) plan . vectorize ( j ) Applying this sequence of instructions allows the compiler to automatically create an optimized kernel from loops i, k, j . For simplicity, let's assume that the matrix sizes, defined by M, N, S are M=3, N=4, S=2. After applying kernelize , the schedule is equivalent to the following Python code: C [ 0 , 0 : 4 ] += A [ 0 , 0 ] * B [ 0 , 0 : 4 ] # vectorized C [ 0 , 0 : 4 ] += A [ 0 , 1 ] * B [ 1 , 0 : 4 ] # vectorized C [ 1 , 0 : 4 ] += A [ 1 , 0 ] * B [ 0 , 0 : 4 ] # vectorized C [ 1 , 0 : 4 ] += A [ 1 , 1 ] * B [ 1 , 0 : 4 ] # vectorized C [ 2 , 0 : 4 ] += A [ 2 , 0 ] * B [ 0 , 0 : 4 ] # vectorized C [ 2 , 0 : 4 ] += A [ 2 , 1 ] * B [ 1 , 0 : 4 ] # vectorized This would result in the following vectorized instructions on an Intel Haswell CPU: 0000000000000200: C4 C1 78 10 00 vmovups xmm0,xmmword ptr [r8] 0000000000000205: C4 E2 79 18 09 vbroadcastss xmm1,dword ptr [rcx] 000000000000020A: C5 F8 10 12 vmovups xmm2,xmmword ptr [rdx] 000000000000020E: C4 E2 69 A8 C8 vfmadd213ps xmm1,xmm2,xmm0 0000000000000213: C5 F8 10 5A 10 vmovups xmm3,xmmword ptr [rdx+10h] 0000000000000218: C4 E2 79 18 61 04 vbroadcastss xmm4,dword ptr [rcx+4] 000000000000021E: C4 E2 61 A8 E1 vfmadd213ps xmm4,xmm3,xmm1 0000000000000223: C4 E2 79 18 49 08 vbroadcastss xmm1,dword ptr [rcx+8] 0000000000000229: C4 E2 69 A8 C8 vfmadd213ps xmm1,xmm2,xmm0 000000000000022E: C4 E2 79 18 69 0C vbroadcastss xmm5,dword ptr [rcx+0Ch] 0000000000000234: C4 E2 61 A8 E9 vfmadd213ps xmm5,xmm3,xmm1 0000000000000239: C4 E2 79 18 49 10 vbroadcastss xmm1,dword ptr [rcx+10h] 000000000000023F: C4 E2 69 A8 C8 vfmadd213ps xmm1,xmm2,xmm0 0000000000000244: C4 E2 79 18 41 14 vbroadcastss xmm0,dword ptr [rcx+14h] 000000000000024A: C4 E2 61 A8 C1 vfmadd213ps xmm0,xmm3,xmm1 000000000000024F: C4 C1 58 58 09 vaddps xmm1,xmm4,xmmword ptr [r9] 0000000000000254: C4 C1 50 58 51 10 vaddps xmm2,xmm5,xmmword ptr [r9+10h] 000000000000025A: C4 C1 78 58 41 20 vaddps xmm0,xmm0,xmmword ptr [r9+20h] parallelize The parallelize instruction performs one or more loops in parallel on multiple cores. xeonPlat = acc . Target ( \"Intel 9221\" , num_threads = 16 ) plan = schedule . create_plan ( xeonPlat ) plan . parallelize ( indices = ( i , j , k )) Specifying multiple dimensions is equivalent to the collapse argument in OpenMP. Therefore, the dimensions must be contiguous in the iteration space dimension order. Static scheduling policy A static scheduling strategy is invoked by setting the argument policy=\"static\" in the call to parallelize . If n iterations are parallelized across c cores, static scheduling partitions the work into c fixed parts, some of size floor(n/c) and some of size ceil(n/c) , and executes each part on a different core. Dynamic scheduling policy A dynamic scheduling strategy is invoked by setting the argument policy=\"dynamic\" in the call to parallelize . Dynamic scheduling creates a single queue of work that is shared across the different cores. Not yet implemented: Pinning to specific cores The pin argument allows the parallel work to be pinned to specific cores. bind Some target platforms, such as GPUs, are specifically designed to execute nested loops. They can take an entire grid of work and schedule its execution on multiple cores. On a GPU, this grid is broken up into multiple blocks, where each block contains multiple threads. Block iterators and thread iterators are identified by special variables in the Target object. To take advantage of a target platform's ability to execute grids, we must bind dimensions of the iteration space with these special iterator variables. For example, v100 = acc . Target ( \"Tesla V100\" ) plan . bind ( indices = ( i , j , k ), grid = ( v100 . GridUnit . BLOCK_X , v100 . GridUnit . THREAD_X , v100 . GridUnit . THREAD_Y ))","title":"Plans - Vectorization and Parallelization"},{"location":"Manual/07%20Plans%20-%20Vectorization%20and%20Parallelization/#section-7-plans-vectorization-and-parallelization","text":"The plan includes operations and optimizations that control instruction pipelining, vectorized SIMD instructions, and parallelization.","title":"Section 7: Plans - Vectorization and Parallelization"},{"location":"Manual/07%20Plans%20-%20Vectorization%20and%20Parallelization/#unroll","text":"By default, each dimension of the iteration space is implemented as a for-loop. The unroll instruction marks a dimension for unrolling rather than looping. For example, imagine that we have the following nest, which multiplies the entires of an array by a constant: import accera as acc my_target = acc . Target ( type = acc . Target . Category . CPU ) nest = acc . Nest ( shape = ( 3 , 5 )) i , j = nest . get_indices () @nest . iteration_logic def _ (): A [ i , j ] *= 2.0 plan = nest . create_plan ( my_target ) If we build plan as is, the resulting implementation would be equivalent to the following Python code: for i in range ( 3 ): for j in range ( 5 ): A [ i , j ] *= 2.0 If we add the instruction plan.unroll(index=j) , the resulting implementation becomes equivalent to: for i in range ( 3 ): A [ i , 0 ] *= 2.0 A [ i , 1 ] *= 2.0 A [ i , 2 ] *= 2.0 A [ i , 3 ] *= 2.0 A [ i , 4 ] *= 2.0 If, instead of unrolling j , we add the instruction plan.unroll(index=i) , the resulting implementation becomes equivalent to: for j in range ( 5 ): A [ 0 , j ] *= 2.0 for j in range ( 5 ): A [ 1 , j ] *= 2.0 for j in range ( 5 ): A [ 2 , j ] *= 2.0 And, of course, we could also unroll both dimensions, removing for-loops completely.","title":"unroll"},{"location":"Manual/07%20Plans%20-%20Vectorization%20and%20Parallelization/#vectorize","text":"Many modern target platforms support SIMD vector instructions. SIMD instructions perform the same operation on an entire vector of elements, all at once. By default, each dimension of an iteration space becomes a for-loop, but the vectorize instruction labels a dimension for vectorized execution, rather than for-looping. For example, assume that the host supports 256-bit vector instructions, which means that its vector instructions operate on 8 floating-point elements at once. Imagine that we already have arrays A , B , and C , and that we write the following code: nest = acc . Nest ( shape = ( 64 ,)) i = nest . get_indices () @nest . iteration_logic def _ (): C [ i ] = A [ i ] * B [ i ] schedule = nest . create_schedule () ii = schedule . split ( i , 8 ) plan = nest . create_plan () plan . vectorize ( index = ii ) The dimension marked for vectorization is of size 8, which is a supported vector size on the specific target platform. Therefore, the resulting binary will contain something like: 00000001400010B0: C5 FC 10 0C 11 vmovups ymm1,ymmword ptr [rcx+rdx] 00000001400010B5: C5 F4 59 0A vmulps ymm1,ymm1,ymmword ptr [rdx] 00000001400010B9: C4 C1 7C 11 0C 10 vmovups ymmword ptr [r8+rdx],ymm1 00000001400010BF: 48 8D 52 20 lea rdx,[rdx+20h] 00000001400010C3: 48 83 E8 01 sub rax,1 00000001400010C7: 75 E7 jne 00000001400010B0 Note how the multiplication instruction vmulps and the memory move instruction vmovups deal with 8 32-bit floating point values at a time. Different targets support different vector instructions, with different vector sizes. The following table includes iteration logic that vectorizes correctly on most targets with vectorization support, such as Intel Haswell, Broadwell or newer, and ARM v7/A32. Other examples of iteration logic may or may not vectorize correctly. Variables prefixed with v are vector types, and those prefixed with s are scalar types. Vector pseudocode Equivalent to Supported types v1 += s0 * v0 for i in range(vector_size): \u2003 v1[i] += s0 * v0[i] float32 v1 += v0 * s0 for i in range(vector_size): \u2003 v1[i] += v0[i] * s0 float32 v1 += v0 / s0 for i in range(vector_size): \u2003 v1[i] += v0[i] / s0 float32 v1 -= s0 * v0 for i in range(vector_size): \u2003 v1[i] -= s0 * v0[i] float32 v1 -= v0 * s0 for i in range(vector_size): \u2003 v1[i] -= v0[i] * s0 float32 v1 -= v0 / s0 for i in range(vector_size): \u2003 v1[i] -= v0[i] / s0 float32 v2 += v0 * v1 for i in range(vector_size): \u2003 v2[i] += v0[i] * v1[i] float32 vector inner (dot) product: s0 += dot(v0, v1) for i in range(vector_size): \u2003 s0 += v0[i] * v1[i] float32 v2 = v0 + v1 for i in range(vector_size): \u2003 v2[i] = v0[i] + v1[i] int8/16/32/64, float32 v2 = v0 - v1 for i in range(vector_size): \u2003 v2[i] = v0[i] - v1[i] int8/16/32/64, float32 v2 = v0 * v1 for i in range(vector_size): \u2003 v2[i] = v0[i] * v1[i] int8/16/32/64, float32 v2 = v0 / v1 for i in range(vector_size): \u2003 v2[i] = v0[i] / v1[i] float32 v1 = abs(v[0]) for i in range(vector_size): \u2003 v1[i] = abs(v0[i]) int8/16/32/64, float32 v2 = (v0 == v1) for i in range(vector_size): \u2003 v2[i] = 0XF..F if v0[i] == v1[i] else 0 int8/16/32/64, float32 v2 = (v0 > v1) for i in range(vector_size): \u2003 v2[i] = 0XF..F if v0[i] > v1[i] else 0 int8/16/32/64, float32 v2 = (v0 >= v1) for i in range(vector_size): \u2003 v2[i] = 0XF..F if v0[i] >= v1[i] else 0 int8/16/32/64, float32 v2 = (v0 < v1) for i in range(vector_size): \u2003 v2[i] = 0XF..F if v0[i] < v1[i] else 0 int8/16/32/64, float32 v2 = (v0 <= v1) for i in range(vector_size): \u2003 v2[i] = 0XF..F if v0[i] <= v1[i] else 0 int8/16/32/64, float32 v1 = v0 << s0 for i in range(vector_size): \u2003 v1[i] = v0[i] << s0 int16/32/64, float32 v1 = v0 >> s0 for i in range(vector_size): \u2003 v1[i] = v0[i] >> s0 int16/32/64, float32 s0 = sum(v0) for i in range(vector_size): \u2003 s0 += v0[i] int8/16/32/64, float32 s0 = max(v0 + v1) for i in range(vector_size): \u2003 s0 = max(v0[i] + v1[i], s0) int/8int16/32/64, float32 s0 = max(v0 - v1) for i in range(vector_size): \u2003 s0 = max(v0[i] - v1[i], s0) int/8int16/32/64, float32 In addition, Accera can perform vectorized load and store operations to/from vector registers and memory if the memory locations are contiguous. To vectorize dimension i , the number of active elements that corresponds to dimension i must exactly match the vector instruction width of the target processor. For example, if the target processor has vector instructions that operate on either 4 or 8 floating point elements at once, then the number of active elements can be either 4 or 8. Additionally, those active elements must occupy adjacent memory locations (they cannot be spread out).","title":"vectorize"},{"location":"Manual/07%20Plans%20-%20Vectorization%20and%20Parallelization/#convenience-syntax-kernelize","text":"The kernelize instruction is a convenience syntax and does not provide any unique functionality. Specifically, kernelize is equivalent to a sequence of unroll instructions, followed by an optional vectorize instruction. A typical Accera design pattern is to break a loop-nest into tiles and then apply an optimized kernel to each tile. For example, imagine that the loop nest multiplies two 256\u00d7256 matrices and the kernel is a highly optimized procedure for multiplying 4\u00d74 matrices. In the future, Accera will introduce different ways to write highly optimized kernels, but currently, it only supports automatic kernelization using the kernelize instruction. As mentioned above, kernelize is shorthand for unrolling and vectorizing. These instructions structure the code in a way that makes it easy for downstream compiler heuristics to automatically generate kernels. Consider, once again, the matrix multiplication example we saw previously in Section 2 . Imagine we declare the schedule and reorder as follows: schedule = nest . create_schedule () schedule . reorder ( i , k , j ) Notice that i, k, j are the last three dimensions in the iteration space and the resulting implementation becomes equivalent to: for i in range ( M ): for k in range ( S ): for j in range ( N ): C [ i , j ] += A [ i , k ] * B [ k , j ] The instruction: plan . kernelize ( unroll_indices = ( i , k ), vectorize_indices = j ) is just shorthand for plan . unroll ( i ) plan . unroll ( k ) plan . vectorize ( j ) Applying this sequence of instructions allows the compiler to automatically create an optimized kernel from loops i, k, j . For simplicity, let's assume that the matrix sizes, defined by M, N, S are M=3, N=4, S=2. After applying kernelize , the schedule is equivalent to the following Python code: C [ 0 , 0 : 4 ] += A [ 0 , 0 ] * B [ 0 , 0 : 4 ] # vectorized C [ 0 , 0 : 4 ] += A [ 0 , 1 ] * B [ 1 , 0 : 4 ] # vectorized C [ 1 , 0 : 4 ] += A [ 1 , 0 ] * B [ 0 , 0 : 4 ] # vectorized C [ 1 , 0 : 4 ] += A [ 1 , 1 ] * B [ 1 , 0 : 4 ] # vectorized C [ 2 , 0 : 4 ] += A [ 2 , 0 ] * B [ 0 , 0 : 4 ] # vectorized C [ 2 , 0 : 4 ] += A [ 2 , 1 ] * B [ 1 , 0 : 4 ] # vectorized This would result in the following vectorized instructions on an Intel Haswell CPU: 0000000000000200: C4 C1 78 10 00 vmovups xmm0,xmmword ptr [r8] 0000000000000205: C4 E2 79 18 09 vbroadcastss xmm1,dword ptr [rcx] 000000000000020A: C5 F8 10 12 vmovups xmm2,xmmword ptr [rdx] 000000000000020E: C4 E2 69 A8 C8 vfmadd213ps xmm1,xmm2,xmm0 0000000000000213: C5 F8 10 5A 10 vmovups xmm3,xmmword ptr [rdx+10h] 0000000000000218: C4 E2 79 18 61 04 vbroadcastss xmm4,dword ptr [rcx+4] 000000000000021E: C4 E2 61 A8 E1 vfmadd213ps xmm4,xmm3,xmm1 0000000000000223: C4 E2 79 18 49 08 vbroadcastss xmm1,dword ptr [rcx+8] 0000000000000229: C4 E2 69 A8 C8 vfmadd213ps xmm1,xmm2,xmm0 000000000000022E: C4 E2 79 18 69 0C vbroadcastss xmm5,dword ptr [rcx+0Ch] 0000000000000234: C4 E2 61 A8 E9 vfmadd213ps xmm5,xmm3,xmm1 0000000000000239: C4 E2 79 18 49 10 vbroadcastss xmm1,dword ptr [rcx+10h] 000000000000023F: C4 E2 69 A8 C8 vfmadd213ps xmm1,xmm2,xmm0 0000000000000244: C4 E2 79 18 41 14 vbroadcastss xmm0,dword ptr [rcx+14h] 000000000000024A: C4 E2 61 A8 C1 vfmadd213ps xmm0,xmm3,xmm1 000000000000024F: C4 C1 58 58 09 vaddps xmm1,xmm4,xmmword ptr [r9] 0000000000000254: C4 C1 50 58 51 10 vaddps xmm2,xmm5,xmmword ptr [r9+10h] 000000000000025A: C4 C1 78 58 41 20 vaddps xmm0,xmm0,xmmword ptr [r9+20h]","title":"Convenience syntax: kernelize"},{"location":"Manual/07%20Plans%20-%20Vectorization%20and%20Parallelization/#parallelize","text":"The parallelize instruction performs one or more loops in parallel on multiple cores. xeonPlat = acc . Target ( \"Intel 9221\" , num_threads = 16 ) plan = schedule . create_plan ( xeonPlat ) plan . parallelize ( indices = ( i , j , k )) Specifying multiple dimensions is equivalent to the collapse argument in OpenMP. Therefore, the dimensions must be contiguous in the iteration space dimension order.","title":"parallelize"},{"location":"Manual/07%20Plans%20-%20Vectorization%20and%20Parallelization/#static-scheduling-policy","text":"A static scheduling strategy is invoked by setting the argument policy=\"static\" in the call to parallelize . If n iterations are parallelized across c cores, static scheduling partitions the work into c fixed parts, some of size floor(n/c) and some of size ceil(n/c) , and executes each part on a different core.","title":"Static scheduling policy"},{"location":"Manual/07%20Plans%20-%20Vectorization%20and%20Parallelization/#dynamic-scheduling-policy","text":"A dynamic scheduling strategy is invoked by setting the argument policy=\"dynamic\" in the call to parallelize . Dynamic scheduling creates a single queue of work that is shared across the different cores.","title":"Dynamic scheduling policy"},{"location":"Manual/07%20Plans%20-%20Vectorization%20and%20Parallelization/#not-yet-implemented-pinning-to-specific-cores","text":"The pin argument allows the parallel work to be pinned to specific cores.","title":"Not yet implemented: Pinning to specific cores"},{"location":"Manual/07%20Plans%20-%20Vectorization%20and%20Parallelization/#bind","text":"Some target platforms, such as GPUs, are specifically designed to execute nested loops. They can take an entire grid of work and schedule its execution on multiple cores. On a GPU, this grid is broken up into multiple blocks, where each block contains multiple threads. Block iterators and thread iterators are identified by special variables in the Target object. To take advantage of a target platform's ability to execute grids, we must bind dimensions of the iteration space with these special iterator variables. For example, v100 = acc . Target ( \"Tesla V100\" ) plan . bind ( indices = ( i , j , k ), grid = ( v100 . GridUnit . BLOCK_X , v100 . GridUnit . THREAD_X , v100 . GridUnit . THREAD_Y ))","title":"bind"},{"location":"Manual/08%20Deferred%20Layout%20of%20Constant%20Arrays/","text":"Section 8: Deferred layout of constant arrays We revisit the topic of memory layout of constant arrays. As mentioned in Section 1 , the contents of constant arrays are known at compile time, they are immutable, and they are not externally visible. This allows Accera to store them using a non-standard layout, optimized for a specific plan. In some cases, there may even be a benefit to storing multiple copies of each array element (e.g., storing a matrix in both row-major and column-major layouts). Deferred layout based on a cache An Accera cache strategy makes local copies of an array's active blocks. The constant array can be laid out based on a defined cache. Namely, the array is stored by serializing its active blocks one-after-the-other. If the caching strategy is thrifty=True , no data needs to be copied at runtime and the active blocks are ready to use. To define an array layout based on a cache, note that the Accera DSL has to overcome a chicken-and-egg situation: on one hand, arrays need to be defined upfront, even before the nest logic; on the other hand, the array layout depends on a cache, which is only defined as part of the plan. We overcome this problem by splitting the array definition into two parts. We still define the constant array upfront, but avoid committing to a specific layout: import accera as acc import numpy as np matrix = np . random . rand ( 16 , 16 ) A = acc . Array ( role = acc . Array . Role . CONST , data = matrix , layout = acc . Array . Layout . DEFERRED ) Then we proceed to define the nest logic, the schedule, and the plan. Imagine that we define a plan named plan and use that plan to define a cache for A based on dimension i : AA = plan . cache ( A , i , layout = acc . Array . Layout . FIRST_MAJOR , thrifty = True ) We can now use the cache AA to determine the layout of the original array A : A . deferred_layout ( cache = AA )","title":"Deferred Layout of Constant Arrays"},{"location":"Manual/08%20Deferred%20Layout%20of%20Constant%20Arrays/#section-8-deferred-layout-of-constant-arrays","text":"We revisit the topic of memory layout of constant arrays. As mentioned in Section 1 , the contents of constant arrays are known at compile time, they are immutable, and they are not externally visible. This allows Accera to store them using a non-standard layout, optimized for a specific plan. In some cases, there may even be a benefit to storing multiple copies of each array element (e.g., storing a matrix in both row-major and column-major layouts).","title":"Section 8: Deferred layout of constant arrays"},{"location":"Manual/08%20Deferred%20Layout%20of%20Constant%20Arrays/#deferred-layout-based-on-a-cache","text":"An Accera cache strategy makes local copies of an array's active blocks. The constant array can be laid out based on a defined cache. Namely, the array is stored by serializing its active blocks one-after-the-other. If the caching strategy is thrifty=True , no data needs to be copied at runtime and the active blocks are ready to use. To define an array layout based on a cache, note that the Accera DSL has to overcome a chicken-and-egg situation: on one hand, arrays need to be defined upfront, even before the nest logic; on the other hand, the array layout depends on a cache, which is only defined as part of the plan. We overcome this problem by splitting the array definition into two parts. We still define the constant array upfront, but avoid committing to a specific layout: import accera as acc import numpy as np matrix = np . random . rand ( 16 , 16 ) A = acc . Array ( role = acc . Array . Role . CONST , data = matrix , layout = acc . Array . Layout . DEFERRED ) Then we proceed to define the nest logic, the schedule, and the plan. Imagine that we define a plan named plan and use that plan to define a cache for A based on dimension i : AA = plan . cache ( A , i , layout = acc . Array . Layout . FIRST_MAJOR , thrifty = True ) We can now use the cache AA to determine the layout of the original array A : A . deferred_layout ( cache = AA )","title":"Deferred layout based on a cache"},{"location":"Manual/09%20Parameters/","text":"Section 9: Parameters Accera parameters are placeholders that get replaced with concrete values when we add a function to a package. A parameter can be used in a Nest , a Schedule , or an Plan . Parameterized nests Recall that a Nest represents the loop-nest logic. We can parameterize the nest shape and the iteration logic. For example, consider the following parameterized version of matrix multiplication: # Create parameters P0 , P1 , P2 , P3 = acc . create_parameters ( 4 ) A = acc . Array ( role = acc . Array . Role . INPUT , shape = ( P0 , P2 )) B = acc . Array ( role = acc . Array . Role . INPUT , shape = ( P2 , P1 )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , shape = ( P0 , P1 )) # Define a simple nest nest = acc . Nest ( shape = ( P0 , P1 , P2 )) i , j , k = nest . get_indices () # Define the loop nest logic and add it to the nest @nest . iteration_logic def _ (): C [ i , j ] += P3 * A [ i , k ] * B [ k , j ] # create a package package = acc . Package () # Use the templated nest to add two different functions to the package package . add ( nest , args = ( A , B , C ), parameters = { P0 : 16 , P1 : 16 , P2 : 16 , P3 : 1.0 }, base_name = \"matmul_16_16_16_1\" ) package . add ( nest , args = ( A , B , C ), parameters = { P0 : 32 , P1 : 32 , P2 : 32 , P3 : 2.0 }, base_name = \"matmul_32_32_32_2\" ) In this example, the shape of the nest is parameterized by ( P0 , P1 , P2 ) and its iteration logic includes the parameter P3 . The nest is used twice, with different settings of these parameters, to create two separate functions in the package. Parameterized schedules and plans Parameters can also appear in schedules and plans. For example, we could add the following to the code above: P4 , P5 = acc . create_parameters ( 2 ) # Create a parameterized schedule schedule = nest . create_schedule () ii = schedule . split ( i , size = P4 ) # Create a parameterized plan plan = schedule . create_plan () plan . cache ( A , level = P5 ) # Add another function to the package package . add ( plan , args = ( A , B , C ), parameters = { P0 : 16 , P1 : 16 , P2 : 16 , P3 : 1.0 , P4 : 4 , P5 : 2 }, base_name = \"alternative_matmul_16_16_16\" ) Tuple parameter values Parameters can be used as placeholders for tuples, and specifically tuples of indices. For example, say that we wanted to parameterize the order of the iteration space dimensions. We could write: P6 = acc . create_parameters ( 1 ) schedule . reorder ( order = P6 ) Later, we could set the value of P6 to the index tuple (j,k,i) . Get parameters from an entire parameter grid Consider the parameterized nest defined above. Rather than setting each parameter to one specific value, imagine that we had a set of different values for each parameter. For example, say that we wanted P0 to take a value in the set {8, 16} , P1 in {16, 32} , P2 would always equal 16, and P3 would take a value in {1,2} . This list of all valid parameter combinations is called a parameter grid . In this case, the grid includes the following parameter settings: { P0 : 8 , P1 : 16 , P2 : 16 , P3 : 1.0 } { P0 : 8 , P1 : 16 , P2 : 16 , P3 : 2.0 } { P0 : 8 , P1 : 32 , P2 : 16 , P3 : 1.0 } { P0 : 8 , P1 : 32 , P2 : 16 , P3 : 2.0 } { P0 : 16 , P1 : 16 , P2 : 16 , P3 : 1.0 } { P0 : 16 , P1 : 16 , P2 : 16 , P3 : 2.0 } { P0 : 16 , P1 : 32 , P2 : 16 , P3 : 1.0 } { P0 : 16 , P1 : 32 , P2 : 16 , P3 : 2.0 } Accera provides an easy way to add all of the functions that correspond to the parameter grid at once. parameters = get_parameters_from_grid ( parameter_grid = { P0 :[ 8 , 16 ], P1 :[ 16 , 32 ], P2 :[ 16 ], P3 :[ 1.0 , 2.0 ]}) package . add ( nest , args = ( A , B , C ), base_name = \"matmul\" , parameters ) In this case, package.add generates a function eight times, once for each parameter combination in the grid. Instead of nest , this function could take a schedule or a plan. All eight functions share the same base name, and Accera automatically adds a unique suffix to each function name to prevent duplicates. This pattern allows you to optionally perform filtering by inspecting the list of generated parameter values before calling package.add .","title":"Parameters"},{"location":"Manual/09%20Parameters/#section-9-parameters","text":"Accera parameters are placeholders that get replaced with concrete values when we add a function to a package. A parameter can be used in a Nest , a Schedule , or an Plan .","title":"Section 9: Parameters"},{"location":"Manual/09%20Parameters/#parameterized-nests","text":"Recall that a Nest represents the loop-nest logic. We can parameterize the nest shape and the iteration logic. For example, consider the following parameterized version of matrix multiplication: # Create parameters P0 , P1 , P2 , P3 = acc . create_parameters ( 4 ) A = acc . Array ( role = acc . Array . Role . INPUT , shape = ( P0 , P2 )) B = acc . Array ( role = acc . Array . Role . INPUT , shape = ( P2 , P1 )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , shape = ( P0 , P1 )) # Define a simple nest nest = acc . Nest ( shape = ( P0 , P1 , P2 )) i , j , k = nest . get_indices () # Define the loop nest logic and add it to the nest @nest . iteration_logic def _ (): C [ i , j ] += P3 * A [ i , k ] * B [ k , j ] # create a package package = acc . Package () # Use the templated nest to add two different functions to the package package . add ( nest , args = ( A , B , C ), parameters = { P0 : 16 , P1 : 16 , P2 : 16 , P3 : 1.0 }, base_name = \"matmul_16_16_16_1\" ) package . add ( nest , args = ( A , B , C ), parameters = { P0 : 32 , P1 : 32 , P2 : 32 , P3 : 2.0 }, base_name = \"matmul_32_32_32_2\" ) In this example, the shape of the nest is parameterized by ( P0 , P1 , P2 ) and its iteration logic includes the parameter P3 . The nest is used twice, with different settings of these parameters, to create two separate functions in the package.","title":"Parameterized nests"},{"location":"Manual/09%20Parameters/#parameterized-schedules-and-plans","text":"Parameters can also appear in schedules and plans. For example, we could add the following to the code above: P4 , P5 = acc . create_parameters ( 2 ) # Create a parameterized schedule schedule = nest . create_schedule () ii = schedule . split ( i , size = P4 ) # Create a parameterized plan plan = schedule . create_plan () plan . cache ( A , level = P5 ) # Add another function to the package package . add ( plan , args = ( A , B , C ), parameters = { P0 : 16 , P1 : 16 , P2 : 16 , P3 : 1.0 , P4 : 4 , P5 : 2 }, base_name = \"alternative_matmul_16_16_16\" )","title":"Parameterized schedules and plans"},{"location":"Manual/09%20Parameters/#tuple-parameter-values","text":"Parameters can be used as placeholders for tuples, and specifically tuples of indices. For example, say that we wanted to parameterize the order of the iteration space dimensions. We could write: P6 = acc . create_parameters ( 1 ) schedule . reorder ( order = P6 ) Later, we could set the value of P6 to the index tuple (j,k,i) .","title":"Tuple parameter values"},{"location":"Manual/09%20Parameters/#get-parameters-from-an-entire-parameter-grid","text":"Consider the parameterized nest defined above. Rather than setting each parameter to one specific value, imagine that we had a set of different values for each parameter. For example, say that we wanted P0 to take a value in the set {8, 16} , P1 in {16, 32} , P2 would always equal 16, and P3 would take a value in {1,2} . This list of all valid parameter combinations is called a parameter grid . In this case, the grid includes the following parameter settings: { P0 : 8 , P1 : 16 , P2 : 16 , P3 : 1.0 } { P0 : 8 , P1 : 16 , P2 : 16 , P3 : 2.0 } { P0 : 8 , P1 : 32 , P2 : 16 , P3 : 1.0 } { P0 : 8 , P1 : 32 , P2 : 16 , P3 : 2.0 } { P0 : 16 , P1 : 16 , P2 : 16 , P3 : 1.0 } { P0 : 16 , P1 : 16 , P2 : 16 , P3 : 2.0 } { P0 : 16 , P1 : 32 , P2 : 16 , P3 : 1.0 } { P0 : 16 , P1 : 32 , P2 : 16 , P3 : 2.0 } Accera provides an easy way to add all of the functions that correspond to the parameter grid at once. parameters = get_parameters_from_grid ( parameter_grid = { P0 :[ 8 , 16 ], P1 :[ 16 , 32 ], P2 :[ 16 ], P3 :[ 1.0 , 2.0 ]}) package . add ( nest , args = ( A , B , C ), base_name = \"matmul\" , parameters ) In this case, package.add generates a function eight times, once for each parameter combination in the grid. Instead of nest , this function could take a schedule or a plan. All eight functions share the same base name, and Accera automatically adds a unique suffix to each function name to prevent duplicates. This pattern allows you to optionally perform filtering by inspecting the list of generated parameter values before calling package.add .","title":"Get parameters from an entire parameter grid"},{"location":"Manual/10%20Packages/","text":"Section 10: Building Packages The Accera Package class represents a collection of Accera-generated functions. When a package is built, it creates a stand-alone function library that can be used by other pieces of software. Accera currently supports two package formats: HAT and MLIR. HAT package format HAT is a format for packaging compiled libraries in the C programming language. HAT stands for \"Header Annotated with TOML\", which implies that a standard C header is decorated with useful metadata in the TOML markup language. Say that nest contains some loop-nest logic. To build a HAT package that contains a function with this logic for the Windows operating system, we do the following: package = acc . Package () package . add ( nest , base_name = \"func1\" ) package . build ( format = acc . Package . Format . HAT_DYNAMIC , name = \"myPackage\" , platform = acc . Package . Platform . WINDOWS ) The result is two files: MyPackage.hat and MyPackage.lib . The output directory defaults to the current working directory. To change the output directory: package . build ( format = acc . Package . Format . HAT_DYNAMIC , name = \"myPackage\" , platform = acc . Package . Platform . WINDOWS , output_dir = \"hat_packages\" ) MLIR package format MLIR format is mainly used for debugging purposes, and as a way to follow the multiple lowering MLIR stages, from Accera DSL all the way to runnable code. package . build ( format = acc . Package . Format . MLIR , name = \"myPackage\" ) Function names in packages When a function is added to a package, the user specifies its base name. The full function name is the base name followed by an automatically generated unique identifier. For example, if the base name is \"myFunc\" then the function name could be \"myFunc_8f24bef5\". If no base name is given, the function name is just the automatically-generated unique identifier. The unique identifier ensures that no two functions share the same name, but also makes it harder to call the function from client code. Specifically, each time that the Accera package is updated and rebuilt, the function name could change. Therefore, the HAT file also includes the client code to call the function without the unique identifier. Concretely, if the function signature in C is void myFunc_8f24bef5(const float* A, float* B); then the HAT file also contains the line: void (*myFunc)(const float* A, float* B) = myFunc_8f24bef5; The above basically makes the abbreviated name myFunc a synonym of the full function name myFunc_8f24bef5 . If multiple functions share the same base name, an arbitrary one of them gets the abbreviation. Debug mode A package can be built with the option mode=acc.Package.Mode.DEBUG . This creates a special version of each function that checks its own correctness each time the function is called. From the outside, a debugging package looks identical to a standard package. However, each of its functions actually contains two different implementations: the RoboCode implementation (with all of the fancy scheduling and planning) and the trivial default implementation (without any of the scheduling or planning). When called, the function runs both implementations and asserts that their outputs are within some predefined tolerance. If the outputs don't match, the function prints error messages to stderr . package . build ( format = acc . Package . Format . HAT_DYNAMIC , name = \"myPackage\" , mode = acc . Package . Mode . DEBUG , tolerance = 1.0e-6 ) Adding descriptions Accera allows us to specify some standard descriptive fields in a package: package . add_description ( version \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b = \"1.0\" , license = \"https://mit-license.org/\" , author = \"Microsoft Research\" ) \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b Additionally, we can add arbitrary metadata to the package description as follows: package . add_description ( other = { \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b \"title\" : \"My Package Title\" , \"source\" : \"https://github.com/\" , \"citations\" : [ \"https://arxiv.org/2021.12345/\" , \"https://arxiv.org/2021.56789/\" ]} \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b )","title":"Packages"},{"location":"Manual/10%20Packages/#section-10-building-packages","text":"The Accera Package class represents a collection of Accera-generated functions. When a package is built, it creates a stand-alone function library that can be used by other pieces of software. Accera currently supports two package formats: HAT and MLIR.","title":"Section 10: Building Packages"},{"location":"Manual/10%20Packages/#hat-package-format","text":"HAT is a format for packaging compiled libraries in the C programming language. HAT stands for \"Header Annotated with TOML\", which implies that a standard C header is decorated with useful metadata in the TOML markup language. Say that nest contains some loop-nest logic. To build a HAT package that contains a function with this logic for the Windows operating system, we do the following: package = acc . Package () package . add ( nest , base_name = \"func1\" ) package . build ( format = acc . Package . Format . HAT_DYNAMIC , name = \"myPackage\" , platform = acc . Package . Platform . WINDOWS ) The result is two files: MyPackage.hat and MyPackage.lib . The output directory defaults to the current working directory. To change the output directory: package . build ( format = acc . Package . Format . HAT_DYNAMIC , name = \"myPackage\" , platform = acc . Package . Platform . WINDOWS , output_dir = \"hat_packages\" )","title":"HAT package format"},{"location":"Manual/10%20Packages/#mlir-package-format","text":"MLIR format is mainly used for debugging purposes, and as a way to follow the multiple lowering MLIR stages, from Accera DSL all the way to runnable code. package . build ( format = acc . Package . Format . MLIR , name = \"myPackage\" )","title":"MLIR package format"},{"location":"Manual/10%20Packages/#function-names-in-packages","text":"When a function is added to a package, the user specifies its base name. The full function name is the base name followed by an automatically generated unique identifier. For example, if the base name is \"myFunc\" then the function name could be \"myFunc_8f24bef5\". If no base name is given, the function name is just the automatically-generated unique identifier. The unique identifier ensures that no two functions share the same name, but also makes it harder to call the function from client code. Specifically, each time that the Accera package is updated and rebuilt, the function name could change. Therefore, the HAT file also includes the client code to call the function without the unique identifier. Concretely, if the function signature in C is void myFunc_8f24bef5(const float* A, float* B); then the HAT file also contains the line: void (*myFunc)(const float* A, float* B) = myFunc_8f24bef5; The above basically makes the abbreviated name myFunc a synonym of the full function name myFunc_8f24bef5 . If multiple functions share the same base name, an arbitrary one of them gets the abbreviation.","title":"Function names in packages"},{"location":"Manual/10%20Packages/#debug-mode","text":"A package can be built with the option mode=acc.Package.Mode.DEBUG . This creates a special version of each function that checks its own correctness each time the function is called. From the outside, a debugging package looks identical to a standard package. However, each of its functions actually contains two different implementations: the RoboCode implementation (with all of the fancy scheduling and planning) and the trivial default implementation (without any of the scheduling or planning). When called, the function runs both implementations and asserts that their outputs are within some predefined tolerance. If the outputs don't match, the function prints error messages to stderr . package . build ( format = acc . Package . Format . HAT_DYNAMIC , name = \"myPackage\" , mode = acc . Package . Mode . DEBUG , tolerance = 1.0e-6 )","title":"Debug mode"},{"location":"Manual/10%20Packages/#adding-descriptions","text":"Accera allows us to specify some standard descriptive fields in a package: package . add_description ( version \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b = \"1.0\" , license = \"https://mit-license.org/\" , author = \"Microsoft Research\" ) \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b Additionally, we can add arbitrary metadata to the package description as follows: package . add_description ( other = { \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b \"title\" : \"My Package Title\" , \"source\" : \"https://github.com/\" , \"citations\" : [ \"https://arxiv.org/2021.12345/\" , \"https://arxiv.org/2021.56789/\" ]} \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b )","title":"Adding descriptions"},{"location":"Reference/accera/","text":"Accera v1.2.1 Reference Module functions accera.create_parameters (number) accera.get_parameters_from_grid (parameter_grid) accera.fuse (schedules[, partial]) Top level enumerations accera.ScalarType Classes class accera.Plan A scheduled (ordered) loop nest with target-specific implementation details. Methods cache (source[, index, layout, level, max_elements, thrifty, type]) bind (indices, grid) kernelize (unroll_indices, vectorize_indices) parallelize (indices[, pin, policy]) unroll (index) vectorize (index) class accera.Array A multidimensional array of scalar elements. Constructors Array (role[, data, element_type, layout, offset, shape]) Enumerations accera.Array.Layout accera.Array.Role Methods deferred_layout (layout) class accera.Cache A local copy of an Array block. class accera.Index An index that represents one of the loops in a Nest or one of the iteration-space dimensions of a Schedule or Plan . class accera.Nest The logic of a loop nest. Constructors Nest (shape) Methods iteration_logic (logic) create_plan ([target]) create_schedule () get_indices () class accera.Package A collection of functions that can be built and emitted for use in client code. Constructors Package () Enumerations accera.Package.Format accera.Package.Mode accera.Package.Platform Methods add_description ([author, license, other, version]) add (args, source[, base_name, parameters]) build (name[, error_path, format, mode, os, tolerance]) class accera.Parameter A placeholder that can used instead of concrete values when constructing or calling the methods of a Nest , Schedule , or Plan . class accera.Schedule A scheduled (ordered) loop nest with no target-specific implementation details. Methods create_plan ([target]) pad (index, size) reorder (indices) skew (index, reference_index) split (index, size) tile (indices, sizes) class accera.Target A target platform for the cross-compiler. Constructors Target ([architecture, cache_lines, cache_sizes, category, extensions, family, frequency_GHz, model, name, num_cores, num_threads, turbo_frequency_GHz]) Enumerations accera.Target.Architecture accera.Target.Category accera.Target.Models","title":"Accera"},{"location":"Reference/accera/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/accera/#module-functions","text":"accera.create_parameters (number) accera.get_parameters_from_grid (parameter_grid) accera.fuse (schedules[, partial])","title":"Module functions"},{"location":"Reference/accera/#top-level-enumerations","text":"accera.ScalarType","title":"Top level enumerations"},{"location":"Reference/accera/#classes","text":"","title":"Classes"},{"location":"Reference/accera/#class-acceraplan","text":"A scheduled (ordered) loop nest with target-specific implementation details.","title":"class accera.Plan"},{"location":"Reference/accera/#methods","text":"cache (source[, index, layout, level, max_elements, thrifty, type]) bind (indices, grid) kernelize (unroll_indices, vectorize_indices) parallelize (indices[, pin, policy]) unroll (index) vectorize (index)","title":"Methods"},{"location":"Reference/accera/#class-acceraarray","text":"A multidimensional array of scalar elements.","title":"class accera.Array"},{"location":"Reference/accera/#constructors","text":"Array (role[, data, element_type, layout, offset, shape])","title":"Constructors"},{"location":"Reference/accera/#enumerations","text":"accera.Array.Layout accera.Array.Role","title":"Enumerations"},{"location":"Reference/accera/#methods_1","text":"deferred_layout (layout)","title":"Methods"},{"location":"Reference/accera/#class-acceracache","text":"A local copy of an Array block.","title":"class accera.Cache"},{"location":"Reference/accera/#class-acceraindex","text":"An index that represents one of the loops in a Nest or one of the iteration-space dimensions of a Schedule or Plan .","title":"class accera.Index"},{"location":"Reference/accera/#class-acceranest","text":"The logic of a loop nest.","title":"class accera.Nest"},{"location":"Reference/accera/#constructors_1","text":"Nest (shape)","title":"Constructors"},{"location":"Reference/accera/#methods_2","text":"iteration_logic (logic) create_plan ([target]) create_schedule () get_indices ()","title":"Methods"},{"location":"Reference/accera/#class-accerapackage","text":"A collection of functions that can be built and emitted for use in client code.","title":"class accera.Package"},{"location":"Reference/accera/#constructors_2","text":"Package ()","title":"Constructors"},{"location":"Reference/accera/#enumerations_1","text":"accera.Package.Format accera.Package.Mode accera.Package.Platform","title":"Enumerations"},{"location":"Reference/accera/#methods_3","text":"add_description ([author, license, other, version]) add (args, source[, base_name, parameters]) build (name[, error_path, format, mode, os, tolerance])","title":"Methods"},{"location":"Reference/accera/#class-acceraparameter","text":"A placeholder that can used instead of concrete values when constructing or calling the methods of a Nest , Schedule , or Plan .","title":"class accera.Parameter"},{"location":"Reference/accera/#class-acceraschedule","text":"A scheduled (ordered) loop nest with no target-specific implementation details.","title":"class accera.Schedule"},{"location":"Reference/accera/#methods_4","text":"create_plan ([target]) pad (index, size) reorder (indices) skew (index, reference_index) split (index, size) tile (indices, sizes)","title":"Methods"},{"location":"Reference/accera/#class-acceratarget","text":"A target platform for the cross-compiler.","title":"class accera.Target"},{"location":"Reference/accera/#constructors_3","text":"Target ([architecture, cache_lines, cache_sizes, category, extensions, family, frequency_GHz, model, name, num_cores, num_threads, turbo_frequency_GHz])","title":"Constructors"},{"location":"Reference/accera/#enumerations_2","text":"accera.Target.Architecture accera.Target.Category accera.Target.Models","title":"Enumerations"},{"location":"Reference/safety_analysis/","text":"Safety Analysis One of the most important features of the Accera language is that it can provide safety guarantees, which make the programmer's job easier. We say that an Accera schedule is safe if its underlying logic is guaranteed not to change, regardless of how we transform it. Not all Accera schedules are safe, but those that are safe are much easier to work with. First, note that order-invariant schedules are obviously safe. This is because Accera transformations only change the order of the loop-nest iterations, never remove any iterations, and possibly add empty iterations in the form of padding. Recall that a Nest represents a simple nest, which is assumed to be order-invariant, and therefore any schedule that was created by a call to create_schedule() is safe. Safety and Fusing Another way to create a schedule is via fusing (see Section 4 of the Accera manual ). Say that we have a sequence of n schedules: schedule0 , schedule1 , ... and we partially fuse their first m dimensions. Namely, schedule = acc . fuse (( schedule0 , schedule1 , ... ), partial = m ) At this point, schedule is equivalent to executing the individual schedules one-by-one. However, is schedule safe in the sense defined above? In other words, does schedule guarantee that its underlying logic is preserved, regardless of how it is transformed? The dimensions of schedule fall into three categories: Fusing dimensions : at first, this category contains a single dimension, which is the first dimension of schedule . However, if this dimension is split, then its derived dimensions are added to the category. Fused dimensions : at first, this category contains the next m dimensions in schedule . If any of these dimensions are split, the derived dimensions are also added to the category. Unfused dimensions : includes all the remaining dimensions. Note that the individual schedules being fused may themselves be the result of a previous fusing operation. The categories noted above only relate to the role of each dimensions in the current fusing operation. A Theorem Imagine that we apply a sequence of transformations to schedule , which may derive new dimensions. Derived dimensions belong to the same category as the dimension from which they were derived. If the fusing dimension (and all dimensions derived from it) precedes all the unfused dimensions, then for any value of the fused dimensions, all the corresponding work from schedule0 is executed before any of the corresponding work from schedule1 ; all the corresponding work from schedule1 is executed before any of the corresponding work from schedule2 ; etc. Proof For simplicity, assume that there is only one fusing dimension and that its name is f . Also for simplicity, assume that we only fused two schedules, schedule0 and schedule1 . Note that these simplifying assumptions can easily be relaxed. Assume that f precedes all of the unfused dimensions. Therefore, dimensions that precede f are necessarily fused dimensions. Let U be a sequence of concrete values for all the fused dimensions and let V denote only those values that correspond to dimensions that precede f . The work from schedule0 that corresponds to the concrete values in U is contained in the slice (V, 0, *, ..., *). Similarly, the work form schedule1 that corresponds to the values in U is contained in (V, 1, *, ..., *). Finally, note that the former slice lexicographically precedes the latter. This concludes the proof. An example To make the theorem less abstract, we demonstrate how it applies to a simple example. Assume that we start with two schedules, schedule0 and schedule1 , both three-dimensional, and we fuse their first two dimensions: i0 , j0 , k0 = schedule0 . get_indices () # redundant operation, included for clarity i1 , j1 , k1 = schedule1 . get_indices () # redundant operation, included for clarity schedule = acc . fuse (( schedule0 , schedule1 ), partial = 2 ) f , i , j , k0 , k1 = schedule . get_indices () Next, say that we transform schedule by tiling dimensions j and k0 and reordering the dimensions as follows: jj , kk0 = schedule . tile (( j , k0 ), ( 4 , 4 )) schedule . reorder ( j , i , f , k0 , k1 , kk0 , jj ) Dimensions i , j , and jj are fused dimensions, while k0 , kk0 , and k1 are unfused dimensions. Note that the fusing dimension f precedes all of the unfused dimensions, so the condition of the theorem is satisfied. Next, choose concrete values for the fused dimensions, say, i=4 , j=3 , and jj=2 . The work from schedule0 that corresponds to these values is contained in the slice (3, 4, 0, , , , ), and the work from schedule1 that corresponds to these values is contained in the slice (3, 4, 1, , , , ). The former slice lexicographically precedes the latter and is therefore executed first. Safety The theorem holds for any schedule, but it does not imply that every schedule is safe. Additional effort is required to prove whether a specific schedule is safe or not. When we perform a fuse operation, we must examine the specific circumstances and consider whether the theorem provides a sufficient condition for safety.","title":"Safety Analysis"},{"location":"Reference/safety_analysis/#safety-analysis","text":"One of the most important features of the Accera language is that it can provide safety guarantees, which make the programmer's job easier. We say that an Accera schedule is safe if its underlying logic is guaranteed not to change, regardless of how we transform it. Not all Accera schedules are safe, but those that are safe are much easier to work with. First, note that order-invariant schedules are obviously safe. This is because Accera transformations only change the order of the loop-nest iterations, never remove any iterations, and possibly add empty iterations in the form of padding. Recall that a Nest represents a simple nest, which is assumed to be order-invariant, and therefore any schedule that was created by a call to create_schedule() is safe.","title":"Safety Analysis"},{"location":"Reference/safety_analysis/#safety-and-fusing","text":"Another way to create a schedule is via fusing (see Section 4 of the Accera manual ). Say that we have a sequence of n schedules: schedule0 , schedule1 , ... and we partially fuse their first m dimensions. Namely, schedule = acc . fuse (( schedule0 , schedule1 , ... ), partial = m ) At this point, schedule is equivalent to executing the individual schedules one-by-one. However, is schedule safe in the sense defined above? In other words, does schedule guarantee that its underlying logic is preserved, regardless of how it is transformed? The dimensions of schedule fall into three categories: Fusing dimensions : at first, this category contains a single dimension, which is the first dimension of schedule . However, if this dimension is split, then its derived dimensions are added to the category. Fused dimensions : at first, this category contains the next m dimensions in schedule . If any of these dimensions are split, the derived dimensions are also added to the category. Unfused dimensions : includes all the remaining dimensions. Note that the individual schedules being fused may themselves be the result of a previous fusing operation. The categories noted above only relate to the role of each dimensions in the current fusing operation.","title":"Safety and Fusing"},{"location":"Reference/safety_analysis/#a-theorem","text":"Imagine that we apply a sequence of transformations to schedule , which may derive new dimensions. Derived dimensions belong to the same category as the dimension from which they were derived. If the fusing dimension (and all dimensions derived from it) precedes all the unfused dimensions, then for any value of the fused dimensions, all the corresponding work from schedule0 is executed before any of the corresponding work from schedule1 ; all the corresponding work from schedule1 is executed before any of the corresponding work from schedule2 ; etc.","title":"A Theorem"},{"location":"Reference/safety_analysis/#proof","text":"For simplicity, assume that there is only one fusing dimension and that its name is f . Also for simplicity, assume that we only fused two schedules, schedule0 and schedule1 . Note that these simplifying assumptions can easily be relaxed. Assume that f precedes all of the unfused dimensions. Therefore, dimensions that precede f are necessarily fused dimensions. Let U be a sequence of concrete values for all the fused dimensions and let V denote only those values that correspond to dimensions that precede f . The work from schedule0 that corresponds to the concrete values in U is contained in the slice (V, 0, *, ..., *). Similarly, the work form schedule1 that corresponds to the values in U is contained in (V, 1, *, ..., *). Finally, note that the former slice lexicographically precedes the latter. This concludes the proof.","title":"Proof"},{"location":"Reference/safety_analysis/#an-example","text":"To make the theorem less abstract, we demonstrate how it applies to a simple example. Assume that we start with two schedules, schedule0 and schedule1 , both three-dimensional, and we fuse their first two dimensions: i0 , j0 , k0 = schedule0 . get_indices () # redundant operation, included for clarity i1 , j1 , k1 = schedule1 . get_indices () # redundant operation, included for clarity schedule = acc . fuse (( schedule0 , schedule1 ), partial = 2 ) f , i , j , k0 , k1 = schedule . get_indices () Next, say that we transform schedule by tiling dimensions j and k0 and reordering the dimensions as follows: jj , kk0 = schedule . tile (( j , k0 ), ( 4 , 4 )) schedule . reorder ( j , i , f , k0 , k1 , kk0 , jj ) Dimensions i , j , and jj are fused dimensions, while k0 , kk0 , and k1 are unfused dimensions. Note that the fusing dimension f precedes all of the unfused dimensions, so the condition of the theorem is satisfied. Next, choose concrete values for the fused dimensions, say, i=4 , j=3 , and jj=2 . The work from schedule0 that corresponds to these values is contained in the slice (3, 4, 0, , , , ), and the work from schedule1 that corresponds to these values is contained in the slice (3, 4, 1, , , , ). The former slice lexicographically precedes the latter and is therefore executed first.","title":"An example"},{"location":"Reference/safety_analysis/#safety","text":"The theorem holds for any schedule, but it does not imply that every schedule is safe. Additional effort is required to prove whether a specific schedule is safe or not. When we perform a fuse operation, we must examine the specific circumstances and consider whether the theorem provides a sufficient condition for safety.","title":"Safety"},{"location":"Reference/classes/Array/Array/","text":"Accera v1.2.1 Reference accera.Array(role[, data, element_type, layout, offset, shape]) Constructs an array. Arguments argument description type/default role The role of the array, which determines if the array scope is internal or external and if the array is mutable or immutable. accera.Array.Role data The contents of a constant array. Required for accera.Array,Role.CONST arrays but should not be specified for other roles. Python buffer or numpy.ndarray element_type The array element type. accera.ScalarType , default: accera.ScalarType.float32 layout The affine memory map. tuple of integers or accera.Array.Layout , default: accera.Array.Layout.FIRST_MAJOR offset The offset of the affine memory map integer (positive, zero, or negative), default: 0 shape The array shape. Required for roles other than accera.Array.Role.CONST , should not be specified for accera.Array.Role.CONST . Examples Construct an input array: import accera as acc A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( 10 , 20 )) # the default layout is acc.Array.Layout.FIRST_MAJOR Construct an input array with an explicit standard layout: A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( 10 , 20 ), layout = acc . Array . Layout . LAST_MAJOR ) Construct an input array with an explicit affine memory map: A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( 10 , 20 ), layout = ( 1 , 10 )) Construct an input array with an infinite (undefined) major dimension: A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( 10 , acc . inf ), layout = acc . Array . Layout . LAST_MAJOR ) Construct an input/output array: A = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , element_type = acc . ScalarType . float32 , shape = ( 10 , 20 )) Construct a constant array: D = np . random . rand ( 10 , 16 ) A = acc . Array ( role = acc . Array . Role . CONST , data = D ) Construct a constant array with an explicit element type and layout, which does not necessarily match the input data: D = np . random . rand ( 10 , 16 ) A = acc . Array ( role = acc . Array . Role . CONST , element_type = acc . ScalarType . float32 , layout = acc . Array . Layout . LAST_MAJOR , data = D ) Construct a temporary array: A = acc . Array ( role = acc . Array . Role . TEMP , element_type = acc . ScalarType . float32 , shape = ( 10 , 20 ), layout = acc . Array . Layout . LAST_MAJOR )","title":"Array"},{"location":"Reference/classes/Array/Array/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Array/Array/#acceraarrayrole-data-element_type-layout-offset-shape","text":"Constructs an array.","title":"accera.Array(role[, data, element_type, layout, offset, shape])"},{"location":"Reference/classes/Array/Array/#arguments","text":"argument description type/default role The role of the array, which determines if the array scope is internal or external and if the array is mutable or immutable. accera.Array.Role data The contents of a constant array. Required for accera.Array,Role.CONST arrays but should not be specified for other roles. Python buffer or numpy.ndarray element_type The array element type. accera.ScalarType , default: accera.ScalarType.float32 layout The affine memory map. tuple of integers or accera.Array.Layout , default: accera.Array.Layout.FIRST_MAJOR offset The offset of the affine memory map integer (positive, zero, or negative), default: 0 shape The array shape. Required for roles other than accera.Array.Role.CONST , should not be specified for accera.Array.Role.CONST .","title":"Arguments"},{"location":"Reference/classes/Array/Array/#examples","text":"Construct an input array: import accera as acc A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( 10 , 20 )) # the default layout is acc.Array.Layout.FIRST_MAJOR Construct an input array with an explicit standard layout: A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( 10 , 20 ), layout = acc . Array . Layout . LAST_MAJOR ) Construct an input array with an explicit affine memory map: A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( 10 , 20 ), layout = ( 1 , 10 )) Construct an input array with an infinite (undefined) major dimension: A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( 10 , acc . inf ), layout = acc . Array . Layout . LAST_MAJOR ) Construct an input/output array: A = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , element_type = acc . ScalarType . float32 , shape = ( 10 , 20 )) Construct a constant array: D = np . random . rand ( 10 , 16 ) A = acc . Array ( role = acc . Array . Role . CONST , data = D ) Construct a constant array with an explicit element type and layout, which does not necessarily match the input data: D = np . random . rand ( 10 , 16 ) A = acc . Array ( role = acc . Array . Role . CONST , element_type = acc . ScalarType . float32 , layout = acc . Array . Layout . LAST_MAJOR , data = D ) Construct a temporary array: A = acc . Array ( role = acc . Array . Role . TEMP , element_type = acc . ScalarType . float32 , shape = ( 10 , 20 ), layout = acc . Array . Layout . LAST_MAJOR )","title":"Examples"},{"location":"Reference/classes/Array/Layout/","text":"Accera v1.2.1 Reference accera.Array.Layout type description accera.Array.Layout.FIRST_MAJOR Specifies a memory layout where the first major axis is in contiguous memory. For example, in a matrix, this corresponds to \"row-major\" accera.Array.Layout.LAST_MAJOR Specifies a memory layout where the last major axis is in contiguous memory. For example, in a matrix, this corresponds to \"column-major\" accera.Array.Layout.DEFERRED Defer specifying the memory layout for a Array.Role.CONST array until a cache is created.","title":"Array.Layout"},{"location":"Reference/classes/Array/Layout/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Array/Layout/#acceraarraylayout","text":"type description accera.Array.Layout.FIRST_MAJOR Specifies a memory layout where the first major axis is in contiguous memory. For example, in a matrix, this corresponds to \"row-major\" accera.Array.Layout.LAST_MAJOR Specifies a memory layout where the last major axis is in contiguous memory. For example, in a matrix, this corresponds to \"column-major\" accera.Array.Layout.DEFERRED Defer specifying the memory layout for a Array.Role.CONST array until a cache is created.","title":"accera.Array.Layout"},{"location":"Reference/classes/Array/Role/","text":"Accera v1.2.1 Reference accera.Array.Role type description accera.Array.Role.CONST A constant array (immutable internal-scope) whose contents are known at compile-time accera.Array.Role.INPUT An input array (immutable external-scope) accera.Array.Role.INPUT_OUTPUT An input/output array (mutable external-scope) accera.Array.Role.TEMP A temporary array (mutable internal-scope)","title":"Array.Role"},{"location":"Reference/classes/Array/Role/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Array/Role/#acceraarrayrole","text":"type description accera.Array.Role.CONST A constant array (immutable internal-scope) whose contents are known at compile-time accera.Array.Role.INPUT An input array (immutable external-scope) accera.Array.Role.INPUT_OUTPUT An input/output array (mutable external-scope) accera.Array.Role.TEMP A temporary array (mutable internal-scope)","title":"accera.Array.Role"},{"location":"Reference/classes/Array/deferred_layout/","text":"Accera v1.2.1 Reference accera.Array.deferred_layout(layout) Specifies the layout for a Array.Role.CONST array based on a Cache . For more details, see Deferred layout of constant arrays Arguments argument description type/default layout The layout to set. accera.Array.Layout Examples Create a constant 16x16 array without specifying a layout. Later on, define its layout based on a cache: import numpy as np import accera as acc matrix = np . random . rand ( 16 , 16 ) # Create a constant array with a deferred layout A = acc . Array ( role = acc . Array . Role . CONST , data = matrix , layout = acc . Array . Layout . DEFERRED ) B = Array ( role = Array . Role . INPUT_OUTPUT , element_type = ScalarType . float32 , shape = matrix . shape ) nest = Nest ( shape = matrix . shape ) i , j = nest . get_indices () @nest . iteration_logic def_ (): B [ i , j ] += A [ i , j ] plan = nest . create_plan () # create a cache for the constant array AA = plan . cache ( A , i , layout = acc . Array . Layout . FIRST_MAJOR , thrifty = True ) # update the constant array's layout based on the cache A . deferred_layout ( cache = AA )","title":"Array.deferred_layout"},{"location":"Reference/classes/Array/deferred_layout/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Array/deferred_layout/#acceraarraydeferred_layoutlayout","text":"Specifies the layout for a Array.Role.CONST array based on a Cache . For more details, see Deferred layout of constant arrays","title":"accera.Array.deferred_layout(layout)"},{"location":"Reference/classes/Array/deferred_layout/#arguments","text":"argument description type/default layout The layout to set. accera.Array.Layout","title":"Arguments"},{"location":"Reference/classes/Array/deferred_layout/#examples","text":"Create a constant 16x16 array without specifying a layout. Later on, define its layout based on a cache: import numpy as np import accera as acc matrix = np . random . rand ( 16 , 16 ) # Create a constant array with a deferred layout A = acc . Array ( role = acc . Array . Role . CONST , data = matrix , layout = acc . Array . Layout . DEFERRED ) B = Array ( role = Array . Role . INPUT_OUTPUT , element_type = ScalarType . float32 , shape = matrix . shape ) nest = Nest ( shape = matrix . shape ) i , j = nest . get_indices () @nest . iteration_logic def_ (): B [ i , j ] += A [ i , j ] plan = nest . create_plan () # create a cache for the constant array AA = plan . cache ( A , i , layout = acc . Array . Layout . FIRST_MAJOR , thrifty = True ) # update the constant array's layout based on the cache A . deferred_layout ( cache = AA )","title":"Examples"},{"location":"Reference/classes/Nest/Nest/","text":"Accera v1.2.1 Reference accera.Nest(shape) Creates an affine loop nest. Arguments argument description type/default shape The shape of the iteration space tuple of positive integers Examples Create a nest with 3 nested for-loops of sizes 16, 10, and 11: nest = acc . Nest ( shape = ( 16 , 10 , 11 ))","title":"Nest"},{"location":"Reference/classes/Nest/Nest/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Nest/Nest/#acceranestshape","text":"Creates an affine loop nest.","title":"accera.Nest(shape)"},{"location":"Reference/classes/Nest/Nest/#arguments","text":"argument description type/default shape The shape of the iteration space tuple of positive integers","title":"Arguments"},{"location":"Reference/classes/Nest/Nest/#examples","text":"Create a nest with 3 nested for-loops of sizes 16, 10, and 11: nest = acc . Nest ( shape = ( 16 , 10 , 11 ))","title":"Examples"},{"location":"Reference/classes/Nest/create_plan/","text":"Accera v1.2.1 Reference accera.Nest.create_plan([target]) Create a plan using the default schedule for the nest. Arguments argument description type/default target The target platform. Defaults to acc.Target.HOST Target Returns Plan Examples Create a plan for the host computer, using the default schedule for a nest: plan = nest . create_plan () Create a plan for an Intel Core 7th Generation, using the default schedule for a nest: corei9 = acc . Target ( \"Intel 7900X\" , num_threads = 44 ) plan = nest . create_plan ( corei9 )","title":"Nest.create_plan"},{"location":"Reference/classes/Nest/create_plan/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Nest/create_plan/#acceranestcreate_plantarget","text":"Create a plan using the default schedule for the nest.","title":"accera.Nest.create_plan([target])"},{"location":"Reference/classes/Nest/create_plan/#arguments","text":"argument description type/default target The target platform. Defaults to acc.Target.HOST Target","title":"Arguments"},{"location":"Reference/classes/Nest/create_plan/#returns","text":"Plan","title":"Returns"},{"location":"Reference/classes/Nest/create_plan/#examples","text":"Create a plan for the host computer, using the default schedule for a nest: plan = nest . create_plan () Create a plan for an Intel Core 7th Generation, using the default schedule for a nest: corei9 = acc . Target ( \"Intel 7900X\" , num_threads = 44 ) plan = nest . create_plan ( corei9 )","title":"Examples"},{"location":"Reference/classes/Nest/create_schedule/","text":"Accera v1.2.1 Reference accera.Nest.create_schedule() Create a default schedule for a nest. Returns Schedule Examples schedule = nest . create_schedule ()","title":"Nest.create_schedule"},{"location":"Reference/classes/Nest/create_schedule/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Nest/create_schedule/#acceranestcreate_schedule","text":"Create a default schedule for a nest.","title":"accera.Nest.create_schedule()"},{"location":"Reference/classes/Nest/create_schedule/#returns","text":"Schedule","title":"Returns"},{"location":"Reference/classes/Nest/create_schedule/#examples","text":"schedule = nest . create_schedule ()","title":"Examples"},{"location":"Reference/classes/Nest/get_indices/","text":"Accera v1.2.1 Reference accera.Nest.get_indices() Gets the iteration space dimensions for a nest. Returns Tuple of Index Examples Get the iteration space dimensions for a 3-dimensional nest: i , j , k = nest . get_indices ()","title":"Nest.get_indices"},{"location":"Reference/classes/Nest/get_indices/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Nest/get_indices/#acceranestget_indices","text":"Gets the iteration space dimensions for a nest.","title":"accera.Nest.get_indices()"},{"location":"Reference/classes/Nest/get_indices/#returns","text":"Tuple of Index","title":"Returns"},{"location":"Reference/classes/Nest/get_indices/#examples","text":"Get the iteration space dimensions for a 3-dimensional nest: i , j , k = nest . get_indices ()","title":"Examples"},{"location":"Reference/classes/Nest/iteration_logic/","text":"Accera v1.2.1 Reference accera.Nest.iteration_logic(logic) Adds an iteration logic function to a Nest . Arguments argument description type/default logic Python function that represents the logic to run in the innermost loop of the nest Examples The preferred syntax uses Python decorators, as follows: import accera as acc A = acc . Array ( role = acc . role . INPUT , shape = ( 16 , 64 )) B = acc . Array ( role = acc . role . INPUT , shape = ( 64 , 32 )) C = acc . Array ( role = acc . role . INPUT_OUTPUT , shape = ( 16 , 32 )) nest = acc . Nest ( shape = ( 16 , 32 , 64 )) i , j , k = nest . get_indices () @nest . iteration_logic def _ (): C [ i , j ] += A [ i , k ] * B [ k , j ] The alternative syntax avoids decorators and instead defines the logic in a function: import accera as acc A = acc . Array ( role = acc . role . INPUT , shape = ( 16 , 64 )) B = acc . Array ( role = acc . role . INPUT , shape = ( 64 , 32 )) C = acc . Array ( role = acc . role . INPUT_OUTPUT , shape = ( 16 , 32 )) nest = acc . Nest ( shape = ( 16 , 32 , 64 )) i , j , k = nest . get_indices () def logic_fn (): C [ i , j ] += A [ i , k ] * B [ k , j ] nest . iteration_logic ( logic_fn )","title":"Nest.iteration_logic"},{"location":"Reference/classes/Nest/iteration_logic/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Nest/iteration_logic/#acceranestiteration_logiclogic","text":"Adds an iteration logic function to a Nest .","title":"accera.Nest.iteration_logic(logic)"},{"location":"Reference/classes/Nest/iteration_logic/#arguments","text":"argument description type/default logic Python function that represents the logic to run in the innermost loop of the nest","title":"Arguments"},{"location":"Reference/classes/Nest/iteration_logic/#examples","text":"The preferred syntax uses Python decorators, as follows: import accera as acc A = acc . Array ( role = acc . role . INPUT , shape = ( 16 , 64 )) B = acc . Array ( role = acc . role . INPUT , shape = ( 64 , 32 )) C = acc . Array ( role = acc . role . INPUT_OUTPUT , shape = ( 16 , 32 )) nest = acc . Nest ( shape = ( 16 , 32 , 64 )) i , j , k = nest . get_indices () @nest . iteration_logic def _ (): C [ i , j ] += A [ i , k ] * B [ k , j ] The alternative syntax avoids decorators and instead defines the logic in a function: import accera as acc A = acc . Array ( role = acc . role . INPUT , shape = ( 16 , 64 )) B = acc . Array ( role = acc . role . INPUT , shape = ( 64 , 32 )) C = acc . Array ( role = acc . role . INPUT_OUTPUT , shape = ( 16 , 32 )) nest = acc . Nest ( shape = ( 16 , 32 , 64 )) i , j , k = nest . get_indices () def logic_fn (): C [ i , j ] += A [ i , k ] * B [ k , j ] nest . iteration_logic ( logic_fn )","title":"Examples"},{"location":"Reference/classes/Package/Format/","text":"Accera v1.2.1 Reference accera.Package.Format type description accera.Package.Format.HAT_DYNAMIC HAT package format, dynamically linked accera.Package.Format.HAT_STATIC HAT package format, statically linked accera.Package.Format.MLIR_DYNAMIC MLIR (debugging) package format, dynamically linked accera.Package.Format.MLIR_STATIC MLIR (debugging) package format, statically linked When cross-compiling, use either accera.Package.Format.HAT_STATIC or accera.Package.Format.MLIR_STATIC .","title":"Package.Format"},{"location":"Reference/classes/Package/Format/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Package/Format/#accerapackageformat","text":"type description accera.Package.Format.HAT_DYNAMIC HAT package format, dynamically linked accera.Package.Format.HAT_STATIC HAT package format, statically linked accera.Package.Format.MLIR_DYNAMIC MLIR (debugging) package format, dynamically linked accera.Package.Format.MLIR_STATIC MLIR (debugging) package format, statically linked When cross-compiling, use either accera.Package.Format.HAT_STATIC or accera.Package.Format.MLIR_STATIC .","title":"accera.Package.Format"},{"location":"Reference/classes/Package/Mode/","text":"Accera v1.2.1 Reference accera.Package.Mode type description accera.Package.Mode.DEBUG Debug mode (automatically tests logical equivalence) accera.Package.Mode.RELEASE Release (maximally optimized)","title":"Package.Mode"},{"location":"Reference/classes/Package/Mode/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Package/Mode/#accerapackagemode","text":"type description accera.Package.Mode.DEBUG Debug mode (automatically tests logical equivalence) accera.Package.Mode.RELEASE Release (maximally optimized)","title":"accera.Package.Mode"},{"location":"Reference/classes/Package/Package/","text":"Accera v1.2.1 Reference accera.Package.Package() A package of functions that can be built and linked with client code. Examples Create a package: package = acc . Package ()","title":"Package"},{"location":"Reference/classes/Package/Package/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Package/Package/#accerapackagepackage","text":"A package of functions that can be built and linked with client code.","title":"accera.Package.Package()"},{"location":"Reference/classes/Package/Package/#examples","text":"Create a package: package = acc . Package ()","title":"Examples"},{"location":"Reference/classes/Package/Platform/","text":"Accera v1.2.1 Reference accera.Package.Platform type description accera.Package.Platform.HOST The host computer's platform accera.Package.Platform.WINDOWS The Windows platform accera.Package.Platform.LINUX The Linux platform accera.Package.Platform.MACOS The MacOS platform accera.Package.Platform.ANDRIOD The Android platform accera.Package.Platform.IOS The iOS platform accera.Package.Platform.RASPBIAN The Raspbian platform","title":"Package.Platform"},{"location":"Reference/classes/Package/Platform/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Package/Platform/#accerapackageplatform","text":"type description accera.Package.Platform.HOST The host computer's platform accera.Package.Platform.WINDOWS The Windows platform accera.Package.Platform.LINUX The Linux platform accera.Package.Platform.MACOS The MacOS platform accera.Package.Platform.ANDRIOD The Android platform accera.Package.Platform.IOS The iOS platform accera.Package.Platform.RASPBIAN The Raspbian platform","title":"accera.Package.Platform"},{"location":"Reference/classes/Package/add/","text":"Accera v1.2.1 Reference accera.Package.add(source, args[, base_name, parameters]) Adds one or more functions to the package. Arguments argument description type source The source which defines the function's implementation. Nest or Schedule or Plan args The order of external-scope arrays to use in the function signature. tuple of Array base_name A base name for the function. The full name for the function will be the base name followed by an automatically-generated unique identifier. string parameters A value for each parameter if the function's implementation is parameterized. See Parameters . A list of dictionaries can also be provided, in which case, multiple functions are generated. Parameter to value dictionary or a list of Parameter to value dictionaries. Examples Adding a function defined by an Plan : package . add ( plan , args = ( A , B , C ), base_name = \"simple_matmul\" ) Convenience syntax to add a function defined by a Schedule . A default Plan will be created automatically: package . add ( schedule , args = ( A , B , C ), base_name = \"simple_matmul\" ) Convenience syntax to add a function defined by a Nest . A default Schedule and Plan will be created internally: package . add ( nest , args = ( A , B , C ), base_name = \"simple_matmul\" ) Adding a function with concrete values specified for its parameters ( P0 , P1 , P2 , P3 ). package . add ( nest , args = ( A , B , C ), parameters = { P0 : 16 , P1 : 16 , P2 : 16 , P3 : 1 }, base_name = \"matmul_16_16_16_1\" )","title":"Package.add"},{"location":"Reference/classes/Package/add/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Package/add/#accerapackageaddsource-args-base_name-parameters","text":"Adds one or more functions to the package.","title":"accera.Package.add(source, args[, base_name, parameters])"},{"location":"Reference/classes/Package/add/#arguments","text":"argument description type source The source which defines the function's implementation. Nest or Schedule or Plan args The order of external-scope arrays to use in the function signature. tuple of Array base_name A base name for the function. The full name for the function will be the base name followed by an automatically-generated unique identifier. string parameters A value for each parameter if the function's implementation is parameterized. See Parameters . A list of dictionaries can also be provided, in which case, multiple functions are generated. Parameter to value dictionary or a list of Parameter to value dictionaries.","title":"Arguments"},{"location":"Reference/classes/Package/add/#examples","text":"Adding a function defined by an Plan : package . add ( plan , args = ( A , B , C ), base_name = \"simple_matmul\" ) Convenience syntax to add a function defined by a Schedule . A default Plan will be created automatically: package . add ( schedule , args = ( A , B , C ), base_name = \"simple_matmul\" ) Convenience syntax to add a function defined by a Nest . A default Schedule and Plan will be created internally: package . add ( nest , args = ( A , B , C ), base_name = \"simple_matmul\" ) Adding a function with concrete values specified for its parameters ( P0 , P1 , P2 , P3 ). package . add ( nest , args = ( A , B , C ), parameters = { P0 : 16 , P1 : 16 , P2 : 16 , P3 : 1 }, base_name = \"matmul_16_16_16_1\" )","title":"Examples"},{"location":"Reference/classes/Package/add_description/","text":"Accera v1.2.1 Reference accera.Package.add_description([author, license, other, version]) Adds descriptive metadata to the HAT package. Arguments argument description type/default author Name of the individual or group that authored the package. string license The internet URL of the license used to release the package. string other User-specific descriptive metadata dictionary version The package version. string Examples Adds the standard version, license, and author description fields to the package: package . add_description ( version \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b = \"1.0\" , license = \"https://mit-license.org/\" , author = \"Microsoft Research\" ) \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b Adds arbitrary user-defined metadata to describe the package: package . add_description ( other = { \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b \"title\" : \"My Package Title\" , \"source\" : \"https://github.com/\" , \"citations\" : [ \"https://arxiv.org/2021.12345/\" , \"https://arxiv.org/2021.56789/\" ]} \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b )","title":"Package.add_description"},{"location":"Reference/classes/Package/add_description/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Package/add_description/#accerapackageadd_descriptionauthor-license-other-version","text":"Adds descriptive metadata to the HAT package.","title":"accera.Package.add_description([author, license, other, version])"},{"location":"Reference/classes/Package/add_description/#arguments","text":"argument description type/default author Name of the individual or group that authored the package. string license The internet URL of the license used to release the package. string other User-specific descriptive metadata dictionary version The package version. string","title":"Arguments"},{"location":"Reference/classes/Package/add_description/#examples","text":"Adds the standard version, license, and author description fields to the package: package . add_description ( version \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b = \"1.0\" , license = \"https://mit-license.org/\" , author = \"Microsoft Research\" ) \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b Adds arbitrary user-defined metadata to describe the package: package . add_description ( other = { \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b \"title\" : \"My Package Title\" , \"source\" : \"https://github.com/\" , \"citations\" : [ \"https://arxiv.org/2021.12345/\" , \"https://arxiv.org/2021.56789/\" ]} \u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b\u200b )","title":"Examples"},{"location":"Reference/classes/Package/build/","text":"Accera v1.2.1 Reference accera.Package.build(name[, format, mode, platform, tolerance, output_dir]) Builds a HAT package. Arguments argument description type/default name The package name. string format The format of the package. accera.Package.Format , defaults to Package.Format.HAT_STATIC mode The package mode, such as whether it is optimized or used for debugging. robopy.Package.Mode , defaults to Package.Mode.Release platform The platform where the package will run. accera.Package.Platform tolerance The tolerance for correctness checking when mode = Package.Mode.Debug . float, defaults to 1e-5 output_dir The path to an output directory. Defaults to the current directory if unspecified. string Examples Build a Dynamically-linked HAT package called myPackage containing func1 for the host platform in the current directory: package = acc . Package () package . add ( plan , base_name = \"func1\" ) package . build ( format = acc . Package . Format . HAT_DYNAMIC , name = \"myPackage\" ) Build a statically-linked HAT package called myPackage containing func1 for the host platform in the hat_packages subdirectory: package = acc . Package () package . add ( plan , base_name = \"func1\" ) package . build ( format = acc . Package . Format . HAT_STATIC , name = \"myPackage\" , output_dir = \"hat_packages\" ) Build a statically-linked myPackage with additional intermediate MLIR files for debugging purposes. To build a dynamically-linked package, use acc.Package.Format.MLIR_DYNAMIC : package = acc . Package () package . add ( plan , base_name = \"func1\" ) package . build ( format = acc . Package . Format . MLIR_STATIC , name = \"myPackage\" ) Build a package with error checking for func1 , outputing error messages to stderr if the default implementation and the Accera implementation do not match within a tolerance of 1.0e-6 : package = acc . Package () package . add ( plan , base_name = \"func1\" ) package . build ( format = acc . Package . Format . HAT_DYNAMIC , name = \"myPackage\" , mode = acc . Package . Mode . DEBUG , tolerance = 1.0e-6 ) Cross-compile a statically-linked HAT package called myPackage containing func1 for the Raspberry Pi 3. Note that dynamically-linked HAT packages are not supported for cross-compilation: pi3 = Target ( \"Raspberry Pi 3B\" , category = Target . Category . CPU ) plan = schedule . create_plan ( target = pi3 ) package = acc . Package () package . add ( plan , base_name = \"func1\" ) package . build ( format = acc . Package . Format . HAT_STATIC , name = \"myPackagePi3\" , platform = acc . Package . Platform . RASPBIAN )","title":"Package.build"},{"location":"Reference/classes/Package/build/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Package/build/#accerapackagebuildname-format-mode-platform-tolerance-output_dir","text":"Builds a HAT package.","title":"accera.Package.build(name[, format, mode, platform, tolerance, output_dir])"},{"location":"Reference/classes/Package/build/#arguments","text":"argument description type/default name The package name. string format The format of the package. accera.Package.Format , defaults to Package.Format.HAT_STATIC mode The package mode, such as whether it is optimized or used for debugging. robopy.Package.Mode , defaults to Package.Mode.Release platform The platform where the package will run. accera.Package.Platform tolerance The tolerance for correctness checking when mode = Package.Mode.Debug . float, defaults to 1e-5 output_dir The path to an output directory. Defaults to the current directory if unspecified. string","title":"Arguments"},{"location":"Reference/classes/Package/build/#examples","text":"Build a Dynamically-linked HAT package called myPackage containing func1 for the host platform in the current directory: package = acc . Package () package . add ( plan , base_name = \"func1\" ) package . build ( format = acc . Package . Format . HAT_DYNAMIC , name = \"myPackage\" ) Build a statically-linked HAT package called myPackage containing func1 for the host platform in the hat_packages subdirectory: package = acc . Package () package . add ( plan , base_name = \"func1\" ) package . build ( format = acc . Package . Format . HAT_STATIC , name = \"myPackage\" , output_dir = \"hat_packages\" ) Build a statically-linked myPackage with additional intermediate MLIR files for debugging purposes. To build a dynamically-linked package, use acc.Package.Format.MLIR_DYNAMIC : package = acc . Package () package . add ( plan , base_name = \"func1\" ) package . build ( format = acc . Package . Format . MLIR_STATIC , name = \"myPackage\" ) Build a package with error checking for func1 , outputing error messages to stderr if the default implementation and the Accera implementation do not match within a tolerance of 1.0e-6 : package = acc . Package () package . add ( plan , base_name = \"func1\" ) package . build ( format = acc . Package . Format . HAT_DYNAMIC , name = \"myPackage\" , mode = acc . Package . Mode . DEBUG , tolerance = 1.0e-6 ) Cross-compile a statically-linked HAT package called myPackage containing func1 for the Raspberry Pi 3. Note that dynamically-linked HAT packages are not supported for cross-compilation: pi3 = Target ( \"Raspberry Pi 3B\" , category = Target . Category . CPU ) plan = schedule . create_plan ( target = pi3 ) package = acc . Package () package . add ( plan , base_name = \"func1\" ) package . build ( format = acc . Package . Format . HAT_STATIC , name = \"myPackagePi3\" , platform = acc . Package . Platform . RASPBIAN )","title":"Examples"},{"location":"Reference/classes/Plan/bind/","text":"Accera v1.2.1 Reference accera.Plan.bind(indices, grid) Only available for targets that can execute a grid of work (such as GPUs). The bind function binds dimensions of the iteration space to axes of the target-specific grid (such as v100.GridUnit.BLOCK_X , v100.GridUnit.THREAD_X on an Nvidia GPU). Arguments argument description type/default indices The iteration-space dimensions to bind. tuple of Index grid The respective target-specific grid axes to bind with. tuple of target-specific identifiers Examples Mark the i , j , and k indices to execute on an NVidia V100's BLOCK_X , THREAD_X , and THREAD_Y grid axes respectively. v100 = acc . Target ( Target . Model . NVIDIA_V100 ) plan . bind ( indices = ( i , j , k ), grid = ( v100 . GridUnit . BLOCK_X , v100 . GridUnit . THREAD_X , v100 . GridUnit . THREAD_Y ))","title":"Plan.bind"},{"location":"Reference/classes/Plan/bind/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Plan/bind/#acceraplanbindindices-grid","text":"Only available for targets that can execute a grid of work (such as GPUs). The bind function binds dimensions of the iteration space to axes of the target-specific grid (such as v100.GridUnit.BLOCK_X , v100.GridUnit.THREAD_X on an Nvidia GPU).","title":"accera.Plan.bind(indices, grid)"},{"location":"Reference/classes/Plan/bind/#arguments","text":"argument description type/default indices The iteration-space dimensions to bind. tuple of Index grid The respective target-specific grid axes to bind with. tuple of target-specific identifiers","title":"Arguments"},{"location":"Reference/classes/Plan/bind/#examples","text":"Mark the i , j , and k indices to execute on an NVidia V100's BLOCK_X , THREAD_X , and THREAD_Y grid axes respectively. v100 = acc . Target ( Target . Model . NVIDIA_V100 ) plan . bind ( indices = ( i , j , k ), grid = ( v100 . GridUnit . BLOCK_X , v100 . GridUnit . THREAD_X , v100 . GridUnit . THREAD_Y ))","title":"Examples"},{"location":"Reference/classes/Plan/cache/","text":"Accera v1.2.1 Reference accera.Plan.cache(source[, index, trigger_index, layout, level, trigger_level, max_elements, thrifty, type]) Adds a caching strategy to a plan. Arguments argument description type/default source The array or cache from which this cache is copied. Array or Cache index The index used to determine the cache level. Specify one and only one of index , level , max_elements . Index trigger_index The index used to determine what level to fill the cache at. trigger_index can't come after index in the schedule order, and will default to index if not specified. Specify at most one of trigger_index or trigger_level . Index layout The affine memory map, if different from the source. accera.Layout level The key-slice level to cache (the number of wildcard dimensions in a key-slice). Specify one and only one of index , level , max_elements . positive integer trigger_level The key-slice level to fill the cache at. trigger_level can't be smaller than level , and will default to level if not specified. Specify at most one of trigger_index or trigger_level . positive integer max_elements The maximum elements to include in the cached region. Specify one and only one of index , level , max_elements . positive integer thrifty Use thrifty caching (copy data into a cache only if the cached data differs from the original active block). True or False location The type of memory used to store the cache. MemoryType Returns A Cache handle that represents the created cache. Examples Create a cache of array A at level 2. AA = plan . cache ( A , level = 2 ) Create a cache of array A with the Array.Layout.FIRST_MAJOR layout: AA = plan . cache ( A , level = 2 , layout = acc . Array . Layout . FIRST_MAJOR ) Create a cache of array A for dimension j : AA = plan . cache ( A , index = j ) Create a cache of array A for the largest active block that does not exceed 1024 elements: AA = plan . cache ( A , max_elements = 1024 ) Create a level 2 cache of array A from its level 4 cache: AA = plan . cache ( A , level = 4 ) AAA = plan . cache ( AA , level = 2 ) Not yet implemented: Create a cache of array A at index i in GPU shared memory: v100 = Target ( Target . Model . NVIDIA_V100 ) AA = plan . cache ( A , i , location = v100 . MemoryType . SHARED )","title":"Plan.cache"},{"location":"Reference/classes/Plan/cache/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Plan/cache/#acceraplancachesource-index-trigger_index-layout-level-trigger_level-max_elements-thrifty-type","text":"Adds a caching strategy to a plan.","title":"accera.Plan.cache(source[, index, trigger_index, layout, level, trigger_level, max_elements, thrifty, type])"},{"location":"Reference/classes/Plan/cache/#arguments","text":"argument description type/default source The array or cache from which this cache is copied. Array or Cache index The index used to determine the cache level. Specify one and only one of index , level , max_elements . Index trigger_index The index used to determine what level to fill the cache at. trigger_index can't come after index in the schedule order, and will default to index if not specified. Specify at most one of trigger_index or trigger_level . Index layout The affine memory map, if different from the source. accera.Layout level The key-slice level to cache (the number of wildcard dimensions in a key-slice). Specify one and only one of index , level , max_elements . positive integer trigger_level The key-slice level to fill the cache at. trigger_level can't be smaller than level , and will default to level if not specified. Specify at most one of trigger_index or trigger_level . positive integer max_elements The maximum elements to include in the cached region. Specify one and only one of index , level , max_elements . positive integer thrifty Use thrifty caching (copy data into a cache only if the cached data differs from the original active block). True or False location The type of memory used to store the cache. MemoryType","title":"Arguments"},{"location":"Reference/classes/Plan/cache/#returns","text":"A Cache handle that represents the created cache.","title":"Returns"},{"location":"Reference/classes/Plan/cache/#examples","text":"Create a cache of array A at level 2. AA = plan . cache ( A , level = 2 ) Create a cache of array A with the Array.Layout.FIRST_MAJOR layout: AA = plan . cache ( A , level = 2 , layout = acc . Array . Layout . FIRST_MAJOR ) Create a cache of array A for dimension j : AA = plan . cache ( A , index = j ) Create a cache of array A for the largest active block that does not exceed 1024 elements: AA = plan . cache ( A , max_elements = 1024 ) Create a level 2 cache of array A from its level 4 cache: AA = plan . cache ( A , level = 4 ) AAA = plan . cache ( AA , level = 2 ) Not yet implemented: Create a cache of array A at index i in GPU shared memory: v100 = Target ( Target . Model . NVIDIA_V100 ) AA = plan . cache ( A , i , location = v100 . MemoryType . SHARED )","title":"Examples"},{"location":"Reference/classes/Plan/kernelize/","text":"Accera v1.2.1 Reference accera.Plan.kernelize(unroll_indices[, vectorize_indices]) A convenience method for a sequence of unroll instructions followed by a possible sequence of vectorize instructions Arguments argument description type/default unroll_indices The iteration-space dimensions to unroll tuple of accera.Index vectorize_indices The optional iteration-space dimensions to vectorize accera.Index or tuple of accera.Index Examples Unroll i and k , and then vectorize j : schedule . reorder ( i , k , j ) plan = schedule . create_plan () plan . kernelize ( unroll_indices = ( i , k ), vectorize_indices = j ) Another example is to Unroll i , and then vectorize j and k : schedule . reorder ( i , j , k ) plan = schedule . create_plan () plan . kernelize ( unroll_indices = ( i ,), vectorize_indices = ( j , k ))","title":"Plan.kernelize"},{"location":"Reference/classes/Plan/kernelize/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Plan/kernelize/#acceraplankernelizeunroll_indices-vectorize_indices","text":"A convenience method for a sequence of unroll instructions followed by a possible sequence of vectorize instructions","title":"accera.Plan.kernelize(unroll_indices[, vectorize_indices])"},{"location":"Reference/classes/Plan/kernelize/#arguments","text":"argument description type/default unroll_indices The iteration-space dimensions to unroll tuple of accera.Index vectorize_indices The optional iteration-space dimensions to vectorize accera.Index or tuple of accera.Index","title":"Arguments"},{"location":"Reference/classes/Plan/kernelize/#examples","text":"Unroll i and k , and then vectorize j : schedule . reorder ( i , k , j ) plan = schedule . create_plan () plan . kernelize ( unroll_indices = ( i , k ), vectorize_indices = j ) Another example is to Unroll i , and then vectorize j and k : schedule . reorder ( i , j , k ) plan = schedule . create_plan () plan . kernelize ( unroll_indices = ( i ,), vectorize_indices = ( j , k ))","title":"Examples"},{"location":"Reference/classes/Plan/parallelize/","text":"Accera v1.2.1 Reference accera.Plan.parallelize(indices[, pin, policy]) Performs one or more loops in parallel on multiple cores or processors. Only available for targets with multiple cores or processors. Arguments argument description type/default indices The iteration-space dimensions to run in parallel. To assign multiple threads to an index, first split that index, then parallelize its split indices. Unsplit indices will be assigned one thread each, split indices will be assigned threads based on the number of split blocks. This is limited by the number of threads supported by the target. tuple of accera.Index pin Pin the computation to a subset of cores or processors. tuple of target-specific identifiers policy The scheduling policy to apply (\"dynamic\" or \"static\"). string. Defaults to \"static\" Examples Parallelize the i , j , and k dimensions using 3 threads: plan . parallelize ( indices = ( i , j , k )) Parallelize the i dimension using 4 threads: N = 1024 # shape of the i dimension num_threads = 4 # divide the shape by num_threads to get the block size per thread block_size = N // num_threads ii = schedule . split ( i , size = block_size ) plan . parallelize ( indices = i ) Not yet implemented: Parallelize the i , j , and k dimensions by pinning them to specific cores on an Intel Xeon E5: plan . parallelize ( indices = ( i , j , k ), pin = ( xeonE5 . cores [ 0 ], xeonE5 . cores [ 1 ], xeonE5 . cores [ 2 ])) Apply a dynamic scheduling policy, which uses a queue to partition the work across multiple cores: plan . parallelize ( indices = ( i , j , k ), policy = \"dynamic\" )","title":"Plan.parallelize"},{"location":"Reference/classes/Plan/parallelize/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Plan/parallelize/#acceraplanparallelizeindices-pin-policy","text":"Performs one or more loops in parallel on multiple cores or processors. Only available for targets with multiple cores or processors.","title":"accera.Plan.parallelize(indices[, pin, policy])"},{"location":"Reference/classes/Plan/parallelize/#arguments","text":"argument description type/default indices The iteration-space dimensions to run in parallel. To assign multiple threads to an index, first split that index, then parallelize its split indices. Unsplit indices will be assigned one thread each, split indices will be assigned threads based on the number of split blocks. This is limited by the number of threads supported by the target. tuple of accera.Index pin Pin the computation to a subset of cores or processors. tuple of target-specific identifiers policy The scheduling policy to apply (\"dynamic\" or \"static\"). string. Defaults to \"static\"","title":"Arguments"},{"location":"Reference/classes/Plan/parallelize/#examples","text":"Parallelize the i , j , and k dimensions using 3 threads: plan . parallelize ( indices = ( i , j , k )) Parallelize the i dimension using 4 threads: N = 1024 # shape of the i dimension num_threads = 4 # divide the shape by num_threads to get the block size per thread block_size = N // num_threads ii = schedule . split ( i , size = block_size ) plan . parallelize ( indices = i ) Not yet implemented: Parallelize the i , j , and k dimensions by pinning them to specific cores on an Intel Xeon E5: plan . parallelize ( indices = ( i , j , k ), pin = ( xeonE5 . cores [ 0 ], xeonE5 . cores [ 1 ], xeonE5 . cores [ 2 ])) Apply a dynamic scheduling policy, which uses a queue to partition the work across multiple cores: plan . parallelize ( indices = ( i , j , k ), policy = \"dynamic\" )","title":"Examples"},{"location":"Reference/classes/Plan/unroll/","text":"Accera v1.2.1 Reference accera.Plan.unroll(index) Marks a dimension of the iteration-space for unrolling. Arguments argument description type/default index The index to unroll. Index Examples Mark the i dimension for unrolling: plan . unroll ( index = i )","title":"Plan.unroll"},{"location":"Reference/classes/Plan/unroll/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Plan/unroll/#acceraplanunrollindex","text":"Marks a dimension of the iteration-space for unrolling.","title":"accera.Plan.unroll(index)"},{"location":"Reference/classes/Plan/unroll/#arguments","text":"argument description type/default index The index to unroll. Index","title":"Arguments"},{"location":"Reference/classes/Plan/unroll/#examples","text":"Mark the i dimension for unrolling: plan . unroll ( index = i )","title":"Examples"},{"location":"Reference/classes/Plan/vectorize/","text":"Accera v1.2.1 Reference accera.Plan.vectorize(index) Only available for targets that have SIMD registers and support vector instructions. Marks a dimension of the iteration-space for vectorization. Arguments argument description type/default index The index to vectorize. Index Examples Mark the dimension ii for vectorized execution: plan . vectorize ( index = ii )","title":"Plan.vectorize"},{"location":"Reference/classes/Plan/vectorize/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Plan/vectorize/#acceraplanvectorizeindex","text":"Only available for targets that have SIMD registers and support vector instructions. Marks a dimension of the iteration-space for vectorization.","title":"accera.Plan.vectorize(index)"},{"location":"Reference/classes/Plan/vectorize/#arguments","text":"argument description type/default index The index to vectorize. Index","title":"Arguments"},{"location":"Reference/classes/Plan/vectorize/#examples","text":"Mark the dimension ii for vectorized execution: plan . vectorize ( index = ii )","title":"Examples"},{"location":"Reference/classes/Schedule/create_plan/","text":"Accera v1.2.1 Reference accera.Schedule.create_plan([target]) Create a plan for the nest. Arguments argument description type/default target The target platform. Defaults to acc.Target.HOST Target Returns Plan Examples TODO","title":"Schedule.create_plan"},{"location":"Reference/classes/Schedule/create_plan/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Schedule/create_plan/#acceraschedulecreate_plantarget","text":"Create a plan for the nest.","title":"accera.Schedule.create_plan([target])"},{"location":"Reference/classes/Schedule/create_plan/#arguments","text":"argument description type/default target The target platform. Defaults to acc.Target.HOST Target","title":"Arguments"},{"location":"Reference/classes/Schedule/create_plan/#returns","text":"Plan","title":"Returns"},{"location":"Reference/classes/Schedule/create_plan/#examples","text":"TODO","title":"Examples"},{"location":"Reference/classes/Schedule/pad/","text":"Accera v1.2.1 Reference accera.Schedule.pad(index, size) Pads the beginning of a specified dimension of the iteration-space with empty (no-op) elements. Arguments argument description type/default index The dimension to pad Index size The number of elements to pad non-negative integer Examples Pads the beginning of dimension i with 10 empty elements schedule . pad ( i , 10 )","title":"Schedule.pad"},{"location":"Reference/classes/Schedule/pad/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Schedule/pad/#acceraschedulepadindex-size","text":"Pads the beginning of a specified dimension of the iteration-space with empty (no-op) elements.","title":"accera.Schedule.pad(index, size)"},{"location":"Reference/classes/Schedule/pad/#arguments","text":"argument description type/default index The dimension to pad Index size The number of elements to pad non-negative integer","title":"Arguments"},{"location":"Reference/classes/Schedule/pad/#examples","text":"Pads the beginning of dimension i with 10 empty elements schedule . pad ( i , 10 )","title":"Examples"},{"location":"Reference/classes/Schedule/reorder/","text":"Accera v1.2.1 Reference accera.Schedule.reorder(order, *args) The reorder transformation sets the order of the indices in the schedule. These orders are not allowed: 1. The outer dimension created by a split transformation must always precede the corresponding inner dimension . 2. The fusing dimension created by a fuse operation must always precede any unfused dimensions . Arguments argument description type/default order Either the order of indices to set, or the outermost index if using variable arguments tuple of Index or Index *args Optional variable arguments containing subsequent indices to set variable Index arguments Examples Reorder a schedule by moving the k dimension to the outermost loop: schedule . reorder ( k , i , j ) Using a tuple to reorder a schedule. This overloaded form is better suited for parameters: schedule . reorder ( order = ( k , i , j ))","title":"Schedule.reorder"},{"location":"Reference/classes/Schedule/reorder/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Schedule/reorder/#acceraschedulereorderorder-args","text":"The reorder transformation sets the order of the indices in the schedule. These orders are not allowed: 1. The outer dimension created by a split transformation must always precede the corresponding inner dimension . 2. The fusing dimension created by a fuse operation must always precede any unfused dimensions .","title":"accera.Schedule.reorder(order, *args)"},{"location":"Reference/classes/Schedule/reorder/#arguments","text":"argument description type/default order Either the order of indices to set, or the outermost index if using variable arguments tuple of Index or Index *args Optional variable arguments containing subsequent indices to set variable Index arguments","title":"Arguments"},{"location":"Reference/classes/Schedule/reorder/#examples","text":"Reorder a schedule by moving the k dimension to the outermost loop: schedule . reorder ( k , i , j ) Using a tuple to reorder a schedule. This overloaded form is better suited for parameters: schedule . reorder ( order = ( k , i , j ))","title":"Examples"},{"location":"Reference/classes/Schedule/skew/","text":"Accera v1.2.1 Reference accera.Schedule.skew(index, reference_index [, unroll_loops_smaller_than]) Transform a dimension with respect to a reference dimension into a parallelogram by padding with empty elements. Arguments argument description type/default index The dimension to skew Index reference_index The reference dimension Index unroll_loops_smaller_than Unroll loops that are smaller than this range (non-inclusive) non-negative integer Examples Skew dimension i with respect to dimension j : schedule . skew ( i , j ) Skew dimension j with respect to dimension i , and unroll if the resulting loops are smaller than 3: schedule . skew ( j , i , unroll_loops_smaller_than = 3 )","title":"Schedule.skew"},{"location":"Reference/classes/Schedule/skew/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Schedule/skew/#accerascheduleskewindex-reference_index-unroll_loops_smaller_than","text":"Transform a dimension with respect to a reference dimension into a parallelogram by padding with empty elements.","title":"accera.Schedule.skew(index, reference_index [, unroll_loops_smaller_than])"},{"location":"Reference/classes/Schedule/skew/#arguments","text":"argument description type/default index The dimension to skew Index reference_index The reference dimension Index unroll_loops_smaller_than Unroll loops that are smaller than this range (non-inclusive) non-negative integer","title":"Arguments"},{"location":"Reference/classes/Schedule/skew/#examples","text":"Skew dimension i with respect to dimension j : schedule . skew ( i , j ) Skew dimension j with respect to dimension i , and unroll if the resulting loops are smaller than 3: schedule . skew ( j , i , unroll_loops_smaller_than = 3 )","title":"Examples"},{"location":"Reference/classes/Schedule/split/","text":"Accera v1.2.1 Reference accera.Schedule.split(index, size) The split transformation takes a dimension i and a size , modifies i , and creates a new dimension ii . Assume that the original size of dimension i was n : The split transformation splits dimension i into ceil(n/size) parts of size size , arranges each of those parts along dimension ii , and stacks the ceil(n/size) parts along dimension i . If the split size does not divide the dimension size, empty elements are added such that the split size does divide the dimension size. Arguments argument description type/default index The dimension to split Index size The split size non-negative integer Returns Index for the new inner dimension Examples Split the i dimension by 5, creating a new dimension ii : ii = schedule . split ( j , 5 )","title":"Schedule.split"},{"location":"Reference/classes/Schedule/split/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Schedule/split/#acceraschedulesplitindex-size","text":"The split transformation takes a dimension i and a size , modifies i , and creates a new dimension ii . Assume that the original size of dimension i was n : The split transformation splits dimension i into ceil(n/size) parts of size size , arranges each of those parts along dimension ii , and stacks the ceil(n/size) parts along dimension i . If the split size does not divide the dimension size, empty elements are added such that the split size does divide the dimension size.","title":"accera.Schedule.split(index, size)"},{"location":"Reference/classes/Schedule/split/#arguments","text":"argument description type/default index The dimension to split Index size The split size non-negative integer","title":"Arguments"},{"location":"Reference/classes/Schedule/split/#returns","text":"Index for the new inner dimension","title":"Returns"},{"location":"Reference/classes/Schedule/split/#examples","text":"Split the i dimension by 5, creating a new dimension ii : ii = schedule . split ( j , 5 )","title":"Examples"},{"location":"Reference/classes/Schedule/tile/","text":"Accera v1.2.1 Reference accera.Schedule.tile(indices, sizes) The tile transformation is a convenience syntax that takes a tuple of indices and a tuple of sizes, and splits each index by the corresponding size. The indices involved in the split are then reordered such that all the outer indices precede all of the inner indices. Arguments argument description type/default indices The indices to tile tuple of Index sizes The tile sizes tuple of non-negative integers Returns Tuple of Index representing the new inner dimensions Examples Tile the i , j , and k dimensions by 8, 2, and 3 respectively ii , jj , kk = schedule . tile (( i , j , k ), ( 8 , 2 , 3 ))","title":"Schedule.tile"},{"location":"Reference/classes/Schedule/tile/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Schedule/tile/#accerascheduletileindices-sizes","text":"The tile transformation is a convenience syntax that takes a tuple of indices and a tuple of sizes, and splits each index by the corresponding size. The indices involved in the split are then reordered such that all the outer indices precede all of the inner indices.","title":"accera.Schedule.tile(indices, sizes)"},{"location":"Reference/classes/Schedule/tile/#arguments","text":"argument description type/default indices The indices to tile tuple of Index sizes The tile sizes tuple of non-negative integers","title":"Arguments"},{"location":"Reference/classes/Schedule/tile/#returns","text":"Tuple of Index representing the new inner dimensions","title":"Returns"},{"location":"Reference/classes/Schedule/tile/#examples","text":"Tile the i , j , and k dimensions by 8, 2, and 3 respectively ii , jj , kk = schedule . tile (( i , j , k ), ( 8 , 2 , 3 ))","title":"Examples"},{"location":"Reference/classes/Target/Architecture/","text":"Accera v1.2.1 Reference accera.Target.Architecture Defines the supported target architectures. type description accera.Target.Architecture.HOST The host computer's architecture accera.Target.Architecture.ARM The ARM architecture accera.Target.Architecture.X86 The 32-bit x86 architecture accera.Target.Architecture.X86_64 The 64-bit x86 architecture TODO: AARCH64?","title":"Target.Architecture"},{"location":"Reference/classes/Target/Architecture/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Target/Architecture/#acceratargetarchitecture","text":"Defines the supported target architectures. type description accera.Target.Architecture.HOST The host computer's architecture accera.Target.Architecture.ARM The ARM architecture accera.Target.Architecture.X86 The 32-bit x86 architecture accera.Target.Architecture.X86_64 The 64-bit x86 architecture TODO: AARCH64?","title":"accera.Target.Architecture"},{"location":"Reference/classes/Target/Category/","text":"Accera v1.2.1 Reference accera.Target.Category Defines the target processor category. type description accera.Target.Category.CPU accera.Target.Category.GPU","title":"Target.Category"},{"location":"Reference/classes/Target/Category/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Target/Category/#acceratargetcategory","text":"Defines the target processor category. type description accera.Target.Category.CPU accera.Target.Category.GPU","title":"accera.Target.Category"},{"location":"Reference/classes/Target/Model/","text":"Accera v1.2.1 Reference accera.Target.Model Defines constants for some well-known CPU models. type description accera.Target.Model.AMD_1200 AMD 1200 accera.Target.Model.AMD_1300X AMD 1300X accera.Target.Model.AMD_1400 AMD 1400 accera.Target.Model.AMD_1500X AMD 1500X accera.Target.Model.AMD_1600 AMD 1600 accera.Target.Model.AMD_1600X AMD 1600X accera.Target.Model.AMD_1700 AMD 1700 accera.Target.Model.AMD_1700X AMD 1700X accera.Target.Model.AMD_1800X AMD 1800X accera.Target.Model.AMD_1900X AMD 1900X accera.Target.Model.AMD_1920X AMD 1920X accera.Target.Model.AMD_1950X AMD 1950X accera.Target.Model.AMD_200GE AMD 200GE accera.Target.Model.AMD_2200G AMD 2200G accera.Target.Model.AMD_2200GE AMD 2200GE accera.Target.Model.AMD_2200U AMD 2200U accera.Target.Model.AMD_220GE AMD 220GE accera.Target.Model.AMD_2300U AMD 2300U accera.Target.Model.AMD_2300X AMD 2300X accera.Target.Model.AMD_2400G AMD 2400G accera.Target.Model.AMD_2400GE AMD 2400GE accera.Target.Model.AMD_240GE AMD 240GE accera.Target.Model.AMD_2500U AMD 2500U accera.Target.Model.AMD_2500X AMD 2500X accera.Target.Model.AMD_2600 AMD 2600 accera.Target.Model.AMD_2600E AMD 2600E accera.Target.Model.AMD_2600H AMD 2600H accera.Target.Model.AMD_2600X AMD 2600X accera.Target.Model.AMD_2700 AMD 2700 accera.Target.Model.AMD_2700E AMD 2700E accera.Target.Model.AMD_2700U AMD 2700U accera.Target.Model.AMD_2700X AMD 2700X accera.Target.Model.AMD_2700X_GOLD_EDITION AMD 2700X Gold Edition accera.Target.Model.AMD_2800H AMD 2800H accera.Target.Model.AMD_2920X AMD 2920X accera.Target.Model.AMD_2950X AMD 2950X accera.Target.Model.AMD_2970WX AMD 2970WX accera.Target.Model.AMD_2990WX AMD 2990WX accera.Target.Model.AMD_3000G AMD 3000G accera.Target.Model.AMD_300U AMD 300U accera.Target.Model.AMD_3050U AMD 3050U accera.Target.Model.AMD_3101 AMD 3101 accera.Target.Model.AMD_3150U AMD 3150U accera.Target.Model.AMD_3151 AMD 3151 accera.Target.Model.AMD_3200G AMD 3200G accera.Target.Model.AMD_3200U AMD 3200U accera.Target.Model.AMD_3201 AMD 3201 accera.Target.Model.AMD_3250U AMD 3250U accera.Target.Model.AMD_3251 AMD 3251 accera.Target.Model.AMD_3255 AMD 3255 accera.Target.Model.AMD_3300U AMD 3300U accera.Target.Model.AMD_3301 AMD 3301 accera.Target.Model.AMD_3351 AMD 3351 accera.Target.Model.AMD_3400G AMD 3400G accera.Target.Model.AMD_3401 AMD 3401 accera.Target.Model.AMD_3451 AMD 3451 accera.Target.Model.AMD_3500 AMD 3500 accera.Target.Model.AMD_3500U AMD 3500U accera.Target.Model.AMD_3500X AMD 3500X accera.Target.Model.AMD_3550H AMD 3550H accera.Target.Model.AMD_3580U AMD 3580U accera.Target.Model.AMD_3600 AMD 3600 accera.Target.Model.AMD_3600X AMD 3600X accera.Target.Model.AMD_3600XT AMD 3600XT accera.Target.Model.AMD_3700U AMD 3700U accera.Target.Model.AMD_3700X AMD 3700X accera.Target.Model.AMD_3750H AMD 3750H accera.Target.Model.AMD_3780U AMD 3780U accera.Target.Model.AMD_3800X AMD 3800X accera.Target.Model.AMD_3800XT AMD 3800XT accera.Target.Model.AMD_3900 AMD 3900 accera.Target.Model.AMD_3900X AMD 3900X accera.Target.Model.AMD_3900XT AMD 3900XT accera.Target.Model.AMD_3950X AMD 3950X accera.Target.Model.AMD_3960X AMD 3960X accera.Target.Model.AMD_3970X AMD 3970X accera.Target.Model.AMD_3980X AMD 3980X accera.Target.Model.AMD_3990X AMD 3990X accera.Target.Model.AMD_4300G AMD 4300G accera.Target.Model.AMD_4300GE AMD 4300GE accera.Target.Model.AMD_4300U AMD 4300U accera.Target.Model.AMD_4500U AMD 4500U accera.Target.Model.AMD_4600G AMD 4600G accera.Target.Model.AMD_4600GE AMD 4600GE accera.Target.Model.AMD_4600H AMD 4600H accera.Target.Model.AMD_4600HS AMD 4600HS accera.Target.Model.AMD_4600U AMD 4600U accera.Target.Model.AMD_4680U AMD 4680U accera.Target.Model.AMD_4700G AMD 4700G accera.Target.Model.AMD_4700GE AMD 4700GE accera.Target.Model.AMD_4700U AMD 4700U accera.Target.Model.AMD_4800H AMD 4800H accera.Target.Model.AMD_4800HS AMD 4800HS accera.Target.Model.AMD_4800U AMD 4800U accera.Target.Model.AMD_4900H AMD 4900H accera.Target.Model.AMD_4900HS AMD 4900HS accera.Target.Model.AMD_4980U AMD 4980U accera.Target.Model.AMD_5300G AMD 5300G accera.Target.Model.AMD_5300GE AMD 5300GE accera.Target.Model.AMD_5300U AMD 5300U accera.Target.Model.AMD_5400U AMD 5400U accera.Target.Model.AMD_5500U AMD 5500U accera.Target.Model.AMD_5600G AMD 5600G accera.Target.Model.AMD_5600GE AMD 5600GE accera.Target.Model.AMD_5600H AMD 5600H accera.Target.Model.AMD_5600HS AMD 5600HS accera.Target.Model.AMD_5600U AMD 5600U accera.Target.Model.AMD_5600X AMD 5600X accera.Target.Model.AMD_5700G AMD 5700G accera.Target.Model.AMD_5700GE AMD 5700GE accera.Target.Model.AMD_5700U AMD 5700U accera.Target.Model.AMD_5800 AMD 5800 accera.Target.Model.AMD_5800H AMD 5800H accera.Target.Model.AMD_5800HS AMD 5800HS accera.Target.Model.AMD_5800U AMD 5800U accera.Target.Model.AMD_5800X AMD 5800X accera.Target.Model.AMD_5900 AMD 5900 accera.Target.Model.AMD_5900HS AMD 5900HS accera.Target.Model.AMD_5900HX AMD 5900HX accera.Target.Model.AMD_5900X AMD 5900X accera.Target.Model.AMD_5950X AMD 5950X accera.Target.Model.AMD_5980HS AMD 5980HS accera.Target.Model.AMD_5980HX AMD 5980HX accera.Target.Model.AMD_7232P AMD 7232P accera.Target.Model.AMD_7251 AMD 7251 accera.Target.Model.AMD_7252 AMD 7252 accera.Target.Model.AMD_7261 AMD 7261 accera.Target.Model.AMD_7262 AMD 7262 accera.Target.Model.AMD_7272 AMD 7272 accera.Target.Model.AMD_7281 AMD 7281 accera.Target.Model.AMD_7282 AMD 7282 accera.Target.Model.AMD_72F3 AMD 72F3 accera.Target.Model.AMD_7301 AMD 7301 accera.Target.Model.AMD_7302 AMD 7302 accera.Target.Model.AMD_7302P AMD 7302P accera.Target.Model.AMD_7313 AMD 7313 accera.Target.Model.AMD_7313P AMD 7313P accera.Target.Model.AMD_7343 AMD 7343 accera.Target.Model.AMD_7351 AMD 7351 accera.Target.Model.AMD_7351P AMD 7351P accera.Target.Model.AMD_7352 AMD 7352 accera.Target.Model.AMD_7371 AMD 7371 accera.Target.Model.AMD_73F3 AMD 73F3 accera.Target.Model.AMD_7401 AMD 7401 accera.Target.Model.AMD_7401P AMD 7401P accera.Target.Model.AMD_7402 AMD 7402 accera.Target.Model.AMD_7402P AMD 7402P accera.Target.Model.AMD_7413 AMD 7413 accera.Target.Model.AMD_7443 AMD 7443 accera.Target.Model.AMD_7443P AMD 7443P accera.Target.Model.AMD_7451 AMD 7451 accera.Target.Model.AMD_7452 AMD 7452 accera.Target.Model.AMD_7453 AMD 7453 accera.Target.Model.AMD_74F3 AMD 74F3 accera.Target.Model.AMD_7501 AMD 7501 accera.Target.Model.AMD_7502 AMD 7502 accera.Target.Model.AMD_7502P AMD 7502P accera.Target.Model.AMD_7513 AMD 7513 accera.Target.Model.AMD_7532 AMD 7532 accera.Target.Model.AMD_7542 AMD 7542 accera.Target.Model.AMD_7543 AMD 7543 accera.Target.Model.AMD_7543P AMD 7543P accera.Target.Model.AMD_7551 AMD 7551 accera.Target.Model.AMD_7551P AMD 7551P accera.Target.Model.AMD_7552 AMD 7552 accera.Target.Model.AMD_75F3 AMD 75F3 accera.Target.Model.AMD_7601 AMD 7601 accera.Target.Model.AMD_7642 AMD 7642 accera.Target.Model.AMD_7643 AMD 7643 accera.Target.Model.AMD_7662 AMD 7662 accera.Target.Model.AMD_7663 AMD 7663 accera.Target.Model.AMD_7702 AMD 7702 accera.Target.Model.AMD_7702P AMD 7702P accera.Target.Model.AMD_7713 AMD 7713 accera.Target.Model.AMD_7713P AMD 7713P accera.Target.Model.AMD_7742 AMD 7742 accera.Target.Model.AMD_7763 AMD 7763 accera.Target.Model.AMD_7F32 AMD 7F32 accera.Target.Model.AMD_7F52 AMD 7F52 accera.Target.Model.AMD_7F72 AMD 7F72 accera.Target.Model.AMD_7H12 AMD 7H12 accera.Target.Model.AMD_FIREFLIGHT AMD FireFlight accera.Target.Model.AMD_PRO_1200 AMD PRO 1200 accera.Target.Model.AMD_PRO_1300 AMD PRO 1300 accera.Target.Model.AMD_PRO_1500 AMD PRO 1500 accera.Target.Model.AMD_PRO_1600 AMD PRO 1600 accera.Target.Model.AMD_PRO_1700 AMD PRO 1700 accera.Target.Model.AMD_PRO_1700X AMD PRO 1700X accera.Target.Model.AMD_PRO_200GE AMD PRO 200GE accera.Target.Model.AMD_PRO_2200G AMD PRO 2200G accera.Target.Model.AMD_PRO_2200GE AMD PRO 2200GE accera.Target.Model.AMD_PRO_2300U AMD PRO 2300U accera.Target.Model.AMD_PRO_2400G AMD PRO 2400G accera.Target.Model.AMD_PRO_2400GE AMD PRO 2400GE accera.Target.Model.AMD_PRO_2500U AMD PRO 2500U accera.Target.Model.AMD_PRO_2600 AMD PRO 2600 accera.Target.Model.AMD_PRO_2700 AMD PRO 2700 accera.Target.Model.AMD_PRO_2700U AMD PRO 2700U accera.Target.Model.AMD_PRO_2700X AMD PRO 2700X accera.Target.Model.AMD_PRO_300GE AMD PRO 300GE accera.Target.Model.AMD_PRO_300U AMD PRO 300U accera.Target.Model.AMD_PRO_3200G AMD PRO 3200G accera.Target.Model.AMD_PRO_3200GE AMD PRO 3200GE accera.Target.Model.AMD_PRO_3300U AMD PRO 3300U accera.Target.Model.AMD_PRO_3400G AMD PRO 3400G accera.Target.Model.AMD_PRO_3400GE AMD PRO 3400GE accera.Target.Model.AMD_PRO_3500U AMD PRO 3500U accera.Target.Model.AMD_PRO_3600 AMD PRO 3600 accera.Target.Model.AMD_PRO_3700 AMD PRO 3700 accera.Target.Model.AMD_PRO_3700U AMD PRO 3700U accera.Target.Model.AMD_PRO_3900 AMD PRO 3900 accera.Target.Model.AMD_PRO_4350G AMD PRO 4350G accera.Target.Model.AMD_PRO_4350GE AMD PRO 4350GE accera.Target.Model.AMD_PRO_4450U AMD PRO 4450U accera.Target.Model.AMD_PRO_4650G AMD PRO 4650G accera.Target.Model.AMD_PRO_4650GE AMD PRO 4650GE accera.Target.Model.AMD_PRO_4650U AMD PRO 4650U accera.Target.Model.AMD_PRO_4750G AMD PRO 4750G accera.Target.Model.AMD_PRO_4750GE AMD PRO 4750GE accera.Target.Model.AMD_PRO_4750U AMD PRO 4750U accera.Target.Model.AMD_PRO_5350G AMD PRO 5350G accera.Target.Model.AMD_PRO_5350GE AMD PRO 5350GE accera.Target.Model.AMD_PRO_5450U AMD PRO 5450U accera.Target.Model.AMD_PRO_5650G AMD PRO 5650G accera.Target.Model.AMD_PRO_5650GE AMD PRO 5650GE accera.Target.Model.AMD_PRO_5650U AMD PRO 5650U accera.Target.Model.AMD_PRO_5750G AMD PRO 5750G accera.Target.Model.AMD_PRO_5750GE AMD PRO 5750GE accera.Target.Model.AMD_PRO_5850U AMD PRO 5850U accera.Target.Model.AMD_R1102G AMD R1102G accera.Target.Model.AMD_R1305G AMD R1305G accera.Target.Model.AMD_R1505G AMD R1505G accera.Target.Model.AMD_R1606G AMD R1606G accera.Target.Model.AMD_V1202B AMD V1202B accera.Target.Model.AMD_V1404I AMD V1404I accera.Target.Model.AMD_V1500B AMD V1500B accera.Target.Model.AMD_V1605B AMD V1605B accera.Target.Model.AMD_V1756B AMD V1756B accera.Target.Model.AMD_V1780B AMD V1780B accera.Target.Model.AMD_V1807B AMD V1807B accera.Target.Model.AMD_V2516 AMD V2516 accera.Target.Model.AMD_V2546 AMD V2546 accera.Target.Model.AMD_V2718 AMD V2718 accera.Target.Model.AMD_V2748 AMD V2748 accera.Target.Model.INTEL_1000G1 Intel 1000G1 accera.Target.Model.INTEL_1000G4 Intel 1000G4 accera.Target.Model.INTEL_1005G1 Intel 1005G1 accera.Target.Model.INTEL_10100 Intel 10100 accera.Target.Model.INTEL_10100F Intel 10100F accera.Target.Model.INTEL_10100T Intel 10100T accera.Target.Model.INTEL_10300 Intel 10300 accera.Target.Model.INTEL_10300T Intel 10300T accera.Target.Model.INTEL_1030G4 Intel 1030G4 accera.Target.Model.INTEL_1030G7 Intel 1030G7 accera.Target.Model.INTEL_10320 Intel 10320 accera.Target.Model.INTEL_1035G1 Intel 1035G1 accera.Target.Model.INTEL_1035G4 Intel 1035G4 accera.Target.Model.INTEL_1035G7 Intel 1035G7 accera.Target.Model.INTEL_10400 Intel 10400 accera.Target.Model.INTEL_10400F Intel 10400F accera.Target.Model.INTEL_10400T Intel 10400T accera.Target.Model.INTEL_10500 Intel 10500 accera.Target.Model.INTEL_10500T Intel 10500T accera.Target.Model.INTEL_10600 Intel 10600 accera.Target.Model.INTEL_10600K Intel 10600K accera.Target.Model.INTEL_10600KF Intel 10600KF accera.Target.Model.INTEL_10600T Intel 10600T accera.Target.Model.INTEL_1060G7 Intel 1060G7 accera.Target.Model.INTEL_1065G7 Intel 1065G7 accera.Target.Model.INTEL_1068G7 Intel 1068G7 accera.Target.Model.INTEL_10700 Intel 10700 accera.Target.Model.INTEL_10700F Intel 10700F accera.Target.Model.INTEL_10700K Intel 10700K accera.Target.Model.INTEL_10700KF Intel 10700KF accera.Target.Model.INTEL_10700T Intel 10700T accera.Target.Model.INTEL_10850K Intel 10850K accera.Target.Model.INTEL_10900 Intel 10900 accera.Target.Model.INTEL_10900F Intel 10900F accera.Target.Model.INTEL_10900K Intel 10900K accera.Target.Model.INTEL_10900KF Intel 10900KF accera.Target.Model.INTEL_10900T Intel 10900T accera.Target.Model.INTEL_10910 Intel 10910 accera.Target.Model.INTEL_11400 Intel 11400 accera.Target.Model.INTEL_11400F Intel 11400F accera.Target.Model.INTEL_11400T Intel 11400T accera.Target.Model.INTEL_11500 Intel 11500 accera.Target.Model.INTEL_11500T Intel 11500T accera.Target.Model.INTEL_11600 Intel 11600 accera.Target.Model.INTEL_11600K Intel 11600K accera.Target.Model.INTEL_11600KF Intel 11600KF accera.Target.Model.INTEL_11600T Intel 11600T accera.Target.Model.INTEL_11700 Intel 11700 accera.Target.Model.INTEL_11700F Intel 11700F accera.Target.Model.INTEL_11700K Intel 11700K accera.Target.Model.INTEL_11700KF Intel 11700KF accera.Target.Model.INTEL_11700T Intel 11700T accera.Target.Model.INTEL_11900 Intel 11900 accera.Target.Model.INTEL_11900F Intel 11900F accera.Target.Model.INTEL_11900K Intel 11900K accera.Target.Model.INTEL_11900KF Intel 11900KF accera.Target.Model.INTEL_11900T Intel 11900T accera.Target.Model.INTEL_1250 Intel 1250 accera.Target.Model.INTEL_1250P Intel 1250P accera.Target.Model.INTEL_1270 Intel 1270 accera.Target.Model.INTEL_1270P Intel 1270P accera.Target.Model.INTEL_1290 Intel 1290 accera.Target.Model.INTEL_1290P Intel 1290P accera.Target.Model.INTEL_1290T Intel 1290T accera.Target.Model.INTEL_1350 Intel 1350 accera.Target.Model.INTEL_1350P Intel 1350P accera.Target.Model.INTEL_1370 Intel 1370 accera.Target.Model.INTEL_1370P Intel 1370P accera.Target.Model.INTEL_1390 Intel 1390 accera.Target.Model.INTEL_1390P Intel 1390P accera.Target.Model.INTEL_1390T Intel 1390T accera.Target.Model.INTEL_2104G Intel 2104G accera.Target.Model.INTEL_2124 Intel 2124 accera.Target.Model.INTEL_2124G Intel 2124G accera.Target.Model.INTEL_2126G Intel 2126G accera.Target.Model.INTEL_2134 Intel 2134 accera.Target.Model.INTEL_2136 Intel 2136 accera.Target.Model.INTEL_2144G Intel 2144G accera.Target.Model.INTEL_2146G Intel 2146G accera.Target.Model.INTEL_2174G Intel 2174G accera.Target.Model.INTEL_2176G Intel 2176G accera.Target.Model.INTEL_2186G Intel 2186G accera.Target.Model.INTEL_2314 Intel 2314 accera.Target.Model.INTEL_2324G Intel 2324G accera.Target.Model.INTEL_2334 Intel 2334 accera.Target.Model.INTEL_2336 Intel 2336 accera.Target.Model.INTEL_2356G Intel 2356G accera.Target.Model.INTEL_2374G Intel 2374G accera.Target.Model.INTEL_2378 Intel 2378 accera.Target.Model.INTEL_2378G Intel 2378G accera.Target.Model.INTEL_2386G Intel 2386G accera.Target.Model.INTEL_2388G Intel 2388G accera.Target.Model.INTEL_3204 Intel 3204 accera.Target.Model.INTEL_4208 Intel 4208 accera.Target.Model.INTEL_4209T Intel 4209T accera.Target.Model.INTEL_4210 Intel 4210 accera.Target.Model.INTEL_4210R Intel 4210R accera.Target.Model.INTEL_4214 Intel 4214 accera.Target.Model.INTEL_4214R Intel 4214R accera.Target.Model.INTEL_4214Y Intel 4214Y accera.Target.Model.INTEL_4215 Intel 4215 accera.Target.Model.INTEL_4215R Intel 4215R accera.Target.Model.INTEL_4216 Intel 4216 accera.Target.Model.INTEL_5215 Intel 5215 accera.Target.Model.INTEL_5215L Intel 5215L accera.Target.Model.INTEL_5215M Intel 5215M accera.Target.Model.INTEL_5217 Intel 5217 accera.Target.Model.INTEL_5218 Intel 5218 accera.Target.Model.INTEL_5218B Intel 5218B accera.Target.Model.INTEL_5218N Intel 5218N accera.Target.Model.INTEL_5218R Intel 5218R accera.Target.Model.INTEL_5218T Intel 5218T accera.Target.Model.INTEL_5220 Intel 5220 accera.Target.Model.INTEL_5220R Intel 5220R accera.Target.Model.INTEL_5220S Intel 5220S accera.Target.Model.INTEL_5220T Intel 5220T accera.Target.Model.INTEL_5222 Intel 5222 accera.Target.Model.INTEL_6098P Intel 6098P accera.Target.Model.INTEL_6100 Intel 6100 accera.Target.Model.INTEL_6100T Intel 6100T accera.Target.Model.INTEL_6209U Intel 6209U accera.Target.Model.INTEL_6210U Intel 6210U accera.Target.Model.INTEL_6212U Intel 6212U accera.Target.Model.INTEL_6222V Intel 6222V accera.Target.Model.INTEL_6226 Intel 6226 accera.Target.Model.INTEL_6226R Intel 6226R accera.Target.Model.INTEL_6230 Intel 6230 accera.Target.Model.INTEL_6230N Intel 6230N accera.Target.Model.INTEL_6230R Intel 6230R accera.Target.Model.INTEL_6230T Intel 6230T accera.Target.Model.INTEL_6234 Intel 6234 accera.Target.Model.INTEL_6238 Intel 6238 accera.Target.Model.INTEL_6238L Intel 6238L accera.Target.Model.INTEL_6238M Intel 6238M accera.Target.Model.INTEL_6238R Intel 6238R accera.Target.Model.INTEL_6238T Intel 6238T accera.Target.Model.INTEL_6240 Intel 6240 accera.Target.Model.INTEL_6240L Intel 6240L accera.Target.Model.INTEL_6240M Intel 6240M accera.Target.Model.INTEL_6240R Intel 6240R accera.Target.Model.INTEL_6240Y Intel 6240Y accera.Target.Model.INTEL_6242 Intel 6242 accera.Target.Model.INTEL_6242R Intel 6242R accera.Target.Model.INTEL_6244 Intel 6244 accera.Target.Model.INTEL_6246 Intel 6246 accera.Target.Model.INTEL_6246R Intel 6246R accera.Target.Model.INTEL_6248 Intel 6248 accera.Target.Model.INTEL_6248R Intel 6248R accera.Target.Model.INTEL_6252 Intel 6252 accera.Target.Model.INTEL_6252N Intel 6252N accera.Target.Model.INTEL_6254 Intel 6254 accera.Target.Model.INTEL_6258R Intel 6258R accera.Target.Model.INTEL_6262V Intel 6262V accera.Target.Model.INTEL_6300 Intel 6300 accera.Target.Model.INTEL_6300T Intel 6300T accera.Target.Model.INTEL_6320 Intel 6320 accera.Target.Model.INTEL_6400 Intel 6400 accera.Target.Model.INTEL_6400T Intel 6400T accera.Target.Model.INTEL_6402P Intel 6402P accera.Target.Model.INTEL_6500 Intel 6500 accera.Target.Model.INTEL_6500T Intel 6500T accera.Target.Model.INTEL_6585R Intel 6585R accera.Target.Model.INTEL_6600 Intel 6600 accera.Target.Model.INTEL_6600K Intel 6600K accera.Target.Model.INTEL_6600T Intel 6600T accera.Target.Model.INTEL_6685R Intel 6685R accera.Target.Model.INTEL_6700 Intel 6700 accera.Target.Model.INTEL_6700K Intel 6700K accera.Target.Model.INTEL_6700T Intel 6700T accera.Target.Model.INTEL_6785R Intel 6785R accera.Target.Model.INTEL_7100 Intel 7100 accera.Target.Model.INTEL_7100T Intel 7100T accera.Target.Model.INTEL_7101E Intel 7101E accera.Target.Model.INTEL_7101TE Intel 7101TE accera.Target.Model.INTEL_7300 Intel 7300 accera.Target.Model.INTEL_7300T Intel 7300T accera.Target.Model.INTEL_7320 Intel 7320 accera.Target.Model.INTEL_7350K Intel 7350K accera.Target.Model.INTEL_7400 Intel 7400 accera.Target.Model.INTEL_7400T Intel 7400T accera.Target.Model.INTEL_7500 Intel 7500 accera.Target.Model.INTEL_7500T Intel 7500T accera.Target.Model.INTEL_7600 Intel 7600 accera.Target.Model.INTEL_7600K Intel 7600K accera.Target.Model.INTEL_7600T Intel 7600T accera.Target.Model.INTEL_7640X Intel 7640X accera.Target.Model.INTEL_7700 Intel 7700 accera.Target.Model.INTEL_7700K Intel 7700K accera.Target.Model.INTEL_7700T Intel 7700T accera.Target.Model.INTEL_7740X Intel 7740X accera.Target.Model.INTEL_7800X Intel 7800X accera.Target.Model.INTEL_7820X Intel 7820X accera.Target.Model.INTEL_7900X Intel 7900X accera.Target.Model.INTEL_7920X Intel 7920X accera.Target.Model.INTEL_7940X Intel 7940X accera.Target.Model.INTEL_7960X Intel 7960X accera.Target.Model.INTEL_7980XE Intel 7980XE accera.Target.Model.INTEL_8086K Intel 8086K accera.Target.Model.INTEL_8100 Intel 8100 accera.Target.Model.INTEL_8100F Intel 8100F accera.Target.Model.INTEL_8100T Intel 8100T accera.Target.Model.INTEL_8253 Intel 8253 accera.Target.Model.INTEL_8256 Intel 8256 accera.Target.Model.INTEL_8260 Intel 8260 accera.Target.Model.INTEL_8260L Intel 8260L accera.Target.Model.INTEL_8260M Intel 8260M accera.Target.Model.INTEL_8260Y Intel 8260Y accera.Target.Model.INTEL_8268 Intel 8268 accera.Target.Model.INTEL_8270 Intel 8270 accera.Target.Model.INTEL_8276 Intel 8276 accera.Target.Model.INTEL_8276L Intel 8276L accera.Target.Model.INTEL_8276M Intel 8276M accera.Target.Model.INTEL_8280 Intel 8280 accera.Target.Model.INTEL_8280L Intel 8280L accera.Target.Model.INTEL_8280M Intel 8280M accera.Target.Model.INTEL_8284 Intel 8284 accera.Target.Model.INTEL_8300 Intel 8300 accera.Target.Model.INTEL_8300T Intel 8300T accera.Target.Model.INTEL_8350K Intel 8350K accera.Target.Model.INTEL_8351N Intel 8351N accera.Target.Model.INTEL_8352S Intel 8352S accera.Target.Model.INTEL_8352V Intel 8352V accera.Target.Model.INTEL_8352Y Intel 8352Y accera.Target.Model.INTEL_8358 Intel 8358 accera.Target.Model.INTEL_8358P Intel 8358P accera.Target.Model.INTEL_8360Y Intel 8360Y accera.Target.Model.INTEL_8362 Intel 8362 accera.Target.Model.INTEL_8368 Intel 8368 accera.Target.Model.INTEL_8368Q Intel 8368Q accera.Target.Model.INTEL_8380 Intel 8380 accera.Target.Model.INTEL_8400 Intel 8400 accera.Target.Model.INTEL_8400T Intel 8400T accera.Target.Model.INTEL_8500 Intel 8500 accera.Target.Model.INTEL_8500T Intel 8500T accera.Target.Model.INTEL_8600 Intel 8600 accera.Target.Model.INTEL_8600K Intel 8600K accera.Target.Model.INTEL_8600T Intel 8600T accera.Target.Model.INTEL_8700 Intel 8700 accera.Target.Model.INTEL_8700K Intel 8700K accera.Target.Model.INTEL_8700T Intel 8700T accera.Target.Model.INTEL_9221 Intel 9221 accera.Target.Model.INTEL_9222 Intel 9222 accera.Target.Model.INTEL_9242 Intel 9242 accera.Target.Model.INTEL_9282 Intel 9282 accera.Target.Model.INTEL_9800X Intel 9800X accera.Target.Model.INTEL_9820X Intel 9820X accera.Target.Model.INTEL_9900X Intel 9900X accera.Target.Model.INTEL_9920X Intel 9920X accera.Target.Model.INTEL_9940X Intel 9940X accera.Target.Model.INTEL_9960X Intel 9960X accera.Target.Model.INTEL_9980XE Intel 9980XE accera.Target.Model.INTEL_9990XE Intel 9990XE accera.Target.Model.INTEL_E3_1220_V6 Intel E3-1220 v6 accera.Target.Model.INTEL_E3_1225_V6 Intel E3-1225 v6 accera.Target.Model.INTEL_E3_1230_V6 Intel E3-1230 v6 accera.Target.Model.INTEL_E3_1240_V6 Intel E3-1240 v6 accera.Target.Model.INTEL_E3_1245_V6 Intel E3-1245 v6 accera.Target.Model.INTEL_E3_1270_V6 Intel E3-1270 v6 accera.Target.Model.INTEL_E3_1275_V6 Intel E3-1275 v6 accera.Target.Model.INTEL_E3_1280_V6 Intel E3-1280 v6 accera.Target.Model.INTEL_E3_1285_V6 Intel E3-1285 v6 accera.Target.Model.INTEL_G3900 Intel G3900 accera.Target.Model.INTEL_G3900T Intel G3900T accera.Target.Model.INTEL_G3900TE Intel G3900TE accera.Target.Model.INTEL_G3920 Intel G3920 accera.Target.Model.INTEL_G4400 Intel G4400 accera.Target.Model.INTEL_G4400T Intel G4400T accera.Target.Model.INTEL_G4400TE Intel G4400TE accera.Target.Model.INTEL_G4500 Intel G4500 accera.Target.Model.INTEL_G4500T Intel G4500T accera.Target.Model.INTEL_G4520 Intel G4520 accera.Target.Model.INTEL_W_3175X Intel W-3175X accera.Target.Model.INTEL_W_3223 Intel W-3223 accera.Target.Model.INTEL_W_3225 Intel W-3225 accera.Target.Model.INTEL_W_3235 Intel W-3235 accera.Target.Model.INTEL_W_3245 Intel W-3245 accera.Target.Model.INTEL_W_3245M Intel W-3245M accera.Target.Model.INTEL_W_3265 Intel W-3265 accera.Target.Model.INTEL_W_3265M Intel W-3265M accera.Target.Model.INTEL_W_3275 Intel W-3275 accera.Target.Model.INTEL_W_3275M Intel W-3275M accera.Target.Model.RASPBERRY_PI_3B Raspberry Pi 3B accera.Target.Model.RASPBERRY_PI_4B Raspberry Pi 4B accera.Target.Model.RASPBERRY_PI_ZERO Raspberry Pi Zero The enum also defines constants for some well-known GPU models. type description accera.Target.Model.NVIDIA_V100 NVidia V100","title":"Target.Model"},{"location":"Reference/classes/Target/Model/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Target/Model/#acceratargetmodel","text":"Defines constants for some well-known CPU models. type description accera.Target.Model.AMD_1200 AMD 1200 accera.Target.Model.AMD_1300X AMD 1300X accera.Target.Model.AMD_1400 AMD 1400 accera.Target.Model.AMD_1500X AMD 1500X accera.Target.Model.AMD_1600 AMD 1600 accera.Target.Model.AMD_1600X AMD 1600X accera.Target.Model.AMD_1700 AMD 1700 accera.Target.Model.AMD_1700X AMD 1700X accera.Target.Model.AMD_1800X AMD 1800X accera.Target.Model.AMD_1900X AMD 1900X accera.Target.Model.AMD_1920X AMD 1920X accera.Target.Model.AMD_1950X AMD 1950X accera.Target.Model.AMD_200GE AMD 200GE accera.Target.Model.AMD_2200G AMD 2200G accera.Target.Model.AMD_2200GE AMD 2200GE accera.Target.Model.AMD_2200U AMD 2200U accera.Target.Model.AMD_220GE AMD 220GE accera.Target.Model.AMD_2300U AMD 2300U accera.Target.Model.AMD_2300X AMD 2300X accera.Target.Model.AMD_2400G AMD 2400G accera.Target.Model.AMD_2400GE AMD 2400GE accera.Target.Model.AMD_240GE AMD 240GE accera.Target.Model.AMD_2500U AMD 2500U accera.Target.Model.AMD_2500X AMD 2500X accera.Target.Model.AMD_2600 AMD 2600 accera.Target.Model.AMD_2600E AMD 2600E accera.Target.Model.AMD_2600H AMD 2600H accera.Target.Model.AMD_2600X AMD 2600X accera.Target.Model.AMD_2700 AMD 2700 accera.Target.Model.AMD_2700E AMD 2700E accera.Target.Model.AMD_2700U AMD 2700U accera.Target.Model.AMD_2700X AMD 2700X accera.Target.Model.AMD_2700X_GOLD_EDITION AMD 2700X Gold Edition accera.Target.Model.AMD_2800H AMD 2800H accera.Target.Model.AMD_2920X AMD 2920X accera.Target.Model.AMD_2950X AMD 2950X accera.Target.Model.AMD_2970WX AMD 2970WX accera.Target.Model.AMD_2990WX AMD 2990WX accera.Target.Model.AMD_3000G AMD 3000G accera.Target.Model.AMD_300U AMD 300U accera.Target.Model.AMD_3050U AMD 3050U accera.Target.Model.AMD_3101 AMD 3101 accera.Target.Model.AMD_3150U AMD 3150U accera.Target.Model.AMD_3151 AMD 3151 accera.Target.Model.AMD_3200G AMD 3200G accera.Target.Model.AMD_3200U AMD 3200U accera.Target.Model.AMD_3201 AMD 3201 accera.Target.Model.AMD_3250U AMD 3250U accera.Target.Model.AMD_3251 AMD 3251 accera.Target.Model.AMD_3255 AMD 3255 accera.Target.Model.AMD_3300U AMD 3300U accera.Target.Model.AMD_3301 AMD 3301 accera.Target.Model.AMD_3351 AMD 3351 accera.Target.Model.AMD_3400G AMD 3400G accera.Target.Model.AMD_3401 AMD 3401 accera.Target.Model.AMD_3451 AMD 3451 accera.Target.Model.AMD_3500 AMD 3500 accera.Target.Model.AMD_3500U AMD 3500U accera.Target.Model.AMD_3500X AMD 3500X accera.Target.Model.AMD_3550H AMD 3550H accera.Target.Model.AMD_3580U AMD 3580U accera.Target.Model.AMD_3600 AMD 3600 accera.Target.Model.AMD_3600X AMD 3600X accera.Target.Model.AMD_3600XT AMD 3600XT accera.Target.Model.AMD_3700U AMD 3700U accera.Target.Model.AMD_3700X AMD 3700X accera.Target.Model.AMD_3750H AMD 3750H accera.Target.Model.AMD_3780U AMD 3780U accera.Target.Model.AMD_3800X AMD 3800X accera.Target.Model.AMD_3800XT AMD 3800XT accera.Target.Model.AMD_3900 AMD 3900 accera.Target.Model.AMD_3900X AMD 3900X accera.Target.Model.AMD_3900XT AMD 3900XT accera.Target.Model.AMD_3950X AMD 3950X accera.Target.Model.AMD_3960X AMD 3960X accera.Target.Model.AMD_3970X AMD 3970X accera.Target.Model.AMD_3980X AMD 3980X accera.Target.Model.AMD_3990X AMD 3990X accera.Target.Model.AMD_4300G AMD 4300G accera.Target.Model.AMD_4300GE AMD 4300GE accera.Target.Model.AMD_4300U AMD 4300U accera.Target.Model.AMD_4500U AMD 4500U accera.Target.Model.AMD_4600G AMD 4600G accera.Target.Model.AMD_4600GE AMD 4600GE accera.Target.Model.AMD_4600H AMD 4600H accera.Target.Model.AMD_4600HS AMD 4600HS accera.Target.Model.AMD_4600U AMD 4600U accera.Target.Model.AMD_4680U AMD 4680U accera.Target.Model.AMD_4700G AMD 4700G accera.Target.Model.AMD_4700GE AMD 4700GE accera.Target.Model.AMD_4700U AMD 4700U accera.Target.Model.AMD_4800H AMD 4800H accera.Target.Model.AMD_4800HS AMD 4800HS accera.Target.Model.AMD_4800U AMD 4800U accera.Target.Model.AMD_4900H AMD 4900H accera.Target.Model.AMD_4900HS AMD 4900HS accera.Target.Model.AMD_4980U AMD 4980U accera.Target.Model.AMD_5300G AMD 5300G accera.Target.Model.AMD_5300GE AMD 5300GE accera.Target.Model.AMD_5300U AMD 5300U accera.Target.Model.AMD_5400U AMD 5400U accera.Target.Model.AMD_5500U AMD 5500U accera.Target.Model.AMD_5600G AMD 5600G accera.Target.Model.AMD_5600GE AMD 5600GE accera.Target.Model.AMD_5600H AMD 5600H accera.Target.Model.AMD_5600HS AMD 5600HS accera.Target.Model.AMD_5600U AMD 5600U accera.Target.Model.AMD_5600X AMD 5600X accera.Target.Model.AMD_5700G AMD 5700G accera.Target.Model.AMD_5700GE AMD 5700GE accera.Target.Model.AMD_5700U AMD 5700U accera.Target.Model.AMD_5800 AMD 5800 accera.Target.Model.AMD_5800H AMD 5800H accera.Target.Model.AMD_5800HS AMD 5800HS accera.Target.Model.AMD_5800U AMD 5800U accera.Target.Model.AMD_5800X AMD 5800X accera.Target.Model.AMD_5900 AMD 5900 accera.Target.Model.AMD_5900HS AMD 5900HS accera.Target.Model.AMD_5900HX AMD 5900HX accera.Target.Model.AMD_5900X AMD 5900X accera.Target.Model.AMD_5950X AMD 5950X accera.Target.Model.AMD_5980HS AMD 5980HS accera.Target.Model.AMD_5980HX AMD 5980HX accera.Target.Model.AMD_7232P AMD 7232P accera.Target.Model.AMD_7251 AMD 7251 accera.Target.Model.AMD_7252 AMD 7252 accera.Target.Model.AMD_7261 AMD 7261 accera.Target.Model.AMD_7262 AMD 7262 accera.Target.Model.AMD_7272 AMD 7272 accera.Target.Model.AMD_7281 AMD 7281 accera.Target.Model.AMD_7282 AMD 7282 accera.Target.Model.AMD_72F3 AMD 72F3 accera.Target.Model.AMD_7301 AMD 7301 accera.Target.Model.AMD_7302 AMD 7302 accera.Target.Model.AMD_7302P AMD 7302P accera.Target.Model.AMD_7313 AMD 7313 accera.Target.Model.AMD_7313P AMD 7313P accera.Target.Model.AMD_7343 AMD 7343 accera.Target.Model.AMD_7351 AMD 7351 accera.Target.Model.AMD_7351P AMD 7351P accera.Target.Model.AMD_7352 AMD 7352 accera.Target.Model.AMD_7371 AMD 7371 accera.Target.Model.AMD_73F3 AMD 73F3 accera.Target.Model.AMD_7401 AMD 7401 accera.Target.Model.AMD_7401P AMD 7401P accera.Target.Model.AMD_7402 AMD 7402 accera.Target.Model.AMD_7402P AMD 7402P accera.Target.Model.AMD_7413 AMD 7413 accera.Target.Model.AMD_7443 AMD 7443 accera.Target.Model.AMD_7443P AMD 7443P accera.Target.Model.AMD_7451 AMD 7451 accera.Target.Model.AMD_7452 AMD 7452 accera.Target.Model.AMD_7453 AMD 7453 accera.Target.Model.AMD_74F3 AMD 74F3 accera.Target.Model.AMD_7501 AMD 7501 accera.Target.Model.AMD_7502 AMD 7502 accera.Target.Model.AMD_7502P AMD 7502P accera.Target.Model.AMD_7513 AMD 7513 accera.Target.Model.AMD_7532 AMD 7532 accera.Target.Model.AMD_7542 AMD 7542 accera.Target.Model.AMD_7543 AMD 7543 accera.Target.Model.AMD_7543P AMD 7543P accera.Target.Model.AMD_7551 AMD 7551 accera.Target.Model.AMD_7551P AMD 7551P accera.Target.Model.AMD_7552 AMD 7552 accera.Target.Model.AMD_75F3 AMD 75F3 accera.Target.Model.AMD_7601 AMD 7601 accera.Target.Model.AMD_7642 AMD 7642 accera.Target.Model.AMD_7643 AMD 7643 accera.Target.Model.AMD_7662 AMD 7662 accera.Target.Model.AMD_7663 AMD 7663 accera.Target.Model.AMD_7702 AMD 7702 accera.Target.Model.AMD_7702P AMD 7702P accera.Target.Model.AMD_7713 AMD 7713 accera.Target.Model.AMD_7713P AMD 7713P accera.Target.Model.AMD_7742 AMD 7742 accera.Target.Model.AMD_7763 AMD 7763 accera.Target.Model.AMD_7F32 AMD 7F32 accera.Target.Model.AMD_7F52 AMD 7F52 accera.Target.Model.AMD_7F72 AMD 7F72 accera.Target.Model.AMD_7H12 AMD 7H12 accera.Target.Model.AMD_FIREFLIGHT AMD FireFlight accera.Target.Model.AMD_PRO_1200 AMD PRO 1200 accera.Target.Model.AMD_PRO_1300 AMD PRO 1300 accera.Target.Model.AMD_PRO_1500 AMD PRO 1500 accera.Target.Model.AMD_PRO_1600 AMD PRO 1600 accera.Target.Model.AMD_PRO_1700 AMD PRO 1700 accera.Target.Model.AMD_PRO_1700X AMD PRO 1700X accera.Target.Model.AMD_PRO_200GE AMD PRO 200GE accera.Target.Model.AMD_PRO_2200G AMD PRO 2200G accera.Target.Model.AMD_PRO_2200GE AMD PRO 2200GE accera.Target.Model.AMD_PRO_2300U AMD PRO 2300U accera.Target.Model.AMD_PRO_2400G AMD PRO 2400G accera.Target.Model.AMD_PRO_2400GE AMD PRO 2400GE accera.Target.Model.AMD_PRO_2500U AMD PRO 2500U accera.Target.Model.AMD_PRO_2600 AMD PRO 2600 accera.Target.Model.AMD_PRO_2700 AMD PRO 2700 accera.Target.Model.AMD_PRO_2700U AMD PRO 2700U accera.Target.Model.AMD_PRO_2700X AMD PRO 2700X accera.Target.Model.AMD_PRO_300GE AMD PRO 300GE accera.Target.Model.AMD_PRO_300U AMD PRO 300U accera.Target.Model.AMD_PRO_3200G AMD PRO 3200G accera.Target.Model.AMD_PRO_3200GE AMD PRO 3200GE accera.Target.Model.AMD_PRO_3300U AMD PRO 3300U accera.Target.Model.AMD_PRO_3400G AMD PRO 3400G accera.Target.Model.AMD_PRO_3400GE AMD PRO 3400GE accera.Target.Model.AMD_PRO_3500U AMD PRO 3500U accera.Target.Model.AMD_PRO_3600 AMD PRO 3600 accera.Target.Model.AMD_PRO_3700 AMD PRO 3700 accera.Target.Model.AMD_PRO_3700U AMD PRO 3700U accera.Target.Model.AMD_PRO_3900 AMD PRO 3900 accera.Target.Model.AMD_PRO_4350G AMD PRO 4350G accera.Target.Model.AMD_PRO_4350GE AMD PRO 4350GE accera.Target.Model.AMD_PRO_4450U AMD PRO 4450U accera.Target.Model.AMD_PRO_4650G AMD PRO 4650G accera.Target.Model.AMD_PRO_4650GE AMD PRO 4650GE accera.Target.Model.AMD_PRO_4650U AMD PRO 4650U accera.Target.Model.AMD_PRO_4750G AMD PRO 4750G accera.Target.Model.AMD_PRO_4750GE AMD PRO 4750GE accera.Target.Model.AMD_PRO_4750U AMD PRO 4750U accera.Target.Model.AMD_PRO_5350G AMD PRO 5350G accera.Target.Model.AMD_PRO_5350GE AMD PRO 5350GE accera.Target.Model.AMD_PRO_5450U AMD PRO 5450U accera.Target.Model.AMD_PRO_5650G AMD PRO 5650G accera.Target.Model.AMD_PRO_5650GE AMD PRO 5650GE accera.Target.Model.AMD_PRO_5650U AMD PRO 5650U accera.Target.Model.AMD_PRO_5750G AMD PRO 5750G accera.Target.Model.AMD_PRO_5750GE AMD PRO 5750GE accera.Target.Model.AMD_PRO_5850U AMD PRO 5850U accera.Target.Model.AMD_R1102G AMD R1102G accera.Target.Model.AMD_R1305G AMD R1305G accera.Target.Model.AMD_R1505G AMD R1505G accera.Target.Model.AMD_R1606G AMD R1606G accera.Target.Model.AMD_V1202B AMD V1202B accera.Target.Model.AMD_V1404I AMD V1404I accera.Target.Model.AMD_V1500B AMD V1500B accera.Target.Model.AMD_V1605B AMD V1605B accera.Target.Model.AMD_V1756B AMD V1756B accera.Target.Model.AMD_V1780B AMD V1780B accera.Target.Model.AMD_V1807B AMD V1807B accera.Target.Model.AMD_V2516 AMD V2516 accera.Target.Model.AMD_V2546 AMD V2546 accera.Target.Model.AMD_V2718 AMD V2718 accera.Target.Model.AMD_V2748 AMD V2748 accera.Target.Model.INTEL_1000G1 Intel 1000G1 accera.Target.Model.INTEL_1000G4 Intel 1000G4 accera.Target.Model.INTEL_1005G1 Intel 1005G1 accera.Target.Model.INTEL_10100 Intel 10100 accera.Target.Model.INTEL_10100F Intel 10100F accera.Target.Model.INTEL_10100T Intel 10100T accera.Target.Model.INTEL_10300 Intel 10300 accera.Target.Model.INTEL_10300T Intel 10300T accera.Target.Model.INTEL_1030G4 Intel 1030G4 accera.Target.Model.INTEL_1030G7 Intel 1030G7 accera.Target.Model.INTEL_10320 Intel 10320 accera.Target.Model.INTEL_1035G1 Intel 1035G1 accera.Target.Model.INTEL_1035G4 Intel 1035G4 accera.Target.Model.INTEL_1035G7 Intel 1035G7 accera.Target.Model.INTEL_10400 Intel 10400 accera.Target.Model.INTEL_10400F Intel 10400F accera.Target.Model.INTEL_10400T Intel 10400T accera.Target.Model.INTEL_10500 Intel 10500 accera.Target.Model.INTEL_10500T Intel 10500T accera.Target.Model.INTEL_10600 Intel 10600 accera.Target.Model.INTEL_10600K Intel 10600K accera.Target.Model.INTEL_10600KF Intel 10600KF accera.Target.Model.INTEL_10600T Intel 10600T accera.Target.Model.INTEL_1060G7 Intel 1060G7 accera.Target.Model.INTEL_1065G7 Intel 1065G7 accera.Target.Model.INTEL_1068G7 Intel 1068G7 accera.Target.Model.INTEL_10700 Intel 10700 accera.Target.Model.INTEL_10700F Intel 10700F accera.Target.Model.INTEL_10700K Intel 10700K accera.Target.Model.INTEL_10700KF Intel 10700KF accera.Target.Model.INTEL_10700T Intel 10700T accera.Target.Model.INTEL_10850K Intel 10850K accera.Target.Model.INTEL_10900 Intel 10900 accera.Target.Model.INTEL_10900F Intel 10900F accera.Target.Model.INTEL_10900K Intel 10900K accera.Target.Model.INTEL_10900KF Intel 10900KF accera.Target.Model.INTEL_10900T Intel 10900T accera.Target.Model.INTEL_10910 Intel 10910 accera.Target.Model.INTEL_11400 Intel 11400 accera.Target.Model.INTEL_11400F Intel 11400F accera.Target.Model.INTEL_11400T Intel 11400T accera.Target.Model.INTEL_11500 Intel 11500 accera.Target.Model.INTEL_11500T Intel 11500T accera.Target.Model.INTEL_11600 Intel 11600 accera.Target.Model.INTEL_11600K Intel 11600K accera.Target.Model.INTEL_11600KF Intel 11600KF accera.Target.Model.INTEL_11600T Intel 11600T accera.Target.Model.INTEL_11700 Intel 11700 accera.Target.Model.INTEL_11700F Intel 11700F accera.Target.Model.INTEL_11700K Intel 11700K accera.Target.Model.INTEL_11700KF Intel 11700KF accera.Target.Model.INTEL_11700T Intel 11700T accera.Target.Model.INTEL_11900 Intel 11900 accera.Target.Model.INTEL_11900F Intel 11900F accera.Target.Model.INTEL_11900K Intel 11900K accera.Target.Model.INTEL_11900KF Intel 11900KF accera.Target.Model.INTEL_11900T Intel 11900T accera.Target.Model.INTEL_1250 Intel 1250 accera.Target.Model.INTEL_1250P Intel 1250P accera.Target.Model.INTEL_1270 Intel 1270 accera.Target.Model.INTEL_1270P Intel 1270P accera.Target.Model.INTEL_1290 Intel 1290 accera.Target.Model.INTEL_1290P Intel 1290P accera.Target.Model.INTEL_1290T Intel 1290T accera.Target.Model.INTEL_1350 Intel 1350 accera.Target.Model.INTEL_1350P Intel 1350P accera.Target.Model.INTEL_1370 Intel 1370 accera.Target.Model.INTEL_1370P Intel 1370P accera.Target.Model.INTEL_1390 Intel 1390 accera.Target.Model.INTEL_1390P Intel 1390P accera.Target.Model.INTEL_1390T Intel 1390T accera.Target.Model.INTEL_2104G Intel 2104G accera.Target.Model.INTEL_2124 Intel 2124 accera.Target.Model.INTEL_2124G Intel 2124G accera.Target.Model.INTEL_2126G Intel 2126G accera.Target.Model.INTEL_2134 Intel 2134 accera.Target.Model.INTEL_2136 Intel 2136 accera.Target.Model.INTEL_2144G Intel 2144G accera.Target.Model.INTEL_2146G Intel 2146G accera.Target.Model.INTEL_2174G Intel 2174G accera.Target.Model.INTEL_2176G Intel 2176G accera.Target.Model.INTEL_2186G Intel 2186G accera.Target.Model.INTEL_2314 Intel 2314 accera.Target.Model.INTEL_2324G Intel 2324G accera.Target.Model.INTEL_2334 Intel 2334 accera.Target.Model.INTEL_2336 Intel 2336 accera.Target.Model.INTEL_2356G Intel 2356G accera.Target.Model.INTEL_2374G Intel 2374G accera.Target.Model.INTEL_2378 Intel 2378 accera.Target.Model.INTEL_2378G Intel 2378G accera.Target.Model.INTEL_2386G Intel 2386G accera.Target.Model.INTEL_2388G Intel 2388G accera.Target.Model.INTEL_3204 Intel 3204 accera.Target.Model.INTEL_4208 Intel 4208 accera.Target.Model.INTEL_4209T Intel 4209T accera.Target.Model.INTEL_4210 Intel 4210 accera.Target.Model.INTEL_4210R Intel 4210R accera.Target.Model.INTEL_4214 Intel 4214 accera.Target.Model.INTEL_4214R Intel 4214R accera.Target.Model.INTEL_4214Y Intel 4214Y accera.Target.Model.INTEL_4215 Intel 4215 accera.Target.Model.INTEL_4215R Intel 4215R accera.Target.Model.INTEL_4216 Intel 4216 accera.Target.Model.INTEL_5215 Intel 5215 accera.Target.Model.INTEL_5215L Intel 5215L accera.Target.Model.INTEL_5215M Intel 5215M accera.Target.Model.INTEL_5217 Intel 5217 accera.Target.Model.INTEL_5218 Intel 5218 accera.Target.Model.INTEL_5218B Intel 5218B accera.Target.Model.INTEL_5218N Intel 5218N accera.Target.Model.INTEL_5218R Intel 5218R accera.Target.Model.INTEL_5218T Intel 5218T accera.Target.Model.INTEL_5220 Intel 5220 accera.Target.Model.INTEL_5220R Intel 5220R accera.Target.Model.INTEL_5220S Intel 5220S accera.Target.Model.INTEL_5220T Intel 5220T accera.Target.Model.INTEL_5222 Intel 5222 accera.Target.Model.INTEL_6098P Intel 6098P accera.Target.Model.INTEL_6100 Intel 6100 accera.Target.Model.INTEL_6100T Intel 6100T accera.Target.Model.INTEL_6209U Intel 6209U accera.Target.Model.INTEL_6210U Intel 6210U accera.Target.Model.INTEL_6212U Intel 6212U accera.Target.Model.INTEL_6222V Intel 6222V accera.Target.Model.INTEL_6226 Intel 6226 accera.Target.Model.INTEL_6226R Intel 6226R accera.Target.Model.INTEL_6230 Intel 6230 accera.Target.Model.INTEL_6230N Intel 6230N accera.Target.Model.INTEL_6230R Intel 6230R accera.Target.Model.INTEL_6230T Intel 6230T accera.Target.Model.INTEL_6234 Intel 6234 accera.Target.Model.INTEL_6238 Intel 6238 accera.Target.Model.INTEL_6238L Intel 6238L accera.Target.Model.INTEL_6238M Intel 6238M accera.Target.Model.INTEL_6238R Intel 6238R accera.Target.Model.INTEL_6238T Intel 6238T accera.Target.Model.INTEL_6240 Intel 6240 accera.Target.Model.INTEL_6240L Intel 6240L accera.Target.Model.INTEL_6240M Intel 6240M accera.Target.Model.INTEL_6240R Intel 6240R accera.Target.Model.INTEL_6240Y Intel 6240Y accera.Target.Model.INTEL_6242 Intel 6242 accera.Target.Model.INTEL_6242R Intel 6242R accera.Target.Model.INTEL_6244 Intel 6244 accera.Target.Model.INTEL_6246 Intel 6246 accera.Target.Model.INTEL_6246R Intel 6246R accera.Target.Model.INTEL_6248 Intel 6248 accera.Target.Model.INTEL_6248R Intel 6248R accera.Target.Model.INTEL_6252 Intel 6252 accera.Target.Model.INTEL_6252N Intel 6252N accera.Target.Model.INTEL_6254 Intel 6254 accera.Target.Model.INTEL_6258R Intel 6258R accera.Target.Model.INTEL_6262V Intel 6262V accera.Target.Model.INTEL_6300 Intel 6300 accera.Target.Model.INTEL_6300T Intel 6300T accera.Target.Model.INTEL_6320 Intel 6320 accera.Target.Model.INTEL_6400 Intel 6400 accera.Target.Model.INTEL_6400T Intel 6400T accera.Target.Model.INTEL_6402P Intel 6402P accera.Target.Model.INTEL_6500 Intel 6500 accera.Target.Model.INTEL_6500T Intel 6500T accera.Target.Model.INTEL_6585R Intel 6585R accera.Target.Model.INTEL_6600 Intel 6600 accera.Target.Model.INTEL_6600K Intel 6600K accera.Target.Model.INTEL_6600T Intel 6600T accera.Target.Model.INTEL_6685R Intel 6685R accera.Target.Model.INTEL_6700 Intel 6700 accera.Target.Model.INTEL_6700K Intel 6700K accera.Target.Model.INTEL_6700T Intel 6700T accera.Target.Model.INTEL_6785R Intel 6785R accera.Target.Model.INTEL_7100 Intel 7100 accera.Target.Model.INTEL_7100T Intel 7100T accera.Target.Model.INTEL_7101E Intel 7101E accera.Target.Model.INTEL_7101TE Intel 7101TE accera.Target.Model.INTEL_7300 Intel 7300 accera.Target.Model.INTEL_7300T Intel 7300T accera.Target.Model.INTEL_7320 Intel 7320 accera.Target.Model.INTEL_7350K Intel 7350K accera.Target.Model.INTEL_7400 Intel 7400 accera.Target.Model.INTEL_7400T Intel 7400T accera.Target.Model.INTEL_7500 Intel 7500 accera.Target.Model.INTEL_7500T Intel 7500T accera.Target.Model.INTEL_7600 Intel 7600 accera.Target.Model.INTEL_7600K Intel 7600K accera.Target.Model.INTEL_7600T Intel 7600T accera.Target.Model.INTEL_7640X Intel 7640X accera.Target.Model.INTEL_7700 Intel 7700 accera.Target.Model.INTEL_7700K Intel 7700K accera.Target.Model.INTEL_7700T Intel 7700T accera.Target.Model.INTEL_7740X Intel 7740X accera.Target.Model.INTEL_7800X Intel 7800X accera.Target.Model.INTEL_7820X Intel 7820X accera.Target.Model.INTEL_7900X Intel 7900X accera.Target.Model.INTEL_7920X Intel 7920X accera.Target.Model.INTEL_7940X Intel 7940X accera.Target.Model.INTEL_7960X Intel 7960X accera.Target.Model.INTEL_7980XE Intel 7980XE accera.Target.Model.INTEL_8086K Intel 8086K accera.Target.Model.INTEL_8100 Intel 8100 accera.Target.Model.INTEL_8100F Intel 8100F accera.Target.Model.INTEL_8100T Intel 8100T accera.Target.Model.INTEL_8253 Intel 8253 accera.Target.Model.INTEL_8256 Intel 8256 accera.Target.Model.INTEL_8260 Intel 8260 accera.Target.Model.INTEL_8260L Intel 8260L accera.Target.Model.INTEL_8260M Intel 8260M accera.Target.Model.INTEL_8260Y Intel 8260Y accera.Target.Model.INTEL_8268 Intel 8268 accera.Target.Model.INTEL_8270 Intel 8270 accera.Target.Model.INTEL_8276 Intel 8276 accera.Target.Model.INTEL_8276L Intel 8276L accera.Target.Model.INTEL_8276M Intel 8276M accera.Target.Model.INTEL_8280 Intel 8280 accera.Target.Model.INTEL_8280L Intel 8280L accera.Target.Model.INTEL_8280M Intel 8280M accera.Target.Model.INTEL_8284 Intel 8284 accera.Target.Model.INTEL_8300 Intel 8300 accera.Target.Model.INTEL_8300T Intel 8300T accera.Target.Model.INTEL_8350K Intel 8350K accera.Target.Model.INTEL_8351N Intel 8351N accera.Target.Model.INTEL_8352S Intel 8352S accera.Target.Model.INTEL_8352V Intel 8352V accera.Target.Model.INTEL_8352Y Intel 8352Y accera.Target.Model.INTEL_8358 Intel 8358 accera.Target.Model.INTEL_8358P Intel 8358P accera.Target.Model.INTEL_8360Y Intel 8360Y accera.Target.Model.INTEL_8362 Intel 8362 accera.Target.Model.INTEL_8368 Intel 8368 accera.Target.Model.INTEL_8368Q Intel 8368Q accera.Target.Model.INTEL_8380 Intel 8380 accera.Target.Model.INTEL_8400 Intel 8400 accera.Target.Model.INTEL_8400T Intel 8400T accera.Target.Model.INTEL_8500 Intel 8500 accera.Target.Model.INTEL_8500T Intel 8500T accera.Target.Model.INTEL_8600 Intel 8600 accera.Target.Model.INTEL_8600K Intel 8600K accera.Target.Model.INTEL_8600T Intel 8600T accera.Target.Model.INTEL_8700 Intel 8700 accera.Target.Model.INTEL_8700K Intel 8700K accera.Target.Model.INTEL_8700T Intel 8700T accera.Target.Model.INTEL_9221 Intel 9221 accera.Target.Model.INTEL_9222 Intel 9222 accera.Target.Model.INTEL_9242 Intel 9242 accera.Target.Model.INTEL_9282 Intel 9282 accera.Target.Model.INTEL_9800X Intel 9800X accera.Target.Model.INTEL_9820X Intel 9820X accera.Target.Model.INTEL_9900X Intel 9900X accera.Target.Model.INTEL_9920X Intel 9920X accera.Target.Model.INTEL_9940X Intel 9940X accera.Target.Model.INTEL_9960X Intel 9960X accera.Target.Model.INTEL_9980XE Intel 9980XE accera.Target.Model.INTEL_9990XE Intel 9990XE accera.Target.Model.INTEL_E3_1220_V6 Intel E3-1220 v6 accera.Target.Model.INTEL_E3_1225_V6 Intel E3-1225 v6 accera.Target.Model.INTEL_E3_1230_V6 Intel E3-1230 v6 accera.Target.Model.INTEL_E3_1240_V6 Intel E3-1240 v6 accera.Target.Model.INTEL_E3_1245_V6 Intel E3-1245 v6 accera.Target.Model.INTEL_E3_1270_V6 Intel E3-1270 v6 accera.Target.Model.INTEL_E3_1275_V6 Intel E3-1275 v6 accera.Target.Model.INTEL_E3_1280_V6 Intel E3-1280 v6 accera.Target.Model.INTEL_E3_1285_V6 Intel E3-1285 v6 accera.Target.Model.INTEL_G3900 Intel G3900 accera.Target.Model.INTEL_G3900T Intel G3900T accera.Target.Model.INTEL_G3900TE Intel G3900TE accera.Target.Model.INTEL_G3920 Intel G3920 accera.Target.Model.INTEL_G4400 Intel G4400 accera.Target.Model.INTEL_G4400T Intel G4400T accera.Target.Model.INTEL_G4400TE Intel G4400TE accera.Target.Model.INTEL_G4500 Intel G4500 accera.Target.Model.INTEL_G4500T Intel G4500T accera.Target.Model.INTEL_G4520 Intel G4520 accera.Target.Model.INTEL_W_3175X Intel W-3175X accera.Target.Model.INTEL_W_3223 Intel W-3223 accera.Target.Model.INTEL_W_3225 Intel W-3225 accera.Target.Model.INTEL_W_3235 Intel W-3235 accera.Target.Model.INTEL_W_3245 Intel W-3245 accera.Target.Model.INTEL_W_3245M Intel W-3245M accera.Target.Model.INTEL_W_3265 Intel W-3265 accera.Target.Model.INTEL_W_3265M Intel W-3265M accera.Target.Model.INTEL_W_3275 Intel W-3275 accera.Target.Model.INTEL_W_3275M Intel W-3275M accera.Target.Model.RASPBERRY_PI_3B Raspberry Pi 3B accera.Target.Model.RASPBERRY_PI_4B Raspberry Pi 4B accera.Target.Model.RASPBERRY_PI_ZERO Raspberry Pi Zero The enum also defines constants for some well-known GPU models. type description accera.Target.Model.NVIDIA_V100 NVidia V100","title":"accera.Target.Model"},{"location":"Reference/classes/Target/Target/","text":"Accera v1.2.1 Reference accera.Target([architecture, cache_lines, cache_sizes, category, extensions, family, frequency_GHz, model, name, num_cores, num_threads, turbo_frequency_GHz]) Defines the capabilities of a target processor. Arguments argument description type/default known_name A name of a device known to Accera string | accera.Target.Model / \"HOST\" architecture The processor architecture accera.Target.Architecture cache_lines Cache lines (kilobytes) list of positive integers cache_sizes Cache sizes (bytes) list of positive integers category The processor category accera.Target.Category extensions Supported processor extensions list of extension codes family The processor family string frequency_GHz The processor frequency (GHz) positive number model The processor model accera.Target.Model name The processor name string num_cores Number of cores positive integer num_threads Number of threads positive integer turbo_frequency_GHz Turbo frequency (GHz) positive number vector_bytes Bytes per vector register positive number vector_registers total number of SIMD registers positive number Known device names Accera provides a pre-defined list of known target names through the accera.Target.Models enumeration. These known targets provide typical hardware settings and may not fit exactly to your specific hardware characteristics. If your target matches closely with (but not exactly to) one of these targets, you can always start with a known target and update the properties accordingly. Examples Let's take a look at some examples to understand how to define a CPU target in Accera. Create a custom CPU target: cpu_target = acc . Target ( name = \"Custom processor\" , category = acc . Target . Category . CPU , architecture = acc . Target . Architecture . X86_64 , num_cores = 10 ) We further create a known CPU target and can selectively override fields gen10 = acc . Target ( known_name = \"Intel 7940X\" , category = acc . Target . Category . CPU , extensions = [ \"SSE4.1\" , \"SSE4.2\" , \"AVX2\" ]) In this example, we created a target device of a known CPU, but overrode the extensions to remove AVX512 support. You can use this example as a starting point to define any other Intel Core Processor and the specifications of them are listed in the table above. Craete a pre-defined GPU target representing an NVidia Tesla v100 processor: v100 = acc . Target ( model = acc . Target . Model . NVIDIA_TESLA_V100 ) Here is another example to create a custom GPU target: gpu_target = acc . Target ( name = \"Custom GPU processor\" , category = acc . Target . Category . GPU , default_block_size = 16 ) Additional Notes on Instruction Set Extensions The details on extensions are important to identify the number of vector registers and vector bytes of each SIMD register supported by a processor. These values may help you determine if you are leveraging the vector units of an underlying hardware to its best capabilities. AVX Advanced Vector Extensions (AVX) promotes legacy 128-bit SIMD instructions that operate on XMM registers to use a vector-extension (VEX) prefix and operate on 256-bit YMM registers. Intel AVX introduced support for 256-bit wide SIMD registers (YMM0-YMM7 in operating modes that are 32-bit or less, YMM0-YMM15 in 64-bit mode). For Accera, 64-bit mode is the default and we do not add this as an argument to define a target. The lower 128-bits of the YMM registers are aliased to the respective 128-bit XMM registers. In Intel AVX, there are 256-bit wide vector registers, 16 XMM registers and 16 YMM registers to support an extension of 128-bits. AVX512 AVX-512 is a further extension offering 32 ZMM registers and each SIMD register is 512 bits (64 bytes) wide. SSE4 Extension There are 16 XMM registers (XMM0 to XMM15) and each is 128-bit wide. In 64-bit mode, eight additional XMM registers are accessible. Registers XMM8-XMM15 are accessed by using REX prefixes.","title":"Target"},{"location":"Reference/classes/Target/Target/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/classes/Target/Target/#acceratargetarchitecture-cache_lines-cache_sizes-category-extensions-family-frequency_ghz-model-name-num_cores-num_threads-turbo_frequency_ghz","text":"Defines the capabilities of a target processor.","title":"accera.Target([architecture, cache_lines, cache_sizes, category, extensions, family, frequency_GHz, model, name, num_cores, num_threads, turbo_frequency_GHz])"},{"location":"Reference/classes/Target/Target/#arguments","text":"argument description type/default known_name A name of a device known to Accera string | accera.Target.Model / \"HOST\" architecture The processor architecture accera.Target.Architecture cache_lines Cache lines (kilobytes) list of positive integers cache_sizes Cache sizes (bytes) list of positive integers category The processor category accera.Target.Category extensions Supported processor extensions list of extension codes family The processor family string frequency_GHz The processor frequency (GHz) positive number model The processor model accera.Target.Model name The processor name string num_cores Number of cores positive integer num_threads Number of threads positive integer turbo_frequency_GHz Turbo frequency (GHz) positive number vector_bytes Bytes per vector register positive number vector_registers total number of SIMD registers positive number","title":"Arguments"},{"location":"Reference/classes/Target/Target/#known-device-names","text":"Accera provides a pre-defined list of known target names through the accera.Target.Models enumeration. These known targets provide typical hardware settings and may not fit exactly to your specific hardware characteristics. If your target matches closely with (but not exactly to) one of these targets, you can always start with a known target and update the properties accordingly.","title":"Known device names"},{"location":"Reference/classes/Target/Target/#examples","text":"Let's take a look at some examples to understand how to define a CPU target in Accera. Create a custom CPU target: cpu_target = acc . Target ( name = \"Custom processor\" , category = acc . Target . Category . CPU , architecture = acc . Target . Architecture . X86_64 , num_cores = 10 ) We further create a known CPU target and can selectively override fields gen10 = acc . Target ( known_name = \"Intel 7940X\" , category = acc . Target . Category . CPU , extensions = [ \"SSE4.1\" , \"SSE4.2\" , \"AVX2\" ]) In this example, we created a target device of a known CPU, but overrode the extensions to remove AVX512 support. You can use this example as a starting point to define any other Intel Core Processor and the specifications of them are listed in the table above. Craete a pre-defined GPU target representing an NVidia Tesla v100 processor: v100 = acc . Target ( model = acc . Target . Model . NVIDIA_TESLA_V100 ) Here is another example to create a custom GPU target: gpu_target = acc . Target ( name = \"Custom GPU processor\" , category = acc . Target . Category . GPU , default_block_size = 16 )","title":"Examples"},{"location":"Reference/classes/Target/Target/#additional-notes-on-instruction-set-extensions","text":"The details on extensions are important to identify the number of vector registers and vector bytes of each SIMD register supported by a processor. These values may help you determine if you are leveraging the vector units of an underlying hardware to its best capabilities.","title":"Additional Notes on Instruction Set Extensions"},{"location":"Reference/classes/Target/Target/#avx","text":"Advanced Vector Extensions (AVX) promotes legacy 128-bit SIMD instructions that operate on XMM registers to use a vector-extension (VEX) prefix and operate on 256-bit YMM registers. Intel AVX introduced support for 256-bit wide SIMD registers (YMM0-YMM7 in operating modes that are 32-bit or less, YMM0-YMM15 in 64-bit mode). For Accera, 64-bit mode is the default and we do not add this as an argument to define a target. The lower 128-bits of the YMM registers are aliased to the respective 128-bit XMM registers. In Intel AVX, there are 256-bit wide vector registers, 16 XMM registers and 16 YMM registers to support an extension of 128-bits.","title":"AVX"},{"location":"Reference/classes/Target/Target/#avx512","text":"AVX-512 is a further extension offering 32 ZMM registers and each SIMD register is 512 bits (64 bytes) wide.","title":"AVX512"},{"location":"Reference/classes/Target/Target/#sse4-extension","text":"There are 16 XMM registers (XMM0 to XMM15) and each is 128-bit wide. In 64-bit mode, eight additional XMM registers are accessible. Registers XMM8-XMM15 are accessed by using REX prefixes.","title":"SSE4 Extension"},{"location":"Reference/enumerations/ScalarType/","text":"Accera v1.2.1 Reference accera.ScalarType type description accera.ScalarType.bool boolean accera.ScalarType.float32 32-bit floating point number accera.ScalarType.float64 64-bit floating point number accera.ScalarType.int8 8-bit signed integer accera.ScalarType.int16 16-bit signed integer accera.ScalarType.int32 32-bit signed integer accera.ScalarType.int64 64-bit signed integer TODO: unsigned integer types (uint8/16/32/64)","title":"ScalarType"},{"location":"Reference/enumerations/ScalarType/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/enumerations/ScalarType/#accerascalartype","text":"type description accera.ScalarType.bool boolean accera.ScalarType.float32 32-bit floating point number accera.ScalarType.float64 64-bit floating point number accera.ScalarType.int8 8-bit signed integer accera.ScalarType.int16 16-bit signed integer accera.ScalarType.int32 32-bit signed integer accera.ScalarType.int64 64-bit signed integer TODO: unsigned integer types (uint8/16/32/64)","title":"accera.ScalarType"},{"location":"Reference/functions/create_parameters/","text":"Accera v1.2.1 Reference accera.create_parameters(number) Creates placeholder parameters. Arguments argument description type/default number number of parameters to create positive integer Returns Tuple of Parameter Examples Create 3 parameters m , n , k and use them to parameterize the nest shape: m , n , k = acc . create_parameters ( 3 ) nest = acc . Nest ( shape = ( m , n , k ))","title":"create_parameters"},{"location":"Reference/functions/create_parameters/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/functions/create_parameters/#acceracreate_parametersnumber","text":"Creates placeholder parameters.","title":"accera.create_parameters(number)"},{"location":"Reference/functions/create_parameters/#arguments","text":"argument description type/default number number of parameters to create positive integer","title":"Arguments"},{"location":"Reference/functions/create_parameters/#returns","text":"Tuple of Parameter","title":"Returns"},{"location":"Reference/functions/create_parameters/#examples","text":"Create 3 parameters m , n , k and use them to parameterize the nest shape: m , n , k = acc . create_parameters ( 3 ) nest = acc . Nest ( shape = ( m , n , k ))","title":"Examples"},{"location":"Reference/functions/fuse/","text":"Accera v1.2.1 Reference accera.fuse(schedules[, *args, partial]) The fuse operation combines multiple iteration spaces into a single \"fused\" iteration space. The fused iteration space represents the union of the work in the original spaces. In cases where it doesn't make sense to fuse all of the iteration space dimensions, we can choose to fuse a prefix of the dimensions and leave the rest unfused. Arguments argument description type/default schedules Either the schedules to fuse if performing partial fusing, or the first schedule to fuse if fusing all dimensions tuple of Schedule or Schedule *args Optional variable arguments containing subsequent schedules to fuse variable Schedule arguments partial The number of dimensions to fuse. If not specified, all dimensions will be fused non-negative integer Returns The fused Schedule Examples Full fusing of same-shaped iteration spaces: # Fuse all dimensions of schedule0 and schedule1 schedule = acc . fuse ( schedule0 , schedule1 ) f , i , j = schedule . get_indices () # Reorder the indices so that the fused dimension is the innermost schedule . reorder ( i , j , f ) Partial iteration space fusing: # Fuse the first two dimensions of schedule0 and schedule1 schedule = acc . fuse (( schedule0 , schedule1 ), partial = 2 ) f , i , j , k = schedule . get_indices () # Reorder the indices to interleave the schedules schedule . reorder ( i , j , f , k )","title":"fuse"},{"location":"Reference/functions/fuse/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/functions/fuse/#accerafuseschedules-args-partial","text":"The fuse operation combines multiple iteration spaces into a single \"fused\" iteration space. The fused iteration space represents the union of the work in the original spaces. In cases where it doesn't make sense to fuse all of the iteration space dimensions, we can choose to fuse a prefix of the dimensions and leave the rest unfused.","title":"accera.fuse(schedules[, *args, partial])"},{"location":"Reference/functions/fuse/#arguments","text":"argument description type/default schedules Either the schedules to fuse if performing partial fusing, or the first schedule to fuse if fusing all dimensions tuple of Schedule or Schedule *args Optional variable arguments containing subsequent schedules to fuse variable Schedule arguments partial The number of dimensions to fuse. If not specified, all dimensions will be fused non-negative integer","title":"Arguments"},{"location":"Reference/functions/fuse/#returns","text":"The fused Schedule","title":"Returns"},{"location":"Reference/functions/fuse/#examples","text":"Full fusing of same-shaped iteration spaces: # Fuse all dimensions of schedule0 and schedule1 schedule = acc . fuse ( schedule0 , schedule1 ) f , i , j = schedule . get_indices () # Reorder the indices so that the fused dimension is the innermost schedule . reorder ( i , j , f ) Partial iteration space fusing: # Fuse the first two dimensions of schedule0 and schedule1 schedule = acc . fuse (( schedule0 , schedule1 ), partial = 2 ) f , i , j , k = schedule . get_indices () # Reorder the indices to interleave the schedules schedule . reorder ( i , j , f , k )","title":"Examples"},{"location":"Reference/functions/get_parameters_from_grid/","text":"Accera v1.2.1 Reference accera.get_parameters_from_grid(parameter_grid) Get parameters combinations from parameter gid. Arguments argument description type/default parameter_grid A set of different values for each parameter, which will be used to generate a list of all valid parameter combinations dictionary Returns List of dictionary Examples Get parameters combinations from a parameter grid: parameter_grid = { p1 :[ 1 , 2 , 3 ], p2 :[ 4 ], p3 :[ 5 , 6 ]} parameters = acc . get_parameters_from_grid ( parameter_grid )","title":"get_parameters_from_grid"},{"location":"Reference/functions/get_parameters_from_grid/#accera-v121-reference","text":"","title":"Accera v1.2.1 Reference"},{"location":"Reference/functions/get_parameters_from_grid/#acceraget_parameters_from_gridparameter_grid","text":"Get parameters combinations from parameter gid.","title":"accera.get_parameters_from_grid(parameter_grid)"},{"location":"Reference/functions/get_parameters_from_grid/#arguments","text":"argument description type/default parameter_grid A set of different values for each parameter, which will be used to generate a list of all valid parameter combinations dictionary","title":"Arguments"},{"location":"Reference/functions/get_parameters_from_grid/#returns","text":"List of dictionary","title":"Returns"},{"location":"Reference/functions/get_parameters_from_grid/#examples","text":"Get parameters combinations from a parameter grid: parameter_grid = { p1 :[ 1 , 2 , 3 ], p2 :[ 4 ], p3 :[ 5 , 6 ]} parameters = acc . get_parameters_from_grid ( parameter_grid )","title":"Examples"},{"location":"Tutorials/","text":"Accera Tutorials Tutorial Description Hello Matrix Multiplication Start here if you are completely new to Accera and would like to learn more about the workflow Optimized Matrix Multiplication Once you understand the basics, we'll look at how to optimize matrix multiplication for a specific hardware target Hello Matrix Multiplication on GPU We'll look at how to apply the basic concepts for GPU targets Cross Compilation for Raspberry Pi 3 After you know how to generate code for the host target, we'll look at how to generate code for other targets","title":"Index"},{"location":"Tutorials/#accera-tutorials","text":"Tutorial Description Hello Matrix Multiplication Start here if you are completely new to Accera and would like to learn more about the workflow Optimized Matrix Multiplication Once you understand the basics, we'll look at how to optimize matrix multiplication for a specific hardware target Hello Matrix Multiplication on GPU We'll look at how to apply the basic concepts for GPU targets Cross Compilation for Raspberry Pi 3 After you know how to generate code for the host target, we'll look at how to generate code for other targets","title":"Accera Tutorials"},{"location":"Tutorials/Hello_MatMul/","text":"Hello MatMul By the end of this tutorial, you will learn how to: Implement a simple Matrix Multiplication (MatMul) function using Accera's Domain Specific Language (DSL) Produce a HAT package containing the MatMul function Call the function from C or C++ code Prerequisites This tutorial assumes you already have Accera installed. If not, you can find the instructions in here You should also be familiar with writing Python and C++ A naive MatMul algorithm Let's consider the example of multiplying matrices A and B, and adding the result into matrix C. In NumPy syntax, this can be expressed as: C += A @ B A naive algorithm for matrix multiplication typically contains 3 nested for loops. Expressed in Python, this could like: # A.shape = (M, K), B.shape = (K, N), C.shape = (M, N) for i in range(M): for j in range(N): for k in range(K): C[i, j] += A[i, k] * B[k, j] Accera Python DSL We will now walk through a naive Matrix Multiplication (MatMul) using Accera. Create an empty file called hello_matmul_generator.py . First we'll import Accera's module. import accera as acc Define some matrix sizes. A will be M by K, B will be K by N, and C will be M by N. # Define our matrix sizes M = 128 N = 256 K = 256 Write a Python function that receives arrays A , B and C . These are our input and input/output matrices. A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( M , K )) B = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( K , N )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , element_type = acc . ScalarType . float32 , shape = ( M , N )) Here, we will use the Nest class to define our 3-layered nested for loop. The range indices are M , N , and K , with the outermost loop ( M ) listed first. We can get the loop nest indices in order to perform the computation. # Define the loop nest nest = acc . Nest ( shape = ( M , N , K )) # Get the loop nest indices i , j , k = nest . get_indices () Next we define the logic of each iteration of the loop nest: # Define the loop nest logic @nest . iteration_logic def _ (): C [ i , j ] += A [ i , k ] * B [ k , j ] We have finished defining the logic of MatMul, and let's define the schedule which controls how the logic is executed. To do this, we first create the schedule from the nest: sched = nest . create_schedule () At this point, sched represents the default schedule for our algorithm. We can also perform some basic transformations on this schedule. For example, the following lines of code will split the k index in blocks of 4 (so k , k+4 , k+8 , and so on). # Split the k loop into blocks of 4, effectively doing this # (assuming K is divisible by 4): # # for i in range(M): # for j in range(N): # # Split k into two loops # for k in range(0, K, 4): # for kk in range(4): # C[i, j] += A[i, k + kk] * B[k + kk, j] # # If k is not divisible by 4, Accera will take care of the boundary # case for you. kk = sched . split ( k , 4 ) The split index is now k and kk . The next step is to create a plan from the schedule. For instance, we can use this plan to unroll the innermost loop. plan = sched . create_plan () # Unroll kk, effectively doing this # (assuming K is divisible by 4): # # for i in range(M): # for j in range(N): # for k in range(0, K, 4): # # Unrolled kk # C[i, j] += A[i, k + 0] * B[k + 0, j] # C[i, j] += A[i, k + 1] * B[k + 1, j] # C[i, j] += A[i, k + 2] * B[k + 2, j] # C[i, j] += A[i, k + 3] * B[k + 3, j] # # If k is not divisible by 4, Accera will take care of the boundary # case for you. plan . unroll ( kk ) Use the plan to add a callable function named hello_matmul_pi3_py to a HAT package. # Create a package and add a function to the package based on the plan package = acc . Package () package . add ( plan , args = ( A , B , C ), base_name = \"hello_matmul_py\" ) Finally, we build the HAT package: # Build the HAT package package . build ( name = \"hello_matmul\" ) By now, you should have all the code necessary to generate your first Accera MatMul function. You can also find the complete Python script here . Generate HAT package Next, we run the generator script to produce a HAT package. Windows/MacOS python hello_matmul_generator.py Ubuntu python3 hello_matmul_generator.py After this runs, you should see a header file hello_matmul.hat and some object files (such as hello_matmul.obj or hello_matmul.o ). The .hat file format is described here . In Accera, we call these files the \"HAT package\". Runner code We will now walk through how to call our MatMul implementation from the HAT package. Create a file called hello_matmul_runner.cpp with the code below. You can also find it here . #include <stdio.h> #include <algorithm> // Include the HAT file that declares our MatMul function #include \"hello_matmul.hat\" #define M 128 #define N 256 #define K 256 int main ( int argc , const char ** argv ) { // Prepare our matrices float A [ M * K ]; float B [ K * N ]; float C [ M * N ]; // Fill with data std :: fill_n ( A , M * K , 2.0f ); std :: fill_n ( B , K * N , 3.0f ); std :: fill_n ( C , M * N , 0.42f ); printf ( \"Calling MatMul M=%d, K=%d, N=%d \\n \" , M , K , N ); hello_matmul_py ( A , B , C ); printf ( \"Result (first few elements): \" ); for ( int i = 0 ; i < 10 ; ++ i ) { printf ( \"%f \" , C [ i ]); } printf ( \" \\n \" ); return 0 ; } The code above creates the A , B , and C matrices, and calls the function hello_matmul_py to perform MatMul. Now that we have written the code, we will compile and link it with the HAT package to create an executable. Save the file to your working directory, in the same location as hello_matmul_generator.py and the generated *.hat and object files. Build and run Windows We will need the 64-bit Visual C++ tools to link against the generated 64-bit .obj. From an \"x64 Native Tools Command Prompt\" : cl.exe hello_matmul_runner.cpp *.lib hello_matmul_runner.exe MacOS clang hello_matmul_runner.cpp *.a -o hello_matmul_runner ./hello_matmul_runner Ubuntu gcc hello_matmul_runner.cpp *.a -o hello_matmul_runner ./hello_matmul_runner The output should look like: Calling MatMul M=128, K=256, N=256 Result (first few elements): 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 You can now experiment with the generated MatMul function with your own inputs. Optimized MatMul algorithm The above example illustrates a naive algorithm. To see what a more optimized version could like like, see the Optimized MatMul tutorial.","title":"Hello Matrix Multiplication"},{"location":"Tutorials/Hello_MatMul/#hello-matmul","text":"By the end of this tutorial, you will learn how to: Implement a simple Matrix Multiplication (MatMul) function using Accera's Domain Specific Language (DSL) Produce a HAT package containing the MatMul function Call the function from C or C++ code","title":"Hello MatMul"},{"location":"Tutorials/Hello_MatMul/#prerequisites","text":"This tutorial assumes you already have Accera installed. If not, you can find the instructions in here You should also be familiar with writing Python and C++","title":"Prerequisites"},{"location":"Tutorials/Hello_MatMul/#a-naive-matmul-algorithm","text":"Let's consider the example of multiplying matrices A and B, and adding the result into matrix C. In NumPy syntax, this can be expressed as: C += A @ B A naive algorithm for matrix multiplication typically contains 3 nested for loops. Expressed in Python, this could like: # A.shape = (M, K), B.shape = (K, N), C.shape = (M, N) for i in range(M): for j in range(N): for k in range(K): C[i, j] += A[i, k] * B[k, j]","title":"A naive MatMul algorithm"},{"location":"Tutorials/Hello_MatMul/#accera-python-dsl","text":"We will now walk through a naive Matrix Multiplication (MatMul) using Accera. Create an empty file called hello_matmul_generator.py . First we'll import Accera's module. import accera as acc Define some matrix sizes. A will be M by K, B will be K by N, and C will be M by N. # Define our matrix sizes M = 128 N = 256 K = 256 Write a Python function that receives arrays A , B and C . These are our input and input/output matrices. A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( M , K )) B = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( K , N )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , element_type = acc . ScalarType . float32 , shape = ( M , N )) Here, we will use the Nest class to define our 3-layered nested for loop. The range indices are M , N , and K , with the outermost loop ( M ) listed first. We can get the loop nest indices in order to perform the computation. # Define the loop nest nest = acc . Nest ( shape = ( M , N , K )) # Get the loop nest indices i , j , k = nest . get_indices () Next we define the logic of each iteration of the loop nest: # Define the loop nest logic @nest . iteration_logic def _ (): C [ i , j ] += A [ i , k ] * B [ k , j ] We have finished defining the logic of MatMul, and let's define the schedule which controls how the logic is executed. To do this, we first create the schedule from the nest: sched = nest . create_schedule () At this point, sched represents the default schedule for our algorithm. We can also perform some basic transformations on this schedule. For example, the following lines of code will split the k index in blocks of 4 (so k , k+4 , k+8 , and so on). # Split the k loop into blocks of 4, effectively doing this # (assuming K is divisible by 4): # # for i in range(M): # for j in range(N): # # Split k into two loops # for k in range(0, K, 4): # for kk in range(4): # C[i, j] += A[i, k + kk] * B[k + kk, j] # # If k is not divisible by 4, Accera will take care of the boundary # case for you. kk = sched . split ( k , 4 ) The split index is now k and kk . The next step is to create a plan from the schedule. For instance, we can use this plan to unroll the innermost loop. plan = sched . create_plan () # Unroll kk, effectively doing this # (assuming K is divisible by 4): # # for i in range(M): # for j in range(N): # for k in range(0, K, 4): # # Unrolled kk # C[i, j] += A[i, k + 0] * B[k + 0, j] # C[i, j] += A[i, k + 1] * B[k + 1, j] # C[i, j] += A[i, k + 2] * B[k + 2, j] # C[i, j] += A[i, k + 3] * B[k + 3, j] # # If k is not divisible by 4, Accera will take care of the boundary # case for you. plan . unroll ( kk ) Use the plan to add a callable function named hello_matmul_pi3_py to a HAT package. # Create a package and add a function to the package based on the plan package = acc . Package () package . add ( plan , args = ( A , B , C ), base_name = \"hello_matmul_py\" ) Finally, we build the HAT package: # Build the HAT package package . build ( name = \"hello_matmul\" ) By now, you should have all the code necessary to generate your first Accera MatMul function. You can also find the complete Python script here .","title":"Accera Python DSL"},{"location":"Tutorials/Hello_MatMul/#generate-hat-package","text":"Next, we run the generator script to produce a HAT package.","title":"Generate HAT package"},{"location":"Tutorials/Hello_MatMul/#windowsmacos","text":"python hello_matmul_generator.py","title":"Windows/MacOS"},{"location":"Tutorials/Hello_MatMul/#ubuntu","text":"python3 hello_matmul_generator.py After this runs, you should see a header file hello_matmul.hat and some object files (such as hello_matmul.obj or hello_matmul.o ). The .hat file format is described here . In Accera, we call these files the \"HAT package\".","title":"Ubuntu"},{"location":"Tutorials/Hello_MatMul/#runner-code","text":"We will now walk through how to call our MatMul implementation from the HAT package. Create a file called hello_matmul_runner.cpp with the code below. You can also find it here . #include <stdio.h> #include <algorithm> // Include the HAT file that declares our MatMul function #include \"hello_matmul.hat\" #define M 128 #define N 256 #define K 256 int main ( int argc , const char ** argv ) { // Prepare our matrices float A [ M * K ]; float B [ K * N ]; float C [ M * N ]; // Fill with data std :: fill_n ( A , M * K , 2.0f ); std :: fill_n ( B , K * N , 3.0f ); std :: fill_n ( C , M * N , 0.42f ); printf ( \"Calling MatMul M=%d, K=%d, N=%d \\n \" , M , K , N ); hello_matmul_py ( A , B , C ); printf ( \"Result (first few elements): \" ); for ( int i = 0 ; i < 10 ; ++ i ) { printf ( \"%f \" , C [ i ]); } printf ( \" \\n \" ); return 0 ; } The code above creates the A , B , and C matrices, and calls the function hello_matmul_py to perform MatMul. Now that we have written the code, we will compile and link it with the HAT package to create an executable. Save the file to your working directory, in the same location as hello_matmul_generator.py and the generated *.hat and object files.","title":"Runner code"},{"location":"Tutorials/Hello_MatMul/#build-and-run","text":"","title":"Build and run"},{"location":"Tutorials/Hello_MatMul/#windows","text":"We will need the 64-bit Visual C++ tools to link against the generated 64-bit .obj. From an \"x64 Native Tools Command Prompt\" : cl.exe hello_matmul_runner.cpp *.lib hello_matmul_runner.exe","title":"Windows"},{"location":"Tutorials/Hello_MatMul/#macos","text":"clang hello_matmul_runner.cpp *.a -o hello_matmul_runner ./hello_matmul_runner","title":"MacOS"},{"location":"Tutorials/Hello_MatMul/#ubuntu_1","text":"gcc hello_matmul_runner.cpp *.a -o hello_matmul_runner ./hello_matmul_runner The output should look like: Calling MatMul M=128, K=256, N=256 Result (first few elements): 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 You can now experiment with the generated MatMul function with your own inputs.","title":"Ubuntu"},{"location":"Tutorials/Hello_MatMul/#optimized-matmul-algorithm","text":"The above example illustrates a naive algorithm. To see what a more optimized version could like like, see the Optimized MatMul tutorial.","title":"Optimized MatMul algorithm"},{"location":"Tutorials/Hello_MatMul_GPU/","text":"Hello MatMul GPU In this tutorial you will learn how to implement a simple Matrix Multiplication (MatMul) function for execution on a GPU. We will use the Accera's Domain Specific Language (DSL) to produce a HAT package containing the MatMul function that can be called from the host to launch the MatMul function on the GPU. Prerequisites This tutorial assumes you already have Accera installed. If not, you can find the instructions in here You are familiar with writing Python and C++ You are familiar with basic GPU programming and concepts You have completed the Hello_MatMul tutorial You have the Vulkan SDK and runtime installed Review: the naive MatMul algorithm As in the Hello_MatMul tutorial, we'll consider the example of multiplying matrices A and B, and adding the result into matrix C. In NumPy syntax, this can be expressed as: C += A @ B A naive algorithm for matrix multiplication typically contains 3 nested for loops. Expressed in Python, this could look like: for i in range(M): for j in range(N): for k in range(K): C[i, j] += A[i, k] * B[k, j] Accera Python DSL We will now walk through a basic Matrix Multiplication (MatMul) using Accera. Additionally, we will direct Accera to execute this MatMul function on the default GPU. Create an empty file called hello_matmul_gpu_generator.py . Import dependent modules: import accera as acc Define some matrix sizes. A will be M by K, B will be K by N, and C will be M by N. # Define our matrix sizes M = 1024 N = 512 K = 256 Declare our arrays A , B and C . These are our input and input/output matrices and hold 32-bit floating point elements. A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( M , K )) B = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( K , N )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , element_type = acc . ScalarType . float32 , shape = ( M , N )) Use the Nest class to define our 3-layered nested for loop and get the indices: # Define the loop nest nest = acc . Nest ( shape = ( M , N , K )) # Get the loop nest indices i , j , k = nest . get_indices () Next we define the logic of each iteration of the loop nest: # Define the loop nest logic @nest . iteration_logic def _ (): C [ i , j ] += A [ i , k ] * B [ k , j ] We have finished defining the logic of MatMul. Notice how up to this point, this is identical to what we did for the CPU example . Next, define the schedule which controls how the logic is executed. To do this, we first create the schedule from the nest: schedule = nest . create_schedule () In order to execute this efficiently on our chosen hardware target, we will transform the iteration space and change the plan according to some predefined constants. The values of these constants can come either from hardware target characteristics and the shapes of the arrays, or can be found through auto-tuning. These will be explained in more detail in a subsequent tutorial. For now, define: block_x = 16 block_y = 16 Transform the iteration space to specify the thread block behavior. See (GPU blocks)[TODO:markdown...] section to learning more about optimizing block sizes on GPU: ii = schedule . split ( i , block_x ) jj = schedule . split ( j , block_y ) Set the order to traverse the iteration space. Note that on the precise order of execution on GPU targets will be unknown due to the parallel nature of the hardware. Nevertheless, setting the order here is important, since the coarse grain parallelization (e.g. grid) should precede the more fine grained (e.g. warps/wavefronts): schedule . reorder ( i , j , ii , jj , k ) Create a plan from the schedule. The plan allows us to control specific execution behavior on the hardware target, such grid launch dimensions and thread blocks sizes, which are essential for high performance: target = acc . Target ( category = acc . Target . Category . GPU ) plan = schedule . create_plan ( target ) Bind dimensions of the schedule to execution units on the GPU. Use the outer dimensions i , j to be the block indices x , y in the grid, and the ii and jj dimensions to be the thread indices x , y in the block: plan . bind (( i , j , ii , jj ), grid = ( target . GridUnit . BLOCK_X , target . GridUnit . BLOCK_Y , target . GridUnit . THREAD_X , target . GridUnit . THREAD_Y )) Use the plan to add a callable function named hello_matmul_gpu to a HAT package. # Create a package and add a function to the package based on the plan package = acc . Package () package . add ( plan , args = ( A , B , C ), base_name = \"hello_matmul_gpu\" ) Finally, we build the HAT package: # Build a statically-linked HAT package to be consumed by the C++ runner package . build ( name = \"hello_matmul_gpu\" , format = acc . Package . Format . HAT_STATIC ) By now, you have all the code necessary to generate an Accera MatMul function that runs on the GPU. You can also find the complete Python script here . Generate HAT package Next, we run the generator script to produce a HAT package. Windows/MacOS python hello_matmul_gpu_generator.py Ubuntu python3 hello_matmul_gpu_generator.py After this runs, you should see a header file hello_matmul_gpu.hat and some object files (such as hello_matmul_gpu.obj or hello_matmul_gpu.o ). The build process also generates a supporting module, AcceraGPUUtilities.hat and its object file, for GPU initialization and uninitialization. In Accera, we call these files the \"HAT package\". Runner code We will now walk through how to call our MatMul implementation from the HAT package. Create a file called hello_matmul_gpu_runner.cpp with the code below. You can also find it here . #include <stdio.h> #include <algorithm> // Include the HAT file that declares GPU initialization/uninitialization functions #include \"AcceraGPUUtilities.hat\" // Include the HAT file that declares our MatMul function #include \"hello_matmul_gpu.hat\" #define M 1024 #define N 512 #define K 256 int main ( int argc , const char ** argv ) { // Prepare our matrices (using the heap for large matrices) float * A = new float [ M * K ]; float * B = new float [ K * N ]; float * C = new float [ M * N ]; // Fill with data std :: fill_n ( A , M * K , 2.0f ); std :: fill_n ( B , K * N , 3.0f ); std :: fill_n ( C , M * N , 0.42f ); // Initialize the GPU AcceraGPUInitialize (); printf ( \"Calling MatMul M=%d, K=%d, N=%d \\n \" , M , K , N ); hello_matmul_gpu ( A , B , C ); printf ( \"Result (first 10 elements): \" ); for ( int i = 0 ; i < 10 ; ++ i ) { printf ( \"%f \" , C [ i ]); } printf ( \" \\n \" ); // Uninitialize the GPU AcceraGPUDeInitialize (); delete [] A ; delete [] B ; delete [] C ; return 0 ; } The code above creates the A , B , and C matrices, and calls the function hello_matmul_gpu to perform MatMul. Now that we have written the code, we will compile and link it with the HAT package to create an executable. Save the file to your working directory, in the same location as hello_matmul_gpu_generator.py and the generated *.hat and object files. Build and run Accera includes a shared library that wraps the Vulkan APIs ( acc-vulkan-runtime-wrappers.so , acc-vulkan-runtime-wrappers.dll , or acc-vulkan-runtime-wrappers.dylib ). We will need to provide the path to this shared library when building and running the executable. Find the installed path to the \"accera\" package: Windows/MacOS pip show accera Ubuntu pip3 show accera From the output above, find the Location entry, for example: Location: /usr/local/lib/python3.8/dist-packages Note down this path, we will be using it below. Windows We will need the 64-bit Visual C++ tools to link against the generated 64-bit .obj. From an \"x64 Native Tools Command Prompt\" : Set the ACCERA_PATH environment variable to the full install path of the \"accera\" package (derived from pip show accera to locate acc-vulkan-runtime-wrappers.dll ): set ACCERA_PATH = <Location_path> \\a ccera Set the PATH environment variable to allow the runner to locate acc-vulkan-runtime-wrappers.dll : set PATH = %PATH% ; %ACCERA_PATH% Now build and run: cl.exe hello_matmul_gpu_runner.cpp *.lib %ACCERA_PATH%/*.lib hello_matmul_gpu_runner.exe MacOS Set the ACCERA_PATH environment variable to the full install path of the \"accera\" package (derived from pip show accera to locate acc-vulkan-runtime-wrappers.dylib ): export ACCERA_PATH = <Location_path>/accera Now build and run: clang++ hello_matmul_gpu_runner.cpp *.a $ACCERA_PATH /*.dylib -o hello_matmul_gpu_runner DYLD_LIBRARY_PATH = $ACCERA_PATH ./hello_matmul_gpu_runner Ubuntu Set the ACCERA_PATH environment variable to the full install path of the \"accera\" package (derived from pip3 show accera to locate acc-vulkan-runtime-wrappers.so ): export ACCERA_PATH = <Location_path>/accera Now build and run: g++ hello_matmul_gpu_runner.cpp *.a $ACCERA_PATH /*.so -o hello_matmul_gpu_runner LD_LIBRARY_PATH = $ACCERA_PATH ./hello_matmul_gpu_runner The output should look like: Calling MatMul M=1024, K=256, N=512 Result (first 10 elements): 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 You can now experiment with the generated MatMul function with your own inputs.","title":"Hello Matrix Multiplication on GPU"},{"location":"Tutorials/Hello_MatMul_GPU/#hello-matmul-gpu","text":"In this tutorial you will learn how to implement a simple Matrix Multiplication (MatMul) function for execution on a GPU. We will use the Accera's Domain Specific Language (DSL) to produce a HAT package containing the MatMul function that can be called from the host to launch the MatMul function on the GPU.","title":"Hello MatMul GPU"},{"location":"Tutorials/Hello_MatMul_GPU/#prerequisites","text":"This tutorial assumes you already have Accera installed. If not, you can find the instructions in here You are familiar with writing Python and C++ You are familiar with basic GPU programming and concepts You have completed the Hello_MatMul tutorial You have the Vulkan SDK and runtime installed","title":"Prerequisites"},{"location":"Tutorials/Hello_MatMul_GPU/#review-the-naive-matmul-algorithm","text":"As in the Hello_MatMul tutorial, we'll consider the example of multiplying matrices A and B, and adding the result into matrix C. In NumPy syntax, this can be expressed as: C += A @ B A naive algorithm for matrix multiplication typically contains 3 nested for loops. Expressed in Python, this could look like: for i in range(M): for j in range(N): for k in range(K): C[i, j] += A[i, k] * B[k, j]","title":"Review: the naive MatMul algorithm"},{"location":"Tutorials/Hello_MatMul_GPU/#accera-python-dsl","text":"We will now walk through a basic Matrix Multiplication (MatMul) using Accera. Additionally, we will direct Accera to execute this MatMul function on the default GPU. Create an empty file called hello_matmul_gpu_generator.py . Import dependent modules: import accera as acc Define some matrix sizes. A will be M by K, B will be K by N, and C will be M by N. # Define our matrix sizes M = 1024 N = 512 K = 256 Declare our arrays A , B and C . These are our input and input/output matrices and hold 32-bit floating point elements. A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( M , K )) B = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( K , N )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , element_type = acc . ScalarType . float32 , shape = ( M , N )) Use the Nest class to define our 3-layered nested for loop and get the indices: # Define the loop nest nest = acc . Nest ( shape = ( M , N , K )) # Get the loop nest indices i , j , k = nest . get_indices () Next we define the logic of each iteration of the loop nest: # Define the loop nest logic @nest . iteration_logic def _ (): C [ i , j ] += A [ i , k ] * B [ k , j ] We have finished defining the logic of MatMul. Notice how up to this point, this is identical to what we did for the CPU example . Next, define the schedule which controls how the logic is executed. To do this, we first create the schedule from the nest: schedule = nest . create_schedule () In order to execute this efficiently on our chosen hardware target, we will transform the iteration space and change the plan according to some predefined constants. The values of these constants can come either from hardware target characteristics and the shapes of the arrays, or can be found through auto-tuning. These will be explained in more detail in a subsequent tutorial. For now, define: block_x = 16 block_y = 16 Transform the iteration space to specify the thread block behavior. See (GPU blocks)[TODO:markdown...] section to learning more about optimizing block sizes on GPU: ii = schedule . split ( i , block_x ) jj = schedule . split ( j , block_y ) Set the order to traverse the iteration space. Note that on the precise order of execution on GPU targets will be unknown due to the parallel nature of the hardware. Nevertheless, setting the order here is important, since the coarse grain parallelization (e.g. grid) should precede the more fine grained (e.g. warps/wavefronts): schedule . reorder ( i , j , ii , jj , k ) Create a plan from the schedule. The plan allows us to control specific execution behavior on the hardware target, such grid launch dimensions and thread blocks sizes, which are essential for high performance: target = acc . Target ( category = acc . Target . Category . GPU ) plan = schedule . create_plan ( target ) Bind dimensions of the schedule to execution units on the GPU. Use the outer dimensions i , j to be the block indices x , y in the grid, and the ii and jj dimensions to be the thread indices x , y in the block: plan . bind (( i , j , ii , jj ), grid = ( target . GridUnit . BLOCK_X , target . GridUnit . BLOCK_Y , target . GridUnit . THREAD_X , target . GridUnit . THREAD_Y )) Use the plan to add a callable function named hello_matmul_gpu to a HAT package. # Create a package and add a function to the package based on the plan package = acc . Package () package . add ( plan , args = ( A , B , C ), base_name = \"hello_matmul_gpu\" ) Finally, we build the HAT package: # Build a statically-linked HAT package to be consumed by the C++ runner package . build ( name = \"hello_matmul_gpu\" , format = acc . Package . Format . HAT_STATIC ) By now, you have all the code necessary to generate an Accera MatMul function that runs on the GPU. You can also find the complete Python script here .","title":"Accera Python DSL"},{"location":"Tutorials/Hello_MatMul_GPU/#generate-hat-package","text":"Next, we run the generator script to produce a HAT package.","title":"Generate HAT package"},{"location":"Tutorials/Hello_MatMul_GPU/#windowsmacos","text":"python hello_matmul_gpu_generator.py","title":"Windows/MacOS"},{"location":"Tutorials/Hello_MatMul_GPU/#ubuntu","text":"python3 hello_matmul_gpu_generator.py After this runs, you should see a header file hello_matmul_gpu.hat and some object files (such as hello_matmul_gpu.obj or hello_matmul_gpu.o ). The build process also generates a supporting module, AcceraGPUUtilities.hat and its object file, for GPU initialization and uninitialization. In Accera, we call these files the \"HAT package\".","title":"Ubuntu"},{"location":"Tutorials/Hello_MatMul_GPU/#runner-code","text":"We will now walk through how to call our MatMul implementation from the HAT package. Create a file called hello_matmul_gpu_runner.cpp with the code below. You can also find it here . #include <stdio.h> #include <algorithm> // Include the HAT file that declares GPU initialization/uninitialization functions #include \"AcceraGPUUtilities.hat\" // Include the HAT file that declares our MatMul function #include \"hello_matmul_gpu.hat\" #define M 1024 #define N 512 #define K 256 int main ( int argc , const char ** argv ) { // Prepare our matrices (using the heap for large matrices) float * A = new float [ M * K ]; float * B = new float [ K * N ]; float * C = new float [ M * N ]; // Fill with data std :: fill_n ( A , M * K , 2.0f ); std :: fill_n ( B , K * N , 3.0f ); std :: fill_n ( C , M * N , 0.42f ); // Initialize the GPU AcceraGPUInitialize (); printf ( \"Calling MatMul M=%d, K=%d, N=%d \\n \" , M , K , N ); hello_matmul_gpu ( A , B , C ); printf ( \"Result (first 10 elements): \" ); for ( int i = 0 ; i < 10 ; ++ i ) { printf ( \"%f \" , C [ i ]); } printf ( \" \\n \" ); // Uninitialize the GPU AcceraGPUDeInitialize (); delete [] A ; delete [] B ; delete [] C ; return 0 ; } The code above creates the A , B , and C matrices, and calls the function hello_matmul_gpu to perform MatMul. Now that we have written the code, we will compile and link it with the HAT package to create an executable. Save the file to your working directory, in the same location as hello_matmul_gpu_generator.py and the generated *.hat and object files.","title":"Runner code"},{"location":"Tutorials/Hello_MatMul_GPU/#build-and-run","text":"Accera includes a shared library that wraps the Vulkan APIs ( acc-vulkan-runtime-wrappers.so , acc-vulkan-runtime-wrappers.dll , or acc-vulkan-runtime-wrappers.dylib ). We will need to provide the path to this shared library when building and running the executable. Find the installed path to the \"accera\" package:","title":"Build and run"},{"location":"Tutorials/Hello_MatMul_GPU/#windowsmacos_1","text":"pip show accera","title":"Windows/MacOS"},{"location":"Tutorials/Hello_MatMul_GPU/#ubuntu_1","text":"pip3 show accera From the output above, find the Location entry, for example: Location: /usr/local/lib/python3.8/dist-packages Note down this path, we will be using it below.","title":"Ubuntu"},{"location":"Tutorials/Hello_MatMul_GPU/#windows","text":"We will need the 64-bit Visual C++ tools to link against the generated 64-bit .obj. From an \"x64 Native Tools Command Prompt\" : Set the ACCERA_PATH environment variable to the full install path of the \"accera\" package (derived from pip show accera to locate acc-vulkan-runtime-wrappers.dll ): set ACCERA_PATH = <Location_path> \\a ccera Set the PATH environment variable to allow the runner to locate acc-vulkan-runtime-wrappers.dll : set PATH = %PATH% ; %ACCERA_PATH% Now build and run: cl.exe hello_matmul_gpu_runner.cpp *.lib %ACCERA_PATH%/*.lib hello_matmul_gpu_runner.exe","title":"Windows"},{"location":"Tutorials/Hello_MatMul_GPU/#macos","text":"Set the ACCERA_PATH environment variable to the full install path of the \"accera\" package (derived from pip show accera to locate acc-vulkan-runtime-wrappers.dylib ): export ACCERA_PATH = <Location_path>/accera Now build and run: clang++ hello_matmul_gpu_runner.cpp *.a $ACCERA_PATH /*.dylib -o hello_matmul_gpu_runner DYLD_LIBRARY_PATH = $ACCERA_PATH ./hello_matmul_gpu_runner","title":"MacOS"},{"location":"Tutorials/Hello_MatMul_GPU/#ubuntu_2","text":"Set the ACCERA_PATH environment variable to the full install path of the \"accera\" package (derived from pip3 show accera to locate acc-vulkan-runtime-wrappers.so ): export ACCERA_PATH = <Location_path>/accera Now build and run: g++ hello_matmul_gpu_runner.cpp *.a $ACCERA_PATH /*.so -o hello_matmul_gpu_runner LD_LIBRARY_PATH = $ACCERA_PATH ./hello_matmul_gpu_runner The output should look like: Calling MatMul M=1024, K=256, N=512 Result (first 10 elements): 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 You can now experiment with the generated MatMul function with your own inputs.","title":"Ubuntu"},{"location":"Tutorials/Optimized_MatMul/","text":"Optimized MatMul Optimizing MatMul depends heavily on the target platform. The code in the example below is optimized specifically for an Intel Xeon E5-2673 v3 CPU, but will work equally well on CPUs with similar hardware characteristics like AMD Epyc 7551 and so on. By the end of this tutorial, you will learn how to: Implement a performant Matrix Multiplication (MatMul) function targetting AVX2 FMA3 CPUs like Intel Haswell or the AMD Epyc families, using Accera's Domain Specific Language (DSL) Produce a HAT package containing the optimized MatMul function Call the function from C or C++ code Prerequisites This tutorial assumes you already have Accera installed. If not, you can find the instructions in here You are familiar with writing Python and C++ You know about SIMD instructions and registers You have completed the Hello_MatMul tutorial Review: the naive MatMul algorithm As in the Hello_MatMul tutorial, we'll consider the example of multiplying matrices A and B, and adding the result into matrix C. In NumPy syntax, this can be expressed as: C += A @ B A naive algorithm for matrix multiplication typically contains 3 nested for loops. Expressed in Python, this could look like: for i in range(M): for j in range(N): for k in range(K): C[i, j] += A[i, k] * B[k, j] Accera Python DSL We will walk through how to specify an optimized Matrix Multiplication (MatMul) using Accera. This tutorial assumes the following: Specific matrix sizes, input A is 784 x 128, B is 128 x 512, the output C is 784 x 512 elements. These represent an mid-level layer in a Resnet-50 model, where the A matrix contains the activation values from the previous layer and B matrix contains the weights of the neural network layer. Row-major layout of the array elements. The target hardware is capable of AVX2 FMA3 instructions, such as the Intel Xeon E5-2673 v3 or the AMD Epyc 7551. Create an empty file called optimized_matmul_generator.py . Import dependent modules: import accera as acc Define some matrix sizes. A will be M by K, B will be K by N, and C will be M by N. # Define our matrix sizes M = 784 N = 512 K = 128 Declare our arrays A , B and C . These are our input and input/output matrices and hold 32-bit floating point elements. A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( M , K )) B = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( K , N )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , element_type = acc . ScalarType . float32 , shape = ( M , N )) Use the Nest class to define our 3-layered nested for loop and get the indices: # Define the loop nest nest = acc . Nest ( shape = ( M , N , K )) # Get the loop nest indices i , j , k = nest . get_indices () Next we define the logic of each iteration of the loop nest: # Define the loop nest logic @nest . iteration_logic def _ (): C [ i , j ] += A [ i , k ] * B [ k , j ] We have finished defining the logic of MatMul, and let's define the schedule which controls how the logic is executed. To do this, we first create the schedule from the nest: schedule = nest . create_schedule () In order to execute this efficiently on our chosen hardware target, we will transform the iteration space and change the plan according to some predefined constants. The values of these constants come either from hardware target characteristics and the shapes of the arrays, or can be found through auto-tuning. These will be explained in more detail in a subsequent tutorial. For now, define: tile_size_i = 6 tile_size_j = 256 tile_size_k = 128 inner_dim_unroll = 4 num_rows_in_kernel = 6 To use the hardware characteristics, we create a CPU target which will define constants for the SIMD vector sizes and number of vector execution units. target = acc . Target ( category = acc . Target . Category . CPU ) Transform the iteration space to specify the tiling behavior. See (tiling)[TODO:markdown...] section to learning more about tiling: ii = schedule . split ( i , tile_size_i ) jj = schedule . split ( j , tile_size_j ) kk = schedule . split ( k , tile_size_k ) Next, let's split the iteration space to match the kernel characteristics. See (kernels)[TODO:markdown...] section to learning more about kernels: kkk = schedule . split ( kk , inner_dim_unroll ) iii = schedule . split ( ii , num_rows_in_kernel ) jjj = schedule . split ( jj , ( target . vector_bytes // 4 ) * 2 ) # There are 2 vfma execution units, each holding (target.vector_bytes // 4) 32-bit float elements jjjj = schedule . split ( jjj , target . vector_bytes // 4 ) # Each SIMD register holds (target.vector_bytes // 4) 32-bit float elements Note, that for each of these splits, Accera will handle the boundary conditions that arise, and do appropriate optimizations such as loop unswitching to ensure efficient code gets generated in those cases. Set the order to traverse the iteration space. We start with the outer indices that control the tiling, then move to the innermost indices that are used in the kernel: schedule . reorder ( j , k , i , jj , kk , ii , kkk , iii , jjj , jjjj ) Create a plan from the schedule and the current target. The plan allows us to control specific execution behavior on the hardware target, such as vectorization and caching, which are essential for high performance: plan = schedule . create_plan ( target ) Add caching. We use an input cache for the B array exceeds our threshold. The B matrix cache will be packed according to the access pattern specified by the schedule. We use an input/output cache for the C array. See Section 5 caching for more information: # Cache the B array by prefetching and packing the memory footprint along slices of the jj dimension. plan . cache ( B , jj ) # Cache the C array along slices of jj dimension. Since the C array is the output, its footprint is # the size of the kernel. If the kernel is small enough, Accera will use registers for this # accumulation before writing these values back to C. plan . cache ( C , jj ) Kernelize the inner dimensions, which applies unroll and vectorize transformations allowing use of SIMD registers: plan . kernelize ( unroll_indices = [ jjj , iii , kkk ], vectorize_indices = jjjj ) Use the plan to add a callable function named optimized_matmul_py to a HAT package. # Create a package and add a function to the package based on the plan package = acc . Package () package . add ( plan , args = ( A , B , C ), base_name = \"optimized_matmul_py\" ) Finally, we build the HAT package: # Build a statically-linked HAT package to be consumed by the C++ runner package . build ( name = \"optimized_matmul\" , format = acc . Package . Format . HAT_STATIC ) By now, you should have all the code necessary to generate an optimized Accera MatMul function. You can also find the complete Python script here . Generate HAT package Next, we run the generator script to produce a HAT package. Windows/MacOS python optimized_matmul_generator.py Ubuntu python3 optimized_matmul_generator.py The generator script produces a HAT package ( hello_matmul.hat ). Examining that file, you can see that it contains the exported function with the following meta-data: [functions.optimized_matmul_py_4a6286d9] name = 'optimized_matmul_py_4a6286d9' description = '' calling_convention = \"cdecl\" arguments = [ { name = '' , description = '' , logical_type = \"affine_array\" , declared_type = 'float*' , element_type = 'float' , usage = \"input_output\" , shape = [ 784 , 128 ], affine_map = [ 128 , 1 ], affine_offset = 0 }, { name = '' , description = '' , logical_type = \"affine_array\" , declared_type = 'float*' , element_type = 'float' , usage = \"input_output\" , shape = [ 128 , 512 ], affine_map = [ 512 , 1 ], affine_offset = 0 }, { name = '' , description = '' , logical_type = \"affine_array\" , declared_type = 'float*' , element_type = 'float' , usage = \"input_output\" , shape = [ 784 , 512 ], affine_map = [ 512 , 1 ], affine_offset = 0 } ] return = { name = '' , description = '' , logical_type = \"void\" , declared_type = 'void' , element_type = 'void' , usage = \"output\" } The C declaration from the header is: void optimized_matmul_py_4a6286d9 ( float * , float * , float * ); Accera automatically appends a unique identifier to the function implementation, such as optimized_matmul_py_4a6286d9 to support auto-tuning. This name is re-generated every time the HAT package is rebuilt. To make it easier for client code to use the function, Accera also provides a fixed-name alias, optimized_matmul_py , for the same function. To see how Accera has handled the code generation given the iteration space transformations and the final plan, you can change the format=HAT to format=MLIR , which will output MLIR for each of the major lowering phases. Stepping through the progression of lowerings, you can see how Accera moves from simple representation of the Accera DSL , to the final optimized assembly . Compare this to the previous tutorial, whose naive DSL is repsesented here , and whose final assembly can be viewed here . Runner code We will now walk through how to call our MatMul implementation from the HAT package. Create a file called optimized_matmul_runner.cpp with the code below. You can also find it here . #include <stdio.h> #include <algorithm> // Include the HAT file that declares our MatMul function #include \"optimized_matmul.hat\" #define M 784 #define N 512 #define K 128 int main ( int argc , const char ** argv ) { // Prepare our matrices (using the heap for large matrices) float * A = new float [ M * K ]; float * B = new float [ K * N ]; float * C = new float [ M * N ]; // Fill with data std :: fill_n ( A , M * K , 2.0f ); std :: fill_n ( B , K * N , 3.0f ); std :: fill_n ( C , M * N , 0.42f ); printf ( \"Calling MatMul M=%d, K=%d, N=%d \\n \" , M , K , N ); optimized_matmul_py ( A , B , C ); printf ( \"Result (first 10 elements): \" ); for ( int i = 0 ; i < 10 ; ++ i ) { printf ( \"%f \" , C [ i ]); } printf ( \" \\n \" ); delete [] A ; delete [] B ; delete [] C ; return 0 ; } The code above creates the A , B , and C matrices, and calls the function optimized_matmul_py to perform MatMul. Now that we have written the code, we will compile and link it with the HAT package to create an executable. Save the file to your working directory, in the same location as optimized_matmul_generator.py and the generated *.hat and object files. Build and run Windows We will need the 64-bit Visual C++ tools to link against the generated 64-bit .obj. From an \"x64 Native Tools Command Prompt\" : cl.exe optimized_matmul_runner.cpp *.lib optimized_matmul_runner.exe MacOS clang++ optimized_matmul_runner.cpp *.a -o optimized_matmul_runner ./optimized_matmul_runner Ubuntu g++ optimized_matmul_runner.cpp *.a -o optimized_matmul_runner ./optimized_matmul_runner The output should look like: Calling MatMul M=784, K=128, N=512 Result (first 10 elements): 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983 You can now experiment with the generated MatMul function with your own inputs.","title":"Optimized Matrix Multiplication"},{"location":"Tutorials/Optimized_MatMul/#optimized-matmul","text":"Optimizing MatMul depends heavily on the target platform. The code in the example below is optimized specifically for an Intel Xeon E5-2673 v3 CPU, but will work equally well on CPUs with similar hardware characteristics like AMD Epyc 7551 and so on. By the end of this tutorial, you will learn how to: Implement a performant Matrix Multiplication (MatMul) function targetting AVX2 FMA3 CPUs like Intel Haswell or the AMD Epyc families, using Accera's Domain Specific Language (DSL) Produce a HAT package containing the optimized MatMul function Call the function from C or C++ code","title":"Optimized MatMul"},{"location":"Tutorials/Optimized_MatMul/#prerequisites","text":"This tutorial assumes you already have Accera installed. If not, you can find the instructions in here You are familiar with writing Python and C++ You know about SIMD instructions and registers You have completed the Hello_MatMul tutorial","title":"Prerequisites"},{"location":"Tutorials/Optimized_MatMul/#review-the-naive-matmul-algorithm","text":"As in the Hello_MatMul tutorial, we'll consider the example of multiplying matrices A and B, and adding the result into matrix C. In NumPy syntax, this can be expressed as: C += A @ B A naive algorithm for matrix multiplication typically contains 3 nested for loops. Expressed in Python, this could look like: for i in range(M): for j in range(N): for k in range(K): C[i, j] += A[i, k] * B[k, j]","title":"Review: the naive MatMul algorithm"},{"location":"Tutorials/Optimized_MatMul/#accera-python-dsl","text":"We will walk through how to specify an optimized Matrix Multiplication (MatMul) using Accera. This tutorial assumes the following: Specific matrix sizes, input A is 784 x 128, B is 128 x 512, the output C is 784 x 512 elements. These represent an mid-level layer in a Resnet-50 model, where the A matrix contains the activation values from the previous layer and B matrix contains the weights of the neural network layer. Row-major layout of the array elements. The target hardware is capable of AVX2 FMA3 instructions, such as the Intel Xeon E5-2673 v3 or the AMD Epyc 7551. Create an empty file called optimized_matmul_generator.py . Import dependent modules: import accera as acc Define some matrix sizes. A will be M by K, B will be K by N, and C will be M by N. # Define our matrix sizes M = 784 N = 512 K = 128 Declare our arrays A , B and C . These are our input and input/output matrices and hold 32-bit floating point elements. A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( M , K )) B = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( K , N )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , element_type = acc . ScalarType . float32 , shape = ( M , N )) Use the Nest class to define our 3-layered nested for loop and get the indices: # Define the loop nest nest = acc . Nest ( shape = ( M , N , K )) # Get the loop nest indices i , j , k = nest . get_indices () Next we define the logic of each iteration of the loop nest: # Define the loop nest logic @nest . iteration_logic def _ (): C [ i , j ] += A [ i , k ] * B [ k , j ] We have finished defining the logic of MatMul, and let's define the schedule which controls how the logic is executed. To do this, we first create the schedule from the nest: schedule = nest . create_schedule () In order to execute this efficiently on our chosen hardware target, we will transform the iteration space and change the plan according to some predefined constants. The values of these constants come either from hardware target characteristics and the shapes of the arrays, or can be found through auto-tuning. These will be explained in more detail in a subsequent tutorial. For now, define: tile_size_i = 6 tile_size_j = 256 tile_size_k = 128 inner_dim_unroll = 4 num_rows_in_kernel = 6 To use the hardware characteristics, we create a CPU target which will define constants for the SIMD vector sizes and number of vector execution units. target = acc . Target ( category = acc . Target . Category . CPU ) Transform the iteration space to specify the tiling behavior. See (tiling)[TODO:markdown...] section to learning more about tiling: ii = schedule . split ( i , tile_size_i ) jj = schedule . split ( j , tile_size_j ) kk = schedule . split ( k , tile_size_k ) Next, let's split the iteration space to match the kernel characteristics. See (kernels)[TODO:markdown...] section to learning more about kernels: kkk = schedule . split ( kk , inner_dim_unroll ) iii = schedule . split ( ii , num_rows_in_kernel ) jjj = schedule . split ( jj , ( target . vector_bytes // 4 ) * 2 ) # There are 2 vfma execution units, each holding (target.vector_bytes // 4) 32-bit float elements jjjj = schedule . split ( jjj , target . vector_bytes // 4 ) # Each SIMD register holds (target.vector_bytes // 4) 32-bit float elements Note, that for each of these splits, Accera will handle the boundary conditions that arise, and do appropriate optimizations such as loop unswitching to ensure efficient code gets generated in those cases. Set the order to traverse the iteration space. We start with the outer indices that control the tiling, then move to the innermost indices that are used in the kernel: schedule . reorder ( j , k , i , jj , kk , ii , kkk , iii , jjj , jjjj ) Create a plan from the schedule and the current target. The plan allows us to control specific execution behavior on the hardware target, such as vectorization and caching, which are essential for high performance: plan = schedule . create_plan ( target ) Add caching. We use an input cache for the B array exceeds our threshold. The B matrix cache will be packed according to the access pattern specified by the schedule. We use an input/output cache for the C array. See Section 5 caching for more information: # Cache the B array by prefetching and packing the memory footprint along slices of the jj dimension. plan . cache ( B , jj ) # Cache the C array along slices of jj dimension. Since the C array is the output, its footprint is # the size of the kernel. If the kernel is small enough, Accera will use registers for this # accumulation before writing these values back to C. plan . cache ( C , jj ) Kernelize the inner dimensions, which applies unroll and vectorize transformations allowing use of SIMD registers: plan . kernelize ( unroll_indices = [ jjj , iii , kkk ], vectorize_indices = jjjj ) Use the plan to add a callable function named optimized_matmul_py to a HAT package. # Create a package and add a function to the package based on the plan package = acc . Package () package . add ( plan , args = ( A , B , C ), base_name = \"optimized_matmul_py\" ) Finally, we build the HAT package: # Build a statically-linked HAT package to be consumed by the C++ runner package . build ( name = \"optimized_matmul\" , format = acc . Package . Format . HAT_STATIC ) By now, you should have all the code necessary to generate an optimized Accera MatMul function. You can also find the complete Python script here .","title":"Accera Python DSL"},{"location":"Tutorials/Optimized_MatMul/#generate-hat-package","text":"Next, we run the generator script to produce a HAT package.","title":"Generate HAT package"},{"location":"Tutorials/Optimized_MatMul/#windowsmacos","text":"python optimized_matmul_generator.py","title":"Windows/MacOS"},{"location":"Tutorials/Optimized_MatMul/#ubuntu","text":"python3 optimized_matmul_generator.py The generator script produces a HAT package ( hello_matmul.hat ). Examining that file, you can see that it contains the exported function with the following meta-data: [functions.optimized_matmul_py_4a6286d9] name = 'optimized_matmul_py_4a6286d9' description = '' calling_convention = \"cdecl\" arguments = [ { name = '' , description = '' , logical_type = \"affine_array\" , declared_type = 'float*' , element_type = 'float' , usage = \"input_output\" , shape = [ 784 , 128 ], affine_map = [ 128 , 1 ], affine_offset = 0 }, { name = '' , description = '' , logical_type = \"affine_array\" , declared_type = 'float*' , element_type = 'float' , usage = \"input_output\" , shape = [ 128 , 512 ], affine_map = [ 512 , 1 ], affine_offset = 0 }, { name = '' , description = '' , logical_type = \"affine_array\" , declared_type = 'float*' , element_type = 'float' , usage = \"input_output\" , shape = [ 784 , 512 ], affine_map = [ 512 , 1 ], affine_offset = 0 } ] return = { name = '' , description = '' , logical_type = \"void\" , declared_type = 'void' , element_type = 'void' , usage = \"output\" } The C declaration from the header is: void optimized_matmul_py_4a6286d9 ( float * , float * , float * ); Accera automatically appends a unique identifier to the function implementation, such as optimized_matmul_py_4a6286d9 to support auto-tuning. This name is re-generated every time the HAT package is rebuilt. To make it easier for client code to use the function, Accera also provides a fixed-name alias, optimized_matmul_py , for the same function. To see how Accera has handled the code generation given the iteration space transformations and the final plan, you can change the format=HAT to format=MLIR , which will output MLIR for each of the major lowering phases. Stepping through the progression of lowerings, you can see how Accera moves from simple representation of the Accera DSL , to the final optimized assembly . Compare this to the previous tutorial, whose naive DSL is repsesented here , and whose final assembly can be viewed here .","title":"Ubuntu"},{"location":"Tutorials/Optimized_MatMul/#runner-code","text":"We will now walk through how to call our MatMul implementation from the HAT package. Create a file called optimized_matmul_runner.cpp with the code below. You can also find it here . #include <stdio.h> #include <algorithm> // Include the HAT file that declares our MatMul function #include \"optimized_matmul.hat\" #define M 784 #define N 512 #define K 128 int main ( int argc , const char ** argv ) { // Prepare our matrices (using the heap for large matrices) float * A = new float [ M * K ]; float * B = new float [ K * N ]; float * C = new float [ M * N ]; // Fill with data std :: fill_n ( A , M * K , 2.0f ); std :: fill_n ( B , K * N , 3.0f ); std :: fill_n ( C , M * N , 0.42f ); printf ( \"Calling MatMul M=%d, K=%d, N=%d \\n \" , M , K , N ); optimized_matmul_py ( A , B , C ); printf ( \"Result (first 10 elements): \" ); for ( int i = 0 ; i < 10 ; ++ i ) { printf ( \"%f \" , C [ i ]); } printf ( \" \\n \" ); delete [] A ; delete [] B ; delete [] C ; return 0 ; } The code above creates the A , B , and C matrices, and calls the function optimized_matmul_py to perform MatMul. Now that we have written the code, we will compile and link it with the HAT package to create an executable. Save the file to your working directory, in the same location as optimized_matmul_generator.py and the generated *.hat and object files.","title":"Runner code"},{"location":"Tutorials/Optimized_MatMul/#build-and-run","text":"","title":"Build and run"},{"location":"Tutorials/Optimized_MatMul/#windows","text":"We will need the 64-bit Visual C++ tools to link against the generated 64-bit .obj. From an \"x64 Native Tools Command Prompt\" : cl.exe optimized_matmul_runner.cpp *.lib optimized_matmul_runner.exe","title":"Windows"},{"location":"Tutorials/Optimized_MatMul/#macos","text":"clang++ optimized_matmul_runner.cpp *.a -o optimized_matmul_runner ./optimized_matmul_runner","title":"MacOS"},{"location":"Tutorials/Optimized_MatMul/#ubuntu_1","text":"g++ optimized_matmul_runner.cpp *.a -o optimized_matmul_runner ./optimized_matmul_runner The output should look like: Calling MatMul M=784, K=128, N=512 Result (first 10 elements): 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983 768.419983 You can now experiment with the generated MatMul function with your own inputs.","title":"Ubuntu"},{"location":"Tutorials/Pi3_Cross_Compilation/","text":"Cross Compiling for the Raspberry Pi 3 By the end of this tutorial, you will learn how to: Cross compile a simple Matrix Multiplication (MatMul) function for execution on a Raspberry Pi 3 Produce a HAT package containing the MatMul function that can be called on Pi 3 target Call the function from C or C++ code on a Raspberry Pi 3 Prerequisites This tutorial assumes you already have Accera installed. If not, you can find the instructions in here You should also be familiar with writing Python and C++ You have access to a Raspberry Pi 3 device A naive MatMul algorithm Let's consider the example of multiplying matrices A and B, and adding the result into matrix C. In NumPy syntax, this can be expressed as: C += A @ B A naive algorithm for matrix multiplication typically contains 3 nested for loops. Expressed in Python, this could like: # A.shape = (M, K), B.shape = (K, N), C.shape = (M, N) for i in range(M): for j in range(N): for k in range(K): C[i, j] += A[i, k] * B[k, j] Accera Python DSL We will now walk through a naive Matrix Multiplication (MatMul) using Accera. In order to cross compile on the host for a different target, instead of using the default target that is the host machine, we will specify a target that represents a Raspberry Pi 3. Create an empty file called hello_matmul_pi3_generator.py . First we'll import Accera's module. import accera as acc Define some matrix sizes. A will be M by K, B will be K by N, and C will be M by N. # Define our matrix sizes M = 128 N = 256 K = 256 Write a Python function that receives arrays A , B and C . These are our input and input/output matrices. A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( M , K )) B = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( K , N )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , element_type = acc . ScalarType . float32 , shape = ( M , N )) Here, we will use the Nest class to define our 3-layered nested for loop. The range indices are M , N , and K , with the outermost loop ( M ) listed first. We can get the loop nest indices in order to perform the computation. # Define the loop nest nest = acc . Nest ( shape = ( M , N , K )) # Get the loop nest indices i , j , k = nest . get_indices () Next we define the logic of each iteration of the loop nest: # Define the loop nest logic @nest . iteration_logic def _ (): C [ i , j ] += A [ i , k ] * B [ k , j ] We have finished defining the logic of MatMul, and let's define the schedule which controls how the logic is executed. To do this, we first create the schedule from the nest: sched = nest . create_schedule () At this point, sched represents the default schedule for our algorithm. We can also perform some basic transformations on this schedule. For example, the following lines of code will split the k index in blocks of 4 (so k , k+4 , k+8 , and so on). # Split the k loop into blocks of 4, effectively doing this # (assuming K is divisible by 4): # # for i in range(M): # for j in range(N): # # Split k into two loops # for k in range(0, K, 4): # for kk in range(4): # C[i, j] += A[i, k + kk] * B[k + kk, j] # # If k is not divisible by 4, Accera will take care of the boundary # case for you. kk = sched . split ( k , 4 ) The split index is now k and kk . The next step is to create a plan from the schedule. For instance, we can use this plan to unroll the innermost loop. # Create a plan, specify the target to be a Raspberry Pi 3 pi3 = acc . Target ( acc . Target . Model . RASPBERRY_PI_3B ) plan = sched . create_plan ( pi3 ) # Unroll kk, effectively doing this # (assuming K is divisible by 4): # # for i in range(M): # for j in range(N): # for k in range(0, K, 4): # # Unrolled kk # C[i, j] += A[i, k + 0] * B[k + 0, j] # C[i, j] += A[i, k + 1] * B[k + 1, j] # C[i, j] += A[i, k + 2] * B[k + 2, j] # C[i, j] += A[i, k + 3] * B[k + 3, j] # # If k is not divisible by 4, Accera will take care of the boundary # case for you. plan . unroll ( kk ) Use the plan to add a callable function named hello_matmul_pi3_py to a HAT package. # Create a package and add a function to the package based on the plan package = acc . Package () package . add ( plan , args = ( A , B , C ), base_name = \"hello_matmul_pi3_py\" ) Finally, we build the statically-linked HAT package for the Raspbian platform: # Build the HAT package package . build ( name = \"hello_matmul_pi3\" , format = acc . Package . Format . HAT_STATIC , platform = acc . Package . Platform . RASPBIAN ) By now, you should have all the code necessary to generate your Accera MatMul function that can be called on a Raspberry Pi 3 target. You can also find the complete Python script here . Generate HAT package Next, we run the generator script to produce a HAT package for the Raspberry Pi 3 target. Windows/MacOS python hello_matmul_pi3_generator.py Ubuntu python3 hello_matmul_pi3_generator.py After this runs, you should see a header file hello_matmul_pi3.HAT and an object file hello_matmul_pi3.o in the ELF format. The .HAT file format is described here . Together with the obj , this is part of a HAT package. Runner code We will now walk through how to call our MatMul implementation from the HAT package on the Raspberry Pi 3. Create a file called hello_matmul_pi3_runner.cpp with the code below. You can also find it here . #include <stdio.h> #include <algorithm> // Include the HAT file that declares our MatMul function #include \"hello_matmul_p3.HAT\" #define M 128 #define N 256 #define K 256 int main ( int argc , const char ** argv ) { // Prepare our matrices float A [ M * K ]; float B [ K * N ]; float C [ M * N ]; // Fill with data std :: fill_n ( A , M * K , 2.0f ); std :: fill_n ( B , K * N , 3.0f ); std :: fill_n ( C , M * N , 0.42f ); printf ( \"Calling MatMul M=%d, K=%d, N=%d \\n \" , M , K , N ); hello_matmul_py ( A , B , C ); printf ( \"Result (first few elements): \" ); for ( int i = 0 ; i < 10 ; ++ i ) { printf ( \"%f \" , C [ i ]); } printf ( \" \\n \" ); return 0 ; } The code above creates the A , B , and C matrices, and calls the function hello_matmul_pi3_py to perform MatMul. Now that we have written the code, we will compile and link it with the HAT package to create an executable. Save the file to your working directory, in the same location as hello_matmul_pi3_generator.py and the generated *.HAT and *.o files. Build and run On the Raspberry Pi 3 device For this step, you\u2019ll be working with your Raspberry Pi device. If your Pi device is accessible over the network, copy hello_matmul_pi3_runner.cpp , hello_matmul_pi3.HAT , and hello_matmul_pi3.o using the Unix scp tool or the Windows WinSCP tool here ., otherwise use a USB thumb drive to transfer files manually. You do not need to copy the other generated files and folders. You will need gcc , it is often installed by default on Raspberry Pi 3 systems, but to confirm type: sudo apt-get install -y gcc This has been verified with \"Raspbian GNU/Linux 9 (stretch)\" and gcc<4:6.3.0-4>, and should work with subsequent versions. Then you can run the following commands to build and run. gcc hello_matmul_pi3_runner.cpp hello_matmul_pi3.o -o hello_matmul_pi3_runner ./hello_matmul_pi3_runner The output should look like: Calling MatMul M=128, K=256, N=256 Result (first few elements): 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 You can now experiment with the generated MatMul function with your own inputs. Simply modify hello_matmul_pi3_runner.cpp on the Raspberry Pi 3 and recompile with the existing HAT package.","title":"Cross Compilation for Raspberry Pi 3"},{"location":"Tutorials/Pi3_Cross_Compilation/#cross-compiling-for-the-raspberry-pi-3","text":"By the end of this tutorial, you will learn how to: Cross compile a simple Matrix Multiplication (MatMul) function for execution on a Raspberry Pi 3 Produce a HAT package containing the MatMul function that can be called on Pi 3 target Call the function from C or C++ code on a Raspberry Pi 3","title":"Cross Compiling for the Raspberry Pi 3"},{"location":"Tutorials/Pi3_Cross_Compilation/#prerequisites","text":"This tutorial assumes you already have Accera installed. If not, you can find the instructions in here You should also be familiar with writing Python and C++ You have access to a Raspberry Pi 3 device","title":"Prerequisites"},{"location":"Tutorials/Pi3_Cross_Compilation/#a-naive-matmul-algorithm","text":"Let's consider the example of multiplying matrices A and B, and adding the result into matrix C. In NumPy syntax, this can be expressed as: C += A @ B A naive algorithm for matrix multiplication typically contains 3 nested for loops. Expressed in Python, this could like: # A.shape = (M, K), B.shape = (K, N), C.shape = (M, N) for i in range(M): for j in range(N): for k in range(K): C[i, j] += A[i, k] * B[k, j]","title":"A naive MatMul algorithm"},{"location":"Tutorials/Pi3_Cross_Compilation/#accera-python-dsl","text":"We will now walk through a naive Matrix Multiplication (MatMul) using Accera. In order to cross compile on the host for a different target, instead of using the default target that is the host machine, we will specify a target that represents a Raspberry Pi 3. Create an empty file called hello_matmul_pi3_generator.py . First we'll import Accera's module. import accera as acc Define some matrix sizes. A will be M by K, B will be K by N, and C will be M by N. # Define our matrix sizes M = 128 N = 256 K = 256 Write a Python function that receives arrays A , B and C . These are our input and input/output matrices. A = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( M , K )) B = acc . Array ( role = acc . Array . Role . INPUT , element_type = acc . ScalarType . float32 , shape = ( K , N )) C = acc . Array ( role = acc . Array . Role . INPUT_OUTPUT , element_type = acc . ScalarType . float32 , shape = ( M , N )) Here, we will use the Nest class to define our 3-layered nested for loop. The range indices are M , N , and K , with the outermost loop ( M ) listed first. We can get the loop nest indices in order to perform the computation. # Define the loop nest nest = acc . Nest ( shape = ( M , N , K )) # Get the loop nest indices i , j , k = nest . get_indices () Next we define the logic of each iteration of the loop nest: # Define the loop nest logic @nest . iteration_logic def _ (): C [ i , j ] += A [ i , k ] * B [ k , j ] We have finished defining the logic of MatMul, and let's define the schedule which controls how the logic is executed. To do this, we first create the schedule from the nest: sched = nest . create_schedule () At this point, sched represents the default schedule for our algorithm. We can also perform some basic transformations on this schedule. For example, the following lines of code will split the k index in blocks of 4 (so k , k+4 , k+8 , and so on). # Split the k loop into blocks of 4, effectively doing this # (assuming K is divisible by 4): # # for i in range(M): # for j in range(N): # # Split k into two loops # for k in range(0, K, 4): # for kk in range(4): # C[i, j] += A[i, k + kk] * B[k + kk, j] # # If k is not divisible by 4, Accera will take care of the boundary # case for you. kk = sched . split ( k , 4 ) The split index is now k and kk . The next step is to create a plan from the schedule. For instance, we can use this plan to unroll the innermost loop. # Create a plan, specify the target to be a Raspberry Pi 3 pi3 = acc . Target ( acc . Target . Model . RASPBERRY_PI_3B ) plan = sched . create_plan ( pi3 ) # Unroll kk, effectively doing this # (assuming K is divisible by 4): # # for i in range(M): # for j in range(N): # for k in range(0, K, 4): # # Unrolled kk # C[i, j] += A[i, k + 0] * B[k + 0, j] # C[i, j] += A[i, k + 1] * B[k + 1, j] # C[i, j] += A[i, k + 2] * B[k + 2, j] # C[i, j] += A[i, k + 3] * B[k + 3, j] # # If k is not divisible by 4, Accera will take care of the boundary # case for you. plan . unroll ( kk ) Use the plan to add a callable function named hello_matmul_pi3_py to a HAT package. # Create a package and add a function to the package based on the plan package = acc . Package () package . add ( plan , args = ( A , B , C ), base_name = \"hello_matmul_pi3_py\" ) Finally, we build the statically-linked HAT package for the Raspbian platform: # Build the HAT package package . build ( name = \"hello_matmul_pi3\" , format = acc . Package . Format . HAT_STATIC , platform = acc . Package . Platform . RASPBIAN ) By now, you should have all the code necessary to generate your Accera MatMul function that can be called on a Raspberry Pi 3 target. You can also find the complete Python script here .","title":"Accera Python DSL"},{"location":"Tutorials/Pi3_Cross_Compilation/#generate-hat-package","text":"Next, we run the generator script to produce a HAT package for the Raspberry Pi 3 target.","title":"Generate HAT package"},{"location":"Tutorials/Pi3_Cross_Compilation/#windowsmacos","text":"python hello_matmul_pi3_generator.py","title":"Windows/MacOS"},{"location":"Tutorials/Pi3_Cross_Compilation/#ubuntu","text":"python3 hello_matmul_pi3_generator.py After this runs, you should see a header file hello_matmul_pi3.HAT and an object file hello_matmul_pi3.o in the ELF format. The .HAT file format is described here . Together with the obj , this is part of a HAT package.","title":"Ubuntu"},{"location":"Tutorials/Pi3_Cross_Compilation/#runner-code","text":"We will now walk through how to call our MatMul implementation from the HAT package on the Raspberry Pi 3. Create a file called hello_matmul_pi3_runner.cpp with the code below. You can also find it here . #include <stdio.h> #include <algorithm> // Include the HAT file that declares our MatMul function #include \"hello_matmul_p3.HAT\" #define M 128 #define N 256 #define K 256 int main ( int argc , const char ** argv ) { // Prepare our matrices float A [ M * K ]; float B [ K * N ]; float C [ M * N ]; // Fill with data std :: fill_n ( A , M * K , 2.0f ); std :: fill_n ( B , K * N , 3.0f ); std :: fill_n ( C , M * N , 0.42f ); printf ( \"Calling MatMul M=%d, K=%d, N=%d \\n \" , M , K , N ); hello_matmul_py ( A , B , C ); printf ( \"Result (first few elements): \" ); for ( int i = 0 ; i < 10 ; ++ i ) { printf ( \"%f \" , C [ i ]); } printf ( \" \\n \" ); return 0 ; } The code above creates the A , B , and C matrices, and calls the function hello_matmul_pi3_py to perform MatMul. Now that we have written the code, we will compile and link it with the HAT package to create an executable. Save the file to your working directory, in the same location as hello_matmul_pi3_generator.py and the generated *.HAT and *.o files.","title":"Runner code"},{"location":"Tutorials/Pi3_Cross_Compilation/#build-and-run","text":"","title":"Build and run"},{"location":"Tutorials/Pi3_Cross_Compilation/#on-the-raspberry-pi-3-device","text":"For this step, you\u2019ll be working with your Raspberry Pi device. If your Pi device is accessible over the network, copy hello_matmul_pi3_runner.cpp , hello_matmul_pi3.HAT , and hello_matmul_pi3.o using the Unix scp tool or the Windows WinSCP tool here ., otherwise use a USB thumb drive to transfer files manually. You do not need to copy the other generated files and folders. You will need gcc , it is often installed by default on Raspberry Pi 3 systems, but to confirm type: sudo apt-get install -y gcc This has been verified with \"Raspbian GNU/Linux 9 (stretch)\" and gcc<4:6.3.0-4>, and should work with subsequent versions. Then you can run the following commands to build and run. gcc hello_matmul_pi3_runner.cpp hello_matmul_pi3.o -o hello_matmul_pi3_runner ./hello_matmul_pi3_runner The output should look like: Calling MatMul M=128, K=256, N=256 Result (first few elements): 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 1536.419922 You can now experiment with the generated MatMul function with your own inputs. Simply modify hello_matmul_pi3_runner.cpp on the Raspberry Pi 3 and recompile with the existing HAT package.","title":"On the Raspberry Pi 3 device"}]}